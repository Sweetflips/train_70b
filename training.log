W0119 23:15:25.269000 6012 torch/distributed/run.py:803] 
W0119 23:15:25.269000 6012 torch/distributed/run.py:803] *****************************************
W0119 23:15:25.269000 6012 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0119 23:15:25.269000 6012 torch/distributed/run.py:803] *****************************************
[Rank 1] ============================================================[Rank 2] ============================================================
[Rank 0] ============================================================
[Rank 1] INIT: rank=1/8

[Rank 2] INIT: rank=2/8
[Rank 1] ============================================================[Rank 0] INIT: rank=0/8
[Rank 2] ============================================================

[Rank 1] MODEL: Qwen/Qwen2.5-Coder-32B-Instruct[Rank 0] ============================================================

[Rank 2] MODEL: Qwen/Qwen2.5-Coder-32B-Instruct[Rank 1] MEMORY: no RLIMIT_AS set (CUDA needs large virtual address space)

[Rank 0] MODEL: Qwen/Qwen2.5-Coder-32B-Instruct[Rank 2] MEMORY: no RLIMIT_AS set (CUDA needs large virtual address space)[Rank 1] LOCK: waiting for exclusive access...

[Rank 0] MEMORY: no RLIMIT_AS set (CUDA needs large virtual address space)
[Rank 2] LOCK: waiting for exclusive access...

[Rank 0] LOCK: waiting for exclusive access...
[Rank 1] LOCK: acquired! Starting initialization...
[Rank 4] ============================================================
[Rank 4] INIT: rank=4/8
[Rank 4] ============================================================
[Rank 4] MODEL: Qwen/Qwen2.5-Coder-32B-Instruct
[Rank 4] MEMORY: no RLIMIT_AS set (CUDA needs large virtual address space)
[Rank 4] LOCK: waiting for exclusive access...
[Rank 3] ============================================================
[Rank 3] INIT: rank=3/8
[Rank 3] ============================================================
[Rank 3] MODEL: Qwen/Qwen2.5-Coder-32B-Instruct
[Rank 3] MEMORY: no RLIMIT_AS set (CUDA needs large virtual address space)
[Rank 3] LOCK: waiting for exclusive access...
[Rank 5] ============================================================
[Rank 5] INIT: rank=5/8
[Rank 5] ============================================================
[Rank 5] MODEL: Qwen/Qwen2.5-Coder-32B-Instruct
[Rank 5] MEMORY: no RLIMIT_AS set (CUDA needs large virtual address space)
[Rank 5] LOCK: waiting for exclusive access...
[Rank 7] ============================================================
[Rank 7] INIT: rank=7/8
[Rank 7] ============================================================
[Rank 7] MODEL: Qwen/Qwen2.5-Coder-32B-Instruct
[Rank 7] MEMORY: no RLIMIT_AS set (CUDA needs large virtual address space)
[Rank 7] LOCK: waiting for exclusive access...
[Rank 6] ============================================================
[Rank 6] INIT: rank=6/8
[Rank 6] ============================================================
[Rank 6] MODEL: Qwen/Qwen2.5-Coder-32B-Instruct
[Rank 6] MEMORY: no RLIMIT_AS set (CUDA needs large virtual address space)
[Rank 6] LOCK: waiting for exclusive access...
[Rank 1] [MEM:START] RAM: 44,814MB / 2,216,624MB (2.0%)
[Rank 1] IMPORT: torch...
[Rank 1] IMPORT: torch OK (v2.9.1+cu128)
[Rank 1] DEVICE: cuda:1
[Rank 1] CUDA: warming up...
[Rank 1] CUDA: ready
[Rank 1] IMPORT: transformers...
[Rank 1] IMPORT: transformers OK
[Rank 1] IMPORT: peft...
[Rank 1] IMPORT: peft OK
[Rank 1] IMPORT: trl...
[Rank 1] IMPORT: trl OK
[Rank 1] IMPORT: datasets...
[Rank 1] IMPORT: datasets OK
[Rank 1] [MEM:POST-IMPORT] RAM: 45,541MB / 2,216,624MB (2.1%)
[Rank 1] LOCK: released[Rank 0] LOCK: acquired! Starting initialization...

[Rank 0] [MEM:START] RAM: 45,539MB / 2,216,624MB (2.1%)
[Rank 0] [MEM:START] GPU0: 4MB / 275040MB
[Rank 0] [MEM:START] GPU1: 726MB / 275040MB
[Rank 0] [MEM:START] GPU2: 4MB / 275040MB
[Rank 0] [MEM:START] GPU3: 4MB / 275040MB
[Rank 0] [MEM:START] GPU4: 4MB / 275040MB
[Rank 0] [MEM:START] GPU5: 4MB / 275040MB
[Rank 0] [MEM:START] GPU6: 4MB / 275040MB
[Rank 0] [MEM:START] GPU7: 4MB / 275040MB
[Rank 0] IMPORT: torch...
[Rank 1] DISTRIBUTED: initializing...
[Rank 1] DISTRIBUTED: OK (rank=1)
[Rank 1] TOKENIZER: loading...
[Rank 0] IMPORT: torch OK (v2.9.1+cu128)
[Rank 1] TOKENIZER: OK
[Rank 1] DATASET: loading...
[Rank 1] DATASET: 912960 examples
[Rank 0] DEVICE: cuda:0
[Rank 0] CUDA: warming up...
[Rank 0] CUDA: ready
[Rank 0] IMPORT: transformers...
[Rank 1] [MEM:POST-DATASET] RAM: 46,721MB / 2,216,624MB (2.1%)
[Rank 0] IMPORT: transformers OK
[Rank 0] IMPORT: peft...
[Rank 0] IMPORT: peft OK
[Rank 0] IMPORT: trl...
[Rank 0] IMPORT: trl OK
[Rank 0] IMPORT: datasets...
[Rank 0] IMPORT: datasets OK
[Rank 0] [MEM:POST-IMPORT] RAM: 46,868MB / 2,216,624MB (2.1%)
[Rank 0] [MEM:POST-IMPORT] GPU0: 726MB / 275040MB
[Rank 0] [MEM:POST-IMPORT] GPU1: 726MB / 275040MB
[Rank 0] [MEM:POST-IMPORT] GPU2: 4MB / 275040MB
[Rank 0] [MEM:POST-IMPORT] GPU3: 4MB / 275040MB
[Rank 0] [MEM:POST-IMPORT] GPU4: 4MB / 275040MB
[Rank 0] [MEM:POST-IMPORT] GPU5: 4MB / 275040MB
[Rank 0] [MEM:POST-IMPORT] GPU6: 4MB / 275040MB
[Rank 0] [MEM:POST-IMPORT] GPU7: 4MB / 275040MB
[Rank 0] LOCK: released[Rank 2] LOCK: acquired! Starting initialization...

[Rank 2] [MEM:START] RAM: 46,886MB / 2,216,624MB (2.1%)
[Rank 2] IMPORT: torch...
[Rank 0] DISTRIBUTED: initializing...
[Rank 0] DISTRIBUTED: OK (rank=0)
[Rank 0] TOKENIZER: loading...
[Rank 2] IMPORT: torch OK (v2.9.1+cu128)
[Rank 0] TOKENIZER: OK
[Rank 0] DATASET: loading...
[Rank 0] DATASET: 912960 examples
[Rank 2] DEVICE: cuda:2
[Rank 2] CUDA: warming up...
[Rank 2] CUDA: ready
[Rank 2] IMPORT: transformers...
[Rank 0] [MEM:POST-DATASET] RAM: 47,435MB / 2,216,624MB (2.1%)
[Rank 0] [MEM:POST-DATASET] GPU0: 726MB / 275040MB
[Rank 0] [MEM:POST-DATASET] GPU1: 726MB / 275040MB
[Rank 0] [MEM:POST-DATASET] GPU2: 726MB / 275040MB
[Rank 0] [MEM:POST-DATASET] GPU3: 4MB / 275040MB
[Rank 0] [MEM:POST-DATASET] GPU4: 4MB / 275040MB
[Rank 0] [MEM:POST-DATASET] GPU5: 4MB / 275040MB
[Rank 0] [MEM:POST-DATASET] GPU6: 4MB / 275040MB
[Rank 0] [MEM:POST-DATASET] GPU7: 4MB / 275040MB
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
[rank0]:[W119 23:15:44.235765229 ProcessGroupNCCL.cpp:5072] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()
[Rank 2] IMPORT: transformers OK
[Rank 2] IMPORT: peft...
[Rank 2] IMPORT: peft OK
[Rank 2] IMPORT: trl...
[Rank 2] IMPORT: trl OK
[Rank 2] IMPORT: datasets...
[Rank 2] IMPORT: datasets OK
[Rank 2] [MEM:POST-IMPORT] RAM: 48,294MB / 2,216,624MB (2.2%)
[Rank 2] LOCK: released
[Rank 4] LOCK: acquired! Starting initialization...
[Rank 4] [MEM:START] RAM: 48,294MB / 2,216,624MB (2.2%)
[Rank 4] IMPORT: torch...
[Rank 2] DISTRIBUTED: initializing...
[Rank 2] DISTRIBUTED: OK (rank=2)
[Rank 2] TOKENIZER: loading...
[Rank 4] IMPORT: torch OK (v2.9.1+cu128)
[Rank 2] TOKENIZER: OK
[Rank 2] DATASET: loading...
[Rank 2] DATASET: 912960 examples
[Rank 2] [MEM:POST-DATASET] RAM: 48,704MB / 2,216,624MB (2.2%)
[Rank 4] DEVICE: cuda:4
[Rank 4] CUDA: warming up...
[Rank 4] CUDA: ready
[Rank 4] IMPORT: transformers...
[Rank 4] IMPORT: transformers OK
[Rank 4] IMPORT: peft...
[Rank 4] IMPORT: peft OK
[Rank 4] IMPORT: trl...
[Rank 4] IMPORT: trl OK
[Rank 4] IMPORT: datasets...
[Rank 4] IMPORT: datasets OK
[Rank 4] [MEM:POST-IMPORT] RAM: 49,247MB / 2,216,624MB (2.2%)
[Rank 4] LOCK: released
[Rank 3] LOCK: acquired! Starting initialization...
[Rank 3] [MEM:START] RAM: 49,250MB / 2,216,624MB (2.2%)
[Rank 3] IMPORT: torch...
[Rank 4] DISTRIBUTED: initializing...
[Rank 4] DISTRIBUTED: OK (rank=4)
[Rank 4] TOKENIZER: loading...
[Rank 3] IMPORT: torch OK (v2.9.1+cu128)
[Rank 4] TOKENIZER: OK
[Rank 4] DATASET: loading...
[Rank 4] DATASET: 912960 examples
[Rank 3] DEVICE: cuda:3
[Rank 3] CUDA: warming up...
[Rank 3] CUDA: ready
[Rank 3] IMPORT: transformers...
[Rank 4] [MEM:POST-DATASET] RAM: 49,802MB / 2,216,624MB (2.2%)
[Rank 3] IMPORT: transformers OK
[Rank 3] IMPORT: peft...
[Rank 3] IMPORT: peft OK
[Rank 3] IMPORT: trl...
[Rank 3] IMPORT: trl OK
[Rank 3] IMPORT: datasets...
[Rank 3] IMPORT: datasets OK
[Rank 3] [MEM:POST-IMPORT] RAM: 50,250MB / 2,216,624MB (2.3%)
[Rank 3] LOCK: released[Rank 5] LOCK: acquired! Starting initialization...

[Rank 5] [MEM:START] RAM: 50,246MB / 2,216,624MB (2.3%)
[Rank 5] IMPORT: torch...
[Rank 3] DISTRIBUTED: initializing...
[Rank 3] DISTRIBUTED: OK (rank=3)
[Rank 3] TOKENIZER: loading...
[Rank 5] IMPORT: torch OK (v2.9.1+cu128)
[Rank 3] TOKENIZER: OK
[Rank 3] DATASET: loading...
[Rank 3] DATASET: 912960 examples
[Rank 3] [MEM:POST-DATASET] RAM: 50,763MB / 2,216,624MB (2.3%)
[Rank 5] DEVICE: cuda:5
[Rank 5] CUDA: warming up...
[Rank 5] CUDA: ready
[Rank 5] IMPORT: transformers...
[Rank 5] IMPORT: transformers OK
[Rank 5] IMPORT: peft...
[Rank 5] IMPORT: peft OK
[Rank 5] IMPORT: trl...
[Rank 5] IMPORT: trl OK
[Rank 5] IMPORT: datasets...
[Rank 5] IMPORT: datasets OK
[Rank 5] [MEM:POST-IMPORT] RAM: 51,190MB / 2,216,624MB (2.3%)
[Rank 5] LOCK: released[Rank 7] LOCK: acquired! Starting initialization...

[Rank 7] [MEM:START] RAM: 51,192MB / 2,216,624MB (2.3%)
[Rank 7] IMPORT: torch...
[Rank 5] DISTRIBUTED: initializing...
[Rank 5] DISTRIBUTED: OK (rank=5)
[Rank 5] TOKENIZER: loading...
[Rank 7] IMPORT: torch OK (v2.9.1+cu128)
[Rank 7] DEVICE: cuda:7
[Rank 7] CUDA: warming up...
[Rank 7] CUDA: ready
[Rank 7] IMPORT: transformers...
[Rank 5] TOKENIZER: OK
[Rank 5] DATASET: loading...
[Rank 5] DATASET: 912960 examples
[Rank 5] [MEM:POST-DATASET] RAM: 51,751MB / 2,216,624MB (2.3%)
[Rank 7] IMPORT: transformers OK
[Rank 7] IMPORT: peft...
[Rank 7] IMPORT: peft OK
[Rank 7] IMPORT: trl...
[Rank 7] IMPORT: trl OK
[Rank 7] IMPORT: datasets...
[Rank 7] IMPORT: datasets OK
[Rank 7] [MEM:POST-IMPORT] RAM: 52,156MB / 2,216,624MB (2.4%)
[Rank 7] LOCK: released[Rank 6] LOCK: acquired! Starting initialization...

[Rank 6] [MEM:START] RAM: 52,152MB / 2,216,624MB (2.4%)
[Rank 6] IMPORT: torch...
[Rank 7] DISTRIBUTED: initializing...
[Rank 7] DISTRIBUTED: OK (rank=7)
[Rank 7] TOKENIZER: loading...
[Rank 6] IMPORT: torch OK (v2.9.1+cu128)
[Rank 7] TOKENIZER: OK
[Rank 7] DATASET: loading...
[Rank 7] DATASET: 912960 examples
[Rank 7] [MEM:POST-DATASET] RAM: 52,639MB / 2,216,624MB (2.4%)
[Rank 6] DEVICE: cuda:6
[Rank 6] CUDA: warming up...
[Rank 6] CUDA: ready
[Rank 6] IMPORT: transformers...
[Rank 6] IMPORT: transformers OK
[Rank 6] IMPORT: peft...
[Rank 6] IMPORT: peft OK
[Rank 6] IMPORT: trl...
[Rank 6] IMPORT: trl OK
[Rank 6] IMPORT: datasets...
[Rank 6] IMPORT: datasets OK
[Rank 6] [MEM:POST-IMPORT] RAM: 53,134MB / 2,216,624MB (2.4%)
[Rank 6] LOCK: released
[Rank 6] DISTRIBUTED: initializing...
[Rank 6] DISTRIBUTED: OK (rank=6)
[Rank 6] TOKENIZER: loading...
[Rank 6] TOKENIZER: OK
[Rank 6] DATASET: loading...
[Rank 6] DATASET: 912960 examples
[Rank 6] [MEM:POST-DATASET] RAM: 53,205MB / 2,216,624MB (2.4%)
[Rank 5] MODEL: waiting for lock to load model...[Rank 4] MODEL: waiting for lock to load model...[Rank 2] MODEL: waiting for lock to load model...[Rank 7] MODEL: waiting for lock to load model...[Rank 1] MODEL: waiting for lock to load model...[Rank 0] MODEL: waiting for lock to load model...[Rank 3] MODEL: waiting for lock to load model...[Rank 6] MODEL: waiting for lock to load model...







[Rank 4] MODEL: loading to CPU (FSDP will handle GPU sharding)...
[Rank 4] flash_attention_2 not available, using sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards:  21%|██▏       | 3/14 [00:00<00:00, 24.02it/s]Loading checkpoint shards:  43%|████▎     | 6/14 [00:00<00:00, 24.20it/s]Loading checkpoint shards:  64%|██████▍   | 9/14 [00:00<00:00, 24.48it/s]Loading checkpoint shards:  86%|████████▌ | 12/14 [00:00<00:00, 24.97it/s]Loading checkpoint shards: 100%|██████████| 14/14 [00:00<00:00, 26.58it/s]
[Rank 4] MODEL: loaded to CPU!
[Rank 4] [MEM:POST-CPU-LOAD] RAM: 53,755MB / 2,216,624MB (2.4%)
[Rank 4] [MEM:POST-GC] RAM: 53,755MB / 2,216,624MB (2.4%)
[Rank 5] MODEL: loading to CPU (FSDP will handle GPU sharding)...
[Rank 5] flash_attention_2 not available, using sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 14/14 [00:00<00:00, 640.13it/s]
[Rank 5] MODEL: loaded to CPU!
[Rank 5] [MEM:POST-CPU-LOAD] RAM: 53,758MB / 2,216,624MB (2.4%)
[Rank 5] [MEM:POST-GC] RAM: 53,758MB / 2,216,624MB (2.4%)
[Rank 2] MODEL: loading to CPU (FSDP will handle GPU sharding)...
[Rank 2] flash_attention_2 not available, using sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 14/14 [00:00<00:00, 658.19it/s]
[Rank 2] MODEL: loaded to CPU!
[Rank 2] [MEM:POST-CPU-LOAD] RAM: 53,753MB / 2,216,624MB (2.4%)
[Rank 2] [MEM:POST-GC] RAM: 53,753MB / 2,216,624MB (2.4%)
[Rank 7] MODEL: loading to CPU (FSDP will handle GPU sharding)...
[Rank 7] flash_attention_2 not available, using sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 14/14 [00:00<00:00, 633.03it/s]
[Rank 7] MODEL: loaded to CPU!
[Rank 7] [MEM:POST-CPU-LOAD] RAM: 53,759MB / 2,216,624MB (2.4%)
[Rank 7] [MEM:POST-GC] RAM: 53,760MB / 2,216,624MB (2.4%)
[Rank 1] MODEL: loading to CPU (FSDP will handle GPU sharding)...
[Rank 1] flash_attention_2 not available, using sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 14/14 [00:00<00:00, 623.60it/s]
[Rank 1] MODEL: loaded to CPU!
[Rank 1] [MEM:POST-CPU-LOAD] RAM: 53,762MB / 2,216,624MB (2.4%)
[Rank 1] [MEM:POST-GC] RAM: 53,762MB / 2,216,624MB (2.4%)
[Rank 3] MODEL: loading to CPU (FSDP will handle GPU sharding)...
[Rank 3] flash_attention_2 not available, using sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 14/14 [00:00<00:00, 612.26it/s]
[Rank 3] MODEL: loaded to CPU!
[Rank 3] [MEM:POST-CPU-LOAD] RAM: 53,762MB / 2,216,624MB (2.4%)
[Rank 3] [MEM:POST-GC] RAM: 53,762MB / 2,216,624MB (2.4%)
[Rank 0] MODEL: loading to CPU (FSDP will handle GPU sharding)...
[Rank 0] flash_attention_2 not available, using sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 14/14 [00:00<00:00, 603.35it/s]
[Rank 0] MODEL: loaded to CPU!
[Rank 0] [MEM:POST-CPU-LOAD] RAM: 53,765MB / 2,216,624MB (2.4%)
[Rank 0] [MEM:POST-CPU-LOAD] GPU0: 1784MB / 275040MB
[Rank 0] [MEM:POST-CPU-LOAD] GPU1: 1784MB / 275040MB
[Rank 0] [MEM:POST-CPU-LOAD] GPU2: 1784MB / 275040MB
[Rank 0] [MEM:POST-CPU-LOAD] GPU3: 1784MB / 275040MB
[Rank 0] [MEM:POST-CPU-LOAD] GPU4: 1784MB / 275040MB
[Rank 0] [MEM:POST-CPU-LOAD] GPU5: 1784MB / 275040MB
[Rank 0] [MEM:POST-CPU-LOAD] GPU6: 1784MB / 275040MB
[Rank 0] [MEM:POST-CPU-LOAD] GPU7: 1784MB / 275040MB
[Rank 0] [MEM:POST-GC] RAM: 53,784MB / 2,216,624MB (2.4%)
[Rank 0] [MEM:POST-GC] GPU0: 1784MB / 275040MB
[Rank 0] [MEM:POST-GC] GPU1: 1784MB / 275040MB
[Rank 0] [MEM:POST-GC] GPU2: 1784MB / 275040MB
[Rank 0] [MEM:POST-GC] GPU3: 1784MB / 275040MB
[Rank 0] [MEM:POST-GC] GPU4: 1784MB / 275040MB
[Rank 0] [MEM:POST-GC] GPU5: 1784MB / 275040MB
[Rank 0] [MEM:POST-GC] GPU6: 1784MB / 275040MB
[Rank 0] [MEM:POST-GC] GPU7: 1784MB / 275040MB
[Rank 6] MODEL: loading to CPU (FSDP will handle GPU sharding)...
[Rank 6] flash_attention_2 not available, using sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 14/14 [00:00<00:00, 620.07it/s]
[Rank 6] MODEL: loaded to CPU!
[Rank 6] [MEM:POST-CPU-LOAD] RAM: 53,784MB / 2,216,624MB (2.4%)
[Rank 6] [MEM:POST-GC] RAM: 53,782MB / 2,216,624MB (2.4%)
[Rank 0] MODEL: all ranks have loaded model
[Rank 0] LORA: configuring...
[Rank 1] MODEL: all ranks have loaded model
[Rank 1] LORA: configuring...
[Rank 2] MODEL: all ranks have loaded model
[Rank 2] LORA: configuring...
[Rank 3] MODEL: all ranks have loaded model
[Rank 3] LORA: configuring...
[Rank 4] MODEL: all ranks have loaded model[Rank 6] MODEL: all ranks have loaded model
[Rank 5] MODEL: all ranks have loaded model[Rank 4] LORA: configuring...[Rank 7] MODEL: all ranks have loaded model

[Rank 6] LORA: configuring...

[Rank 5] LORA: configuring...

[Rank 7] LORA: configuring...
[Rank 4] [MEM:POST-LORA] RAM: 68,085MB / 2,216,624MB (3.1%)
[Rank 4] CONFIG: creating SFTConfig (memory-optimized)...
[Rank 2] [MEM:POST-LORA] RAM: 68,101MB / 2,216,624MB (3.1%)
[Rank 2] CONFIG: creating SFTConfig (memory-optimized)...
[Rank 5] [MEM:POST-LORA] RAM: 68,356MB / 2,216,624MB (3.1%)
[Rank 5] CONFIG: creating SFTConfig (memory-optimized)...
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[Rank 4] CONFIG: batch_size=56, grad_accum=1, seq_len=1024
[Rank 4] CONFIG: OK
[Rank 4] TRAINER: creating...
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[Rank 2] CONFIG: batch_size=56, grad_accum=1, seq_len=1024
[Rank 2] CONFIG: OK
[Rank 2] TRAINER: creating...
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[Rank 5] CONFIG: batch_size=56, grad_accum=1, seq_len=1024
[Rank 5] CONFIG: OK
[Rank 5] TRAINER: creating...
trainable params: 536,870,912 || all params: 33,300,747,264 || trainable%: 1.6122
[Rank 7] [MEM:POST-LORA] RAM: 71,957MB / 2,216,624MB (3.2%)
[Rank 7] CONFIG: creating SFTConfig (memory-optimized)...
[Rank 6] [MEM:POST-LORA] RAM: 71,957MB / 2,216,624MB (3.2%)
[Rank 6] CONFIG: creating SFTConfig (memory-optimized)...
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[Rank 7] CONFIG: batch_size=56, grad_accum=1, seq_len=1024
[Rank 7] CONFIG: OK
[Rank 7] TRAINER: creating...
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[Rank 6] CONFIG: batch_size=56, grad_accum=1, seq_len=1024
[Rank 6] CONFIG: OK
[Rank 6] TRAINER: creating...
[Rank 1] [MEM:POST-LORA] RAM: 72,130MB / 2,216,624MB (3.3%)
[Rank 1] CONFIG: creating SFTConfig (memory-optimized)...
[Rank 3] [MEM:POST-LORA] RAM: 72,147MB / 2,216,624MB (3.3%)
[Rank 3] CONFIG: creating SFTConfig (memory-optimized)...
[Rank 0] [MEM:POST-LORA] RAM: 72,185MB / 2,216,624MB (3.3%)
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[Rank 1] CONFIG: batch_size=56, grad_accum=1, seq_len=1024
[Rank 1] CONFIG: OK
[Rank 1] TRAINER: creating...
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[Rank 3] CONFIG: batch_size=56, grad_accum=1, seq_len=1024
[Rank 3] CONFIG: OK
[Rank 3] TRAINER: creating...
[Rank 0] [MEM:POST-LORA] GPU0: 1784MB / 275040MB
[Rank 0] [MEM:POST-LORA] GPU1: 1784MB / 275040MB
[Rank 0] [MEM:POST-LORA] GPU2: 1784MB / 275040MB
[Rank 0] [MEM:POST-LORA] GPU3: 1784MB / 275040MB
[Rank 0] [MEM:POST-LORA] GPU4: 1784MB / 275040MB
[Rank 0] [MEM:POST-LORA] GPU5: 1784MB / 275040MB
[Rank 0] [MEM:POST-LORA] GPU6: 1784MB / 275040MB
[Rank 0] [MEM:POST-LORA] GPU7: 1784MB / 275040MB
[Rank 0] CONFIG: creating SFTConfig (memory-optimized)...
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[Rank 0] CONFIG: batch_size=56, grad_accum=1, seq_len=1024
[Rank 0] CONFIG: OK
[Rank 0] TRAINER: creating...
[Rank 0] TRAINER: OK
[Rank 0] [MEM:POST-TRAINER] RAM: 72,894MB / 2,216,624MB (3.3%)
[Rank 4] TRAINER: OK
[Rank 4] [MEM:POST-TRAINER] RAM: 72,897MB / 2,216,624MB (3.3%)
[Rank 5] TRAINER: OK
[Rank 5] [MEM:POST-TRAINER] RAM: 72,897MB / 2,216,624MB (3.3%)
[Rank 6] TRAINER: OK
[Rank 6] [MEM:POST-TRAINER] RAM: 72,896MB / 2,216,624MB (3.3%)
[Rank 2] TRAINER: OK
[Rank 2] [MEM:POST-TRAINER] RAM: 72,896MB / 2,216,624MB (3.3%)
[Rank 7] TRAINER: OK
[Rank 7] [MEM:POST-TRAINER] RAM: 72,896MB / 2,216,624MB (3.3%)
[Rank 1] TRAINER: OK
[Rank 1] [MEM:POST-TRAINER] RAM: 72,896MB / 2,216,624MB (3.3%)
[Rank 3] TRAINER: OK
[Rank 3] [MEM:POST-TRAINER] RAM: 72,895MB / 2,216,624MB (3.3%)
[Rank 0] [MEM:POST-TRAINER] GPU0: 1784MB / 275040MB
[Rank 0] [MEM:POST-TRAINER] GPU1: 1784MB / 275040MB
[Rank 0] [MEM:POST-TRAINER] GPU2: 1784MB / 275040MB
[Rank 0] [MEM:POST-TRAINER] GPU3: 1784MB / 275040MB
[Rank 0] [MEM:POST-TRAINER] GPU4: 1784MB / 275040MB
[Rank 0] [MEM:POST-TRAINER] GPU5: 1784MB / 275040MB
[Rank 0] [MEM:POST-TRAINER] GPU6: 1784MB / 275040MB
[Rank 0] [MEM:POST-TRAINER] GPU7: 1784MB / 275040MB
[Rank 4] TRAIN: starting...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.
[Rank 2] TRAIN: starting...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.
[Rank 5] TRAIN: starting...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.
[Rank 6] TRAIN: starting...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.
[Rank 7] TRAIN: starting...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.
[Rank 3] TRAIN: starting...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.
[Rank 1] TRAIN: starting...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.
[Rank 0] TRAIN: resuming from ./output/checkpoint-1000
[Rank 0] TRAIN: starting...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
  0%|          | 0/2038 [00:00<?, ?it/s] 49%|████▉     | 1001/2038 [00:20<00:21, 48.47it/s] 49%|████▉     | 1001/2038 [00:31<00:21, 48.47it/s] 49%|████▉     | 1002/2038 [00:39<00:48, 21.16it/s] 49%|████▉     | 1003/2038 [00:58<01:28, 11.73it/s] 49%|████▉     | 1004/2038 [01:16<02:22,  7.26it/s] 49%|████▉     | 1005/2038 [01:34<03:39,  4.71it/s] 49%|████▉     | 1006/2038 [01:52<05:27,  3.15it/s] 49%|████▉     | 1007/2038 [02:10<07:59,  2.15it/s] 49%|████▉     | 1008/2038 [02:28<11:31,  1.49it/s] 50%|████▉     | 1009/2038 [02:47<16:38,  1.03it/s] 50%|████▉     | 1010/2038 [03:06<23:23,  1.37s/it]                                                   {'loss': 0.1806, 'grad_norm': 0.026610050350427628, 'learning_rate': 5.207489878542511e-05, 'entropy': 0.18099832832813262, 'num_tokens': 1972424.0, 'mean_token_accuracy': 0.9389932453632355, 'epoch': 0.5}
 50%|████▉     | 1010/2038 [03:06<23:23,  1.37s/it] 50%|████▉     | 1011/2038 [03:24<32:29,  1.90s/it] 50%|████▉     | 1012/2038 [03:42<44:32,  2.60s/it] 50%|████▉     | 1013/2038 [04:00<1:00:03,  3.52s/it] 50%|████▉     | 1014/2038 [04:18<1:19:02,  4.63s/it] 50%|████▉     | 1015/2038 [04:36<1:41:45,  5.97s/it] 50%|████▉     | 1016/2038 [04:55<2:08:57,  7.57s/it] 50%|████▉     | 1017/2038 [05:13<2:35:38,  9.15s/it] 50%|████▉     | 1018/2038 [05:31<3:02:39, 10.74s/it] 50%|█████     | 1019/2038 [05:50<3:28:03, 12.25s/it] 50%|█████     | 1020/2038 [06:08<3:50:10, 13.57s/it]                                                     {'loss': 0.1858, 'grad_norm': 0.025720102712512016, 'learning_rate': 5.1568825910931177e-05, 'entropy': 0.18596912622451783, 'num_tokens': 3968008.0, 'mean_token_accuracy': 0.9374460935592651, 'epoch': 0.5}
 50%|█████     | 1020/2038 [06:08<3:50:10, 13.57s/it] 50%|█████     | 1021/2038 [06:26<4:08:21, 14.65s/it] 50%|█████     | 1022/2038 [06:44<4:24:28, 15.62s/it] 50%|█████     | 1023/2038 [07:03<4:37:44, 16.42s/it] 50%|█████     | 1024/2038 [07:21<4:45:30, 16.89s/it] 50%|█████     | 1025/2038 [07:39<4:51:15, 17.25s/it] 50%|█████     | 1026/2038 [07:57<4:55:19, 17.51s/it] 50%|█████     | 1027/2038 [08:15<4:58:07, 17.69s/it] 50%|█████     | 1028/2038 [08:34<5:01:21, 17.90s/it] 50%|█████     | 1029/2038 [08:53<5:06:39, 18.24s/it] 51%|█████     | 1030/2038 [09:11<5:05:56, 18.21s/it]                                                     {'loss': 0.1789, 'grad_norm': 0.027322009205818176, 'learning_rate': 5.106275303643725e-05, 'entropy': 0.1787508338689804, 'num_tokens': 5933369.0, 'mean_token_accuracy': 0.9394914507865906, 'epoch': 0.51}
 51%|█████     | 1030/2038 [09:11<5:05:56, 18.21s/it] 51%|█████     | 1031/2038 [09:29<5:05:09, 18.18s/it] 51%|█████     | 1032/2038 [09:47<5:04:38, 18.17s/it] 51%|█████     | 1033/2038 [10:05<5:04:14, 18.16s/it] 51%|█████     | 1034/2038 [10:24<5:03:55, 18.16s/it] 51%|█████     | 1035/2038 [10:43<5:09:16, 18.50s/it] 51%|█████     | 1036/2038 [11:01<5:07:09, 18.39s/it] 51%|█████     | 1037/2038 [11:19<5:05:49, 18.33s/it] 51%|█████     | 1038/2038 [11:37<5:04:13, 18.25s/it] 51%|█████     | 1039/2038 [11:55<5:03:12, 18.21s/it] 51%|█████     | 1040/2038 [12:13<5:02:15, 18.17s/it]                                                     {'loss': 0.1833, 'grad_norm': 0.026542911306023598, 'learning_rate': 5.055668016194332e-05, 'entropy': 0.1833755075931549, 'num_tokens': 7921308.0, 'mean_token_accuracy': 0.9382711946964264, 'epoch': 0.51}
 51%|█████     | 1040/2038 [12:13<5:02:15, 18.17s/it] 51%|█████     | 1041/2038 [12:32<5:02:40, 18.21s/it] 51%|█████     | 1042/2038 [12:51<5:06:21, 18.46s/it] 51%|█████     | 1043/2038 [13:09<5:04:20, 18.35s/it] 51%|█████     | 1044/2038 [13:27<5:02:41, 18.27s/it] 51%|█████▏    | 1045/2038 [13:45<5:01:34, 18.22s/it] 51%|█████▏    | 1046/2038 [14:03<5:00:57, 18.20s/it] 51%|█████▏    | 1047/2038 [14:21<5:00:22, 18.19s/it] 51%|█████▏    | 1048/2038 [14:41<5:05:19, 18.50s/it] 51%|█████▏    | 1049/2038 [14:59<5:02:53, 18.38s/it] 52%|█████▏    | 1050/2038 [15:17<5:01:38, 18.32s/it]                                                     {'loss': 0.1804, 'grad_norm': 0.0262804813683033, 'learning_rate': 5.005060728744939e-05, 'entropy': 0.18103027790784837, 'num_tokens': 9892365.0, 'mean_token_accuracy': 0.938872492313385, 'epoch': 0.52}
 52%|█████▏    | 1050/2038 [15:17<5:01:38, 18.32s/it] 52%|█████▏    | 1051/2038 [15:35<5:00:18, 18.26s/it] 52%|█████▏    | 1052/2038 [15:53<4:58:56, 18.19s/it] 52%|█████▏    | 1053/2038 [16:11<4:58:26, 18.18s/it] 52%|█████▏    | 1054/2038 [16:30<5:03:25, 18.50s/it] 52%|█████▏    | 1055/2038 [16:49<5:01:14, 18.39s/it] 52%|█████▏    | 1056/2038 [17:07<5:00:06, 18.34s/it] 52%|█████▏    | 1057/2038 [17:25<4:58:35, 18.26s/it] 52%|█████▏    | 1058/2038 [17:43<4:58:02, 18.25s/it] 52%|█████▏    | 1059/2038 [18:01<4:56:47, 18.19s/it] 52%|█████▏    | 1060/2038 [18:20<4:59:04, 18.35s/it]                                                     {'loss': 0.1812, 'grad_norm': 0.02725955657660961, 'learning_rate': 4.954453441295547e-05, 'entropy': 0.18149595558643342, 'num_tokens': 11880665.0, 'mean_token_accuracy': 0.9387101829051971, 'epoch': 0.52}
 52%|█████▏    | 1060/2038 [18:20<4:59:04, 18.35s/it] 52%|█████▏    | 1061/2038 [18:38<5:00:20, 18.44s/it] 52%|█████▏    | 1062/2038 [18:57<4:58:21, 18.34s/it] 52%|█████▏    | 1063/2038 [19:15<4:57:38, 18.32s/it] 52%|█████▏    | 1064/2038 [19:33<4:56:36, 18.27s/it] 52%|█████▏    | 1065/2038 [19:51<4:55:38, 18.23s/it] 52%|█████▏    | 1066/2038 [20:09<4:54:39, 18.19s/it] 52%|█████▏    | 1067/2038 [20:29<4:59:43, 18.52s/it] 52%|█████▏    | 1068/2038 [20:47<4:57:04, 18.38s/it] 52%|█████▏    | 1069/2038 [21:05<4:55:38, 18.31s/it] 53%|█████▎    | 1070/2038 [21:23<4:54:47, 18.27s/it]                                                     {'loss': 0.183, 'grad_norm': 0.02705450728535652, 'learning_rate': 4.9038461538461536e-05, 'entropy': 0.18303068727254868, 'num_tokens': 13863168.0, 'mean_token_accuracy': 0.9379091560840607, 'epoch': 0.53}
 53%|█████▎    | 1070/2038 [21:23<4:54:47, 18.27s/it] 53%|█████▎    | 1071/2038 [21:41<4:54:06, 18.25s/it] 53%|█████▎    | 1072/2038 [21:59<4:53:26, 18.23s/it] 53%|█████▎    | 1073/2038 [22:18<4:55:56, 18.40s/it] 53%|█████▎    | 1074/2038 [22:37<4:56:31, 18.46s/it] 53%|█████▎    | 1075/2038 [22:55<4:54:26, 18.35s/it] 53%|█████▎    | 1076/2038 [23:13<4:53:12, 18.29s/it] 53%|█████▎    | 1077/2038 [23:31<4:52:34, 18.27s/it] 53%|█████▎    | 1078/2038 [23:49<4:51:20, 18.21s/it] 53%|█████▎    | 1079/2038 [24:07<4:50:57, 18.20s/it] 53%|█████▎    | 1080/2038 [24:27<4:55:21, 18.50s/it]                                                     {'loss': 0.1798, 'grad_norm': 0.027457265183329582, 'learning_rate': 4.8532388663967616e-05, 'entropy': 0.18033381551504135, 'num_tokens': 15836484.0, 'mean_token_accuracy': 0.9389700353145599, 'epoch': 0.53}
 53%|█████▎    | 1080/2038 [24:27<4:55:21, 18.50s/it] 53%|█████▎    | 1081/2038 [24:45<4:53:03, 18.37s/it] 53%|█████▎    | 1082/2038 [25:03<4:51:14, 18.28s/it] 53%|█████▎    | 1083/2038 [25:21<4:49:47, 18.21s/it] 53%|█████▎    | 1084/2038 [25:39<4:49:12, 18.19s/it] 53%|█████▎    | 1085/2038 [25:57<4:48:31, 18.17s/it] 53%|█████▎    | 1086/2038 [26:16<4:53:44, 18.51s/it] 53%|█████▎    | 1087/2038 [26:34<4:51:13, 18.37s/it] 53%|█████▎    | 1088/2038 [26:53<4:49:47, 18.30s/it] 53%|█████▎    | 1089/2038 [27:11<4:48:52, 18.26s/it] 53%|█████▎    | 1090/2038 [27:29<4:47:34, 18.20s/it]                                                     {'loss': 0.1789, 'grad_norm': 0.02621113881468773, 'learning_rate': 4.802631578947368e-05, 'entropy': 0.17827982753515242, 'num_tokens': 17824703.0, 'mean_token_accuracy': 0.9392505347728729, 'epoch': 0.53}
 53%|█████▎    | 1090/2038 [27:29<4:47:34, 18.20s/it] 54%|█████▎    | 1091/2038 [27:47<4:46:32, 18.16s/it] 54%|█████▎    | 1092/2038 [28:05<4:46:50, 18.19s/it] 54%|█████▎    | 1093/2038 [28:24<4:50:08, 18.42s/it] 54%|█████▎    | 1094/2038 [28:42<4:48:39, 18.35s/it] 54%|█████▎    | 1095/2038 [29:00<4:47:18, 18.28s/it] 54%|█████▍    | 1096/2038 [29:19<4:46:34, 18.25s/it] 54%|█████▍    | 1097/2038 [29:37<4:46:04, 18.24s/it] 54%|█████▍    | 1098/2038 [29:55<4:45:27, 18.22s/it] 54%|█████▍    | 1099/2038 [30:14<4:50:10, 18.54s/it] 54%|█████▍    | 1100/2038 [30:32<4:48:19, 18.44s/it]                                                     {'loss': 0.1828, 'grad_norm': 0.027065526694059372, 'learning_rate': 4.7520242914979756e-05, 'entropy': 0.18347689658403396, 'num_tokens': 19809155.0, 'mean_token_accuracy': 0.9382651448249817, 'epoch': 0.54}
 54%|█████▍    | 1100/2038 [30:32<4:48:19, 18.44s/it] 54%|█████▍    | 1101/2038 [30:51<4:46:47, 18.36s/it] 54%|█████▍    | 1102/2038 [31:09<4:45:36, 18.31s/it] 54%|█████▍    | 1103/2038 [31:27<4:44:20, 18.25s/it] 54%|█████▍    | 1104/2038 [31:45<4:43:40, 18.22s/it] 54%|█████▍    | 1105/2038 [32:03<4:44:10, 18.27s/it] 54%|█████▍    | 1106/2038 [32:22<4:46:56, 18.47s/it] 54%|█████▍    | 1107/2038 [32:41<4:45:16, 18.39s/it] 54%|█████▍    | 1108/2038 [32:59<4:43:53, 18.32s/it] 54%|█████▍    | 1109/2038 [33:17<4:42:59, 18.28s/it] 54%|█████▍    | 1110/2038 [33:35<4:42:08, 18.24s/it]                                                     {'loss': 0.1806, 'grad_norm': 0.027471622452139854, 'learning_rate': 4.7014170040485836e-05, 'entropy': 0.1809235468506813, 'num_tokens': 21790337.0, 'mean_token_accuracy': 0.9389594972133637, 'epoch': 0.54}
 54%|█████▍    | 1110/2038 [33:35<4:42:08, 18.24s/it] 55%|█████▍    | 1111/2038 [33:53<4:41:26, 18.22s/it] 55%|█████▍    | 1112/2038 [34:13<4:46:16, 18.55s/it] 55%|█████▍    | 1113/2038 [34:31<4:43:58, 18.42s/it] 55%|█████▍    | 1114/2038 [34:49<4:42:25, 18.34s/it] 55%|█████▍    | 1115/2038 [35:07<4:41:24, 18.29s/it] 55%|█████▍    | 1116/2038 [35:25<4:40:25, 18.25s/it] 55%|█████▍    | 1117/2038 [35:43<4:39:40, 18.22s/it] 55%|█████▍    | 1118/2038 [36:02<4:42:03, 18.40s/it] 55%|█████▍    | 1119/2038 [36:21<4:42:51, 18.47s/it] 55%|█████▍    | 1120/2038 [36:39<4:41:01, 18.37s/it]                                                     {'loss': 0.1811, 'grad_norm': 0.027303269132971764, 'learning_rate': 4.65080971659919e-05, 'entropy': 0.18168720006942748, 'num_tokens': 23767531.0, 'mean_token_accuracy': 0.9387675762176514, 'epoch': 0.55}
 55%|█████▍    | 1120/2038 [36:39<4:41:01, 18.37s/it] 55%|█████▌    | 1121/2038 [36:57<4:39:45, 18.30s/it] 55%|█████▌    | 1122/2038 [37:15<4:38:10, 18.22s/it] 55%|█████▌    | 1123/2038 [37:33<4:37:03, 18.17s/it] 55%|█████▌    | 1124/2038 [37:51<4:37:43, 18.23s/it] 55%|█████▌    | 1125/2038 [38:10<4:38:57, 18.33s/it] 55%|█████▌    | 1126/2038 [38:29<4:39:31, 18.39s/it] 55%|█████▌    | 1127/2038 [38:47<4:38:21, 18.33s/it] 55%|█████▌    | 1128/2038 [39:05<4:37:07, 18.27s/it] 55%|█████▌    | 1129/2038 [39:23<4:35:41, 18.20s/it] 55%|█████▌    | 1130/2038 [39:41<4:34:59, 18.17s/it]                                                     {'loss': 0.184, 'grad_norm': 0.026926591992378235, 'learning_rate': 4.600202429149798e-05, 'entropy': 0.18442529886960984, 'num_tokens': 25775198.0, 'mean_token_accuracy': 0.9378111302852631, 'epoch': 0.55}
 55%|█████▌    | 1130/2038 [39:41<4:34:59, 18.17s/it] 55%|█████▌    | 1131/2038 [40:00<4:37:35, 18.36s/it] 56%|█████▌    | 1132/2038 [40:18<4:38:13, 18.43s/it] 56%|█████▌    | 1133/2038 [40:37<4:36:31, 18.33s/it] 56%|█████▌    | 1134/2038 [40:55<4:35:21, 18.28s/it] 56%|█████▌    | 1135/2038 [41:13<4:34:03, 18.21s/it] 56%|█████▌    | 1136/2038 [41:31<4:33:43, 18.21s/it] 56%|█████▌    | 1137/2038 [41:49<4:34:28, 18.28s/it] 56%|█████▌    | 1138/2038 [42:08<4:35:38, 18.38s/it] 56%|█████▌    | 1139/2038 [42:26<4:35:50, 18.41s/it] 56%|█████▌    | 1140/2038 [42:45<4:34:10, 18.32s/it]                                                     {'loss': 0.1823, 'grad_norm': 0.027883421629667282, 'learning_rate': 4.549595141700405e-05, 'entropy': 0.18245336264371873, 'num_tokens': 27771277.0, 'mean_token_accuracy': 0.9382540881633759, 'epoch': 0.56}
 56%|█████▌    | 1140/2038 [42:45<4:34:10, 18.32s/it] 56%|█████▌    | 1141/2038 [43:03<4:32:59, 18.26s/it] 56%|█████▌    | 1142/2038 [43:21<4:32:26, 18.24s/it] 56%|█████▌    | 1143/2038 [43:39<4:31:50, 18.22s/it] 56%|█████▌    | 1144/2038 [43:58<4:34:24, 18.42s/it]