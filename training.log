W0119 23:15:25.269000 6012 torch/distributed/run.py:803] 
W0119 23:15:25.269000 6012 torch/distributed/run.py:803] *****************************************
W0119 23:15:25.269000 6012 torch/distributed/run.py:803] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0119 23:15:25.269000 6012 torch/distributed/run.py:803] *****************************************
[Rank 1] ============================================================[Rank 2] ============================================================
[Rank 0] ============================================================
[Rank 1] INIT: rank=1/8

[Rank 2] INIT: rank=2/8
[Rank 1] ============================================================[Rank 0] INIT: rank=0/8
[Rank 2] ============================================================

[Rank 1] MODEL: Qwen/Qwen2.5-Coder-32B-Instruct[Rank 0] ============================================================

[Rank 2] MODEL: Qwen/Qwen2.5-Coder-32B-Instruct[Rank 1] MEMORY: no RLIMIT_AS set (CUDA needs large virtual address space)

[Rank 0] MODEL: Qwen/Qwen2.5-Coder-32B-Instruct[Rank 2] MEMORY: no RLIMIT_AS set (CUDA needs large virtual address space)[Rank 1] LOCK: waiting for exclusive access...

[Rank 0] MEMORY: no RLIMIT_AS set (CUDA needs large virtual address space)
[Rank 2] LOCK: waiting for exclusive access...

[Rank 0] LOCK: waiting for exclusive access...
[Rank 1] LOCK: acquired! Starting initialization...
[Rank 4] ============================================================
[Rank 4] INIT: rank=4/8
[Rank 4] ============================================================
[Rank 4] MODEL: Qwen/Qwen2.5-Coder-32B-Instruct
[Rank 4] MEMORY: no RLIMIT_AS set (CUDA needs large virtual address space)
[Rank 4] LOCK: waiting for exclusive access...
[Rank 3] ============================================================
[Rank 3] INIT: rank=3/8
[Rank 3] ============================================================
[Rank 3] MODEL: Qwen/Qwen2.5-Coder-32B-Instruct
[Rank 3] MEMORY: no RLIMIT_AS set (CUDA needs large virtual address space)
[Rank 3] LOCK: waiting for exclusive access...
[Rank 5] ============================================================
[Rank 5] INIT: rank=5/8
[Rank 5] ============================================================
[Rank 5] MODEL: Qwen/Qwen2.5-Coder-32B-Instruct
[Rank 5] MEMORY: no RLIMIT_AS set (CUDA needs large virtual address space)
[Rank 5] LOCK: waiting for exclusive access...
[Rank 7] ============================================================
[Rank 7] INIT: rank=7/8
[Rank 7] ============================================================
[Rank 7] MODEL: Qwen/Qwen2.5-Coder-32B-Instruct
[Rank 7] MEMORY: no RLIMIT_AS set (CUDA needs large virtual address space)
[Rank 7] LOCK: waiting for exclusive access...
[Rank 6] ============================================================
[Rank 6] INIT: rank=6/8
[Rank 6] ============================================================
[Rank 6] MODEL: Qwen/Qwen2.5-Coder-32B-Instruct
[Rank 6] MEMORY: no RLIMIT_AS set (CUDA needs large virtual address space)
[Rank 6] LOCK: waiting for exclusive access...
[Rank 1] [MEM:START] RAM: 44,814MB / 2,216,624MB (2.0%)
[Rank 1] IMPORT: torch...
[Rank 1] IMPORT: torch OK (v2.9.1+cu128)
[Rank 1] DEVICE: cuda:1
[Rank 1] CUDA: warming up...
[Rank 1] CUDA: ready
[Rank 1] IMPORT: transformers...
[Rank 1] IMPORT: transformers OK
[Rank 1] IMPORT: peft...
[Rank 1] IMPORT: peft OK
[Rank 1] IMPORT: trl...
[Rank 1] IMPORT: trl OK
[Rank 1] IMPORT: datasets...
[Rank 1] IMPORT: datasets OK
[Rank 1] [MEM:POST-IMPORT] RAM: 45,541MB / 2,216,624MB (2.1%)
[Rank 1] LOCK: released[Rank 0] LOCK: acquired! Starting initialization...

[Rank 0] [MEM:START] RAM: 45,539MB / 2,216,624MB (2.1%)
[Rank 0] [MEM:START] GPU0: 4MB / 275040MB
[Rank 0] [MEM:START] GPU1: 726MB / 275040MB
[Rank 0] [MEM:START] GPU2: 4MB / 275040MB
[Rank 0] [MEM:START] GPU3: 4MB / 275040MB
[Rank 0] [MEM:START] GPU4: 4MB / 275040MB
[Rank 0] [MEM:START] GPU5: 4MB / 275040MB
[Rank 0] [MEM:START] GPU6: 4MB / 275040MB
[Rank 0] [MEM:START] GPU7: 4MB / 275040MB
[Rank 0] IMPORT: torch...
[Rank 1] DISTRIBUTED: initializing...
[Rank 1] DISTRIBUTED: OK (rank=1)
[Rank 1] TOKENIZER: loading...
[Rank 0] IMPORT: torch OK (v2.9.1+cu128)
[Rank 1] TOKENIZER: OK
[Rank 1] DATASET: loading...
[Rank 1] DATASET: 912960 examples
[Rank 0] DEVICE: cuda:0
[Rank 0] CUDA: warming up...
[Rank 0] CUDA: ready
[Rank 0] IMPORT: transformers...
[Rank 1] [MEM:POST-DATASET] RAM: 46,721MB / 2,216,624MB (2.1%)
[Rank 0] IMPORT: transformers OK
[Rank 0] IMPORT: peft...
[Rank 0] IMPORT: peft OK
[Rank 0] IMPORT: trl...
[Rank 0] IMPORT: trl OK
[Rank 0] IMPORT: datasets...
[Rank 0] IMPORT: datasets OK
[Rank 0] [MEM:POST-IMPORT] RAM: 46,868MB / 2,216,624MB (2.1%)
[Rank 0] [MEM:POST-IMPORT] GPU0: 726MB / 275040MB
[Rank 0] [MEM:POST-IMPORT] GPU1: 726MB / 275040MB
[Rank 0] [MEM:POST-IMPORT] GPU2: 4MB / 275040MB
[Rank 0] [MEM:POST-IMPORT] GPU3: 4MB / 275040MB
[Rank 0] [MEM:POST-IMPORT] GPU4: 4MB / 275040MB
[Rank 0] [MEM:POST-IMPORT] GPU5: 4MB / 275040MB
[Rank 0] [MEM:POST-IMPORT] GPU6: 4MB / 275040MB
[Rank 0] [MEM:POST-IMPORT] GPU7: 4MB / 275040MB
[Rank 0] LOCK: released[Rank 2] LOCK: acquired! Starting initialization...

[Rank 2] [MEM:START] RAM: 46,886MB / 2,216,624MB (2.1%)
[Rank 2] IMPORT: torch...
[Rank 0] DISTRIBUTED: initializing...
[Rank 0] DISTRIBUTED: OK (rank=0)
[Rank 0] TOKENIZER: loading...
[Rank 2] IMPORT: torch OK (v2.9.1+cu128)
[Rank 0] TOKENIZER: OK
[Rank 0] DATASET: loading...
[Rank 0] DATASET: 912960 examples
[Rank 2] DEVICE: cuda:2
[Rank 2] CUDA: warming up...
[Rank 2] CUDA: ready
[Rank 2] IMPORT: transformers...
[Rank 0] [MEM:POST-DATASET] RAM: 47,435MB / 2,216,624MB (2.1%)
[Rank 0] [MEM:POST-DATASET] GPU0: 726MB / 275040MB
[Rank 0] [MEM:POST-DATASET] GPU1: 726MB / 275040MB
[Rank 0] [MEM:POST-DATASET] GPU2: 726MB / 275040MB
[Rank 0] [MEM:POST-DATASET] GPU3: 4MB / 275040MB
[Rank 0] [MEM:POST-DATASET] GPU4: 4MB / 275040MB
[Rank 0] [MEM:POST-DATASET] GPU5: 4MB / 275040MB
[Rank 0] [MEM:POST-DATASET] GPU6: 4MB / 275040MB
[Rank 0] [MEM:POST-DATASET] GPU7: 4MB / 275040MB
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
[rank0]:[W119 23:15:44.235765229 ProcessGroupNCCL.cpp:5072] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()
[Rank 2] IMPORT: transformers OK
[Rank 2] IMPORT: peft...
[Rank 2] IMPORT: peft OK
[Rank 2] IMPORT: trl...
[Rank 2] IMPORT: trl OK
[Rank 2] IMPORT: datasets...
[Rank 2] IMPORT: datasets OK
[Rank 2] [MEM:POST-IMPORT] RAM: 48,294MB / 2,216,624MB (2.2%)
[Rank 2] LOCK: released
[Rank 4] LOCK: acquired! Starting initialization...
[Rank 4] [MEM:START] RAM: 48,294MB / 2,216,624MB (2.2%)
[Rank 4] IMPORT: torch...
[Rank 2] DISTRIBUTED: initializing...
[Rank 2] DISTRIBUTED: OK (rank=2)
[Rank 2] TOKENIZER: loading...
[Rank 4] IMPORT: torch OK (v2.9.1+cu128)
[Rank 2] TOKENIZER: OK
[Rank 2] DATASET: loading...
[Rank 2] DATASET: 912960 examples
[Rank 2] [MEM:POST-DATASET] RAM: 48,704MB / 2,216,624MB (2.2%)
[Rank 4] DEVICE: cuda:4
[Rank 4] CUDA: warming up...
[Rank 4] CUDA: ready
[Rank 4] IMPORT: transformers...
[Rank 4] IMPORT: transformers OK
[Rank 4] IMPORT: peft...
[Rank 4] IMPORT: peft OK
[Rank 4] IMPORT: trl...
[Rank 4] IMPORT: trl OK
[Rank 4] IMPORT: datasets...
[Rank 4] IMPORT: datasets OK
[Rank 4] [MEM:POST-IMPORT] RAM: 49,247MB / 2,216,624MB (2.2%)
[Rank 4] LOCK: released
[Rank 3] LOCK: acquired! Starting initialization...
[Rank 3] [MEM:START] RAM: 49,250MB / 2,216,624MB (2.2%)
[Rank 3] IMPORT: torch...
[Rank 4] DISTRIBUTED: initializing...
[Rank 4] DISTRIBUTED: OK (rank=4)
[Rank 4] TOKENIZER: loading...
[Rank 3] IMPORT: torch OK (v2.9.1+cu128)
[Rank 4] TOKENIZER: OK
[Rank 4] DATASET: loading...
[Rank 4] DATASET: 912960 examples
[Rank 3] DEVICE: cuda:3
[Rank 3] CUDA: warming up...
[Rank 3] CUDA: ready
[Rank 3] IMPORT: transformers...
[Rank 4] [MEM:POST-DATASET] RAM: 49,802MB / 2,216,624MB (2.2%)
[Rank 3] IMPORT: transformers OK
[Rank 3] IMPORT: peft...
[Rank 3] IMPORT: peft OK
[Rank 3] IMPORT: trl...
[Rank 3] IMPORT: trl OK
[Rank 3] IMPORT: datasets...
[Rank 3] IMPORT: datasets OK
[Rank 3] [MEM:POST-IMPORT] RAM: 50,250MB / 2,216,624MB (2.3%)
[Rank 3] LOCK: released[Rank 5] LOCK: acquired! Starting initialization...

[Rank 5] [MEM:START] RAM: 50,246MB / 2,216,624MB (2.3%)
[Rank 5] IMPORT: torch...
[Rank 3] DISTRIBUTED: initializing...
[Rank 3] DISTRIBUTED: OK (rank=3)
[Rank 3] TOKENIZER: loading...
[Rank 5] IMPORT: torch OK (v2.9.1+cu128)
[Rank 3] TOKENIZER: OK
[Rank 3] DATASET: loading...
[Rank 3] DATASET: 912960 examples
[Rank 3] [MEM:POST-DATASET] RAM: 50,763MB / 2,216,624MB (2.3%)
[Rank 5] DEVICE: cuda:5
[Rank 5] CUDA: warming up...
[Rank 5] CUDA: ready
[Rank 5] IMPORT: transformers...
[Rank 5] IMPORT: transformers OK
[Rank 5] IMPORT: peft...
[Rank 5] IMPORT: peft OK
[Rank 5] IMPORT: trl...
[Rank 5] IMPORT: trl OK
[Rank 5] IMPORT: datasets...
[Rank 5] IMPORT: datasets OK
[Rank 5] [MEM:POST-IMPORT] RAM: 51,190MB / 2,216,624MB (2.3%)
[Rank 5] LOCK: released[Rank 7] LOCK: acquired! Starting initialization...

[Rank 7] [MEM:START] RAM: 51,192MB / 2,216,624MB (2.3%)
[Rank 7] IMPORT: torch...
[Rank 5] DISTRIBUTED: initializing...
[Rank 5] DISTRIBUTED: OK (rank=5)
[Rank 5] TOKENIZER: loading...
[Rank 7] IMPORT: torch OK (v2.9.1+cu128)
[Rank 7] DEVICE: cuda:7
[Rank 7] CUDA: warming up...
[Rank 7] CUDA: ready
[Rank 7] IMPORT: transformers...
[Rank 5] TOKENIZER: OK
[Rank 5] DATASET: loading...
[Rank 5] DATASET: 912960 examples
[Rank 5] [MEM:POST-DATASET] RAM: 51,751MB / 2,216,624MB (2.3%)
[Rank 7] IMPORT: transformers OK
[Rank 7] IMPORT: peft...
[Rank 7] IMPORT: peft OK
[Rank 7] IMPORT: trl...
[Rank 7] IMPORT: trl OK
[Rank 7] IMPORT: datasets...
[Rank 7] IMPORT: datasets OK
[Rank 7] [MEM:POST-IMPORT] RAM: 52,156MB / 2,216,624MB (2.4%)
[Rank 7] LOCK: released[Rank 6] LOCK: acquired! Starting initialization...

[Rank 6] [MEM:START] RAM: 52,152MB / 2,216,624MB (2.4%)
[Rank 6] IMPORT: torch...
[Rank 7] DISTRIBUTED: initializing...
[Rank 7] DISTRIBUTED: OK (rank=7)
[Rank 7] TOKENIZER: loading...
[Rank 6] IMPORT: torch OK (v2.9.1+cu128)
[Rank 7] TOKENIZER: OK
[Rank 7] DATASET: loading...
[Rank 7] DATASET: 912960 examples
[Rank 7] [MEM:POST-DATASET] RAM: 52,639MB / 2,216,624MB (2.4%)
[Rank 6] DEVICE: cuda:6
[Rank 6] CUDA: warming up...
[Rank 6] CUDA: ready
[Rank 6] IMPORT: transformers...
[Rank 6] IMPORT: transformers OK
[Rank 6] IMPORT: peft...
[Rank 6] IMPORT: peft OK
[Rank 6] IMPORT: trl...
[Rank 6] IMPORT: trl OK
[Rank 6] IMPORT: datasets...
[Rank 6] IMPORT: datasets OK
[Rank 6] [MEM:POST-IMPORT] RAM: 53,134MB / 2,216,624MB (2.4%)
[Rank 6] LOCK: released
[Rank 6] DISTRIBUTED: initializing...
[Rank 6] DISTRIBUTED: OK (rank=6)
[Rank 6] TOKENIZER: loading...
[Rank 6] TOKENIZER: OK
[Rank 6] DATASET: loading...
[Rank 6] DATASET: 912960 examples
[Rank 6] [MEM:POST-DATASET] RAM: 53,205MB / 2,216,624MB (2.4%)
[Rank 5] MODEL: waiting for lock to load model...[Rank 4] MODEL: waiting for lock to load model...[Rank 2] MODEL: waiting for lock to load model...[Rank 7] MODEL: waiting for lock to load model...[Rank 1] MODEL: waiting for lock to load model...[Rank 0] MODEL: waiting for lock to load model...[Rank 3] MODEL: waiting for lock to load model...[Rank 6] MODEL: waiting for lock to load model...







[Rank 4] MODEL: loading to CPU (FSDP will handle GPU sharding)...
[Rank 4] flash_attention_2 not available, using sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards:  21%|██▏       | 3/14 [00:00<00:00, 24.02it/s]Loading checkpoint shards:  43%|████▎     | 6/14 [00:00<00:00, 24.20it/s]Loading checkpoint shards:  64%|██████▍   | 9/14 [00:00<00:00, 24.48it/s]Loading checkpoint shards:  86%|████████▌ | 12/14 [00:00<00:00, 24.97it/s]Loading checkpoint shards: 100%|██████████| 14/14 [00:00<00:00, 26.58it/s]
[Rank 4] MODEL: loaded to CPU!
[Rank 4] [MEM:POST-CPU-LOAD] RAM: 53,755MB / 2,216,624MB (2.4%)
[Rank 4] [MEM:POST-GC] RAM: 53,755MB / 2,216,624MB (2.4%)
[Rank 5] MODEL: loading to CPU (FSDP will handle GPU sharding)...
[Rank 5] flash_attention_2 not available, using sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 14/14 [00:00<00:00, 640.13it/s]
[Rank 5] MODEL: loaded to CPU!
[Rank 5] [MEM:POST-CPU-LOAD] RAM: 53,758MB / 2,216,624MB (2.4%)
[Rank 5] [MEM:POST-GC] RAM: 53,758MB / 2,216,624MB (2.4%)
[Rank 2] MODEL: loading to CPU (FSDP will handle GPU sharding)...
[Rank 2] flash_attention_2 not available, using sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 14/14 [00:00<00:00, 658.19it/s]
[Rank 2] MODEL: loaded to CPU!
[Rank 2] [MEM:POST-CPU-LOAD] RAM: 53,753MB / 2,216,624MB (2.4%)
[Rank 2] [MEM:POST-GC] RAM: 53,753MB / 2,216,624MB (2.4%)
[Rank 7] MODEL: loading to CPU (FSDP will handle GPU sharding)...
[Rank 7] flash_attention_2 not available, using sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 14/14 [00:00<00:00, 633.03it/s]
[Rank 7] MODEL: loaded to CPU!
[Rank 7] [MEM:POST-CPU-LOAD] RAM: 53,759MB / 2,216,624MB (2.4%)
[Rank 7] [MEM:POST-GC] RAM: 53,760MB / 2,216,624MB (2.4%)
[Rank 1] MODEL: loading to CPU (FSDP will handle GPU sharding)...
[Rank 1] flash_attention_2 not available, using sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 14/14 [00:00<00:00, 623.60it/s]
[Rank 1] MODEL: loaded to CPU!
[Rank 1] [MEM:POST-CPU-LOAD] RAM: 53,762MB / 2,216,624MB (2.4%)
[Rank 1] [MEM:POST-GC] RAM: 53,762MB / 2,216,624MB (2.4%)
[Rank 3] MODEL: loading to CPU (FSDP will handle GPU sharding)...
[Rank 3] flash_attention_2 not available, using sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 14/14 [00:00<00:00, 612.26it/s]
[Rank 3] MODEL: loaded to CPU!
[Rank 3] [MEM:POST-CPU-LOAD] RAM: 53,762MB / 2,216,624MB (2.4%)
[Rank 3] [MEM:POST-GC] RAM: 53,762MB / 2,216,624MB (2.4%)
[Rank 0] MODEL: loading to CPU (FSDP will handle GPU sharding)...
[Rank 0] flash_attention_2 not available, using sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 14/14 [00:00<00:00, 603.35it/s]
[Rank 0] MODEL: loaded to CPU!
[Rank 0] [MEM:POST-CPU-LOAD] RAM: 53,765MB / 2,216,624MB (2.4%)
[Rank 0] [MEM:POST-CPU-LOAD] GPU0: 1784MB / 275040MB
[Rank 0] [MEM:POST-CPU-LOAD] GPU1: 1784MB / 275040MB
[Rank 0] [MEM:POST-CPU-LOAD] GPU2: 1784MB / 275040MB
[Rank 0] [MEM:POST-CPU-LOAD] GPU3: 1784MB / 275040MB
[Rank 0] [MEM:POST-CPU-LOAD] GPU4: 1784MB / 275040MB
[Rank 0] [MEM:POST-CPU-LOAD] GPU5: 1784MB / 275040MB
[Rank 0] [MEM:POST-CPU-LOAD] GPU6: 1784MB / 275040MB
[Rank 0] [MEM:POST-CPU-LOAD] GPU7: 1784MB / 275040MB
[Rank 0] [MEM:POST-GC] RAM: 53,784MB / 2,216,624MB (2.4%)
[Rank 0] [MEM:POST-GC] GPU0: 1784MB / 275040MB
[Rank 0] [MEM:POST-GC] GPU1: 1784MB / 275040MB
[Rank 0] [MEM:POST-GC] GPU2: 1784MB / 275040MB
[Rank 0] [MEM:POST-GC] GPU3: 1784MB / 275040MB
[Rank 0] [MEM:POST-GC] GPU4: 1784MB / 275040MB
[Rank 0] [MEM:POST-GC] GPU5: 1784MB / 275040MB
[Rank 0] [MEM:POST-GC] GPU6: 1784MB / 275040MB
[Rank 0] [MEM:POST-GC] GPU7: 1784MB / 275040MB
[Rank 6] MODEL: loading to CPU (FSDP will handle GPU sharding)...
[Rank 6] flash_attention_2 not available, using sdpa
`torch_dtype` is deprecated! Use `dtype` instead!
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 14/14 [00:00<00:00, 620.07it/s]
[Rank 6] MODEL: loaded to CPU!
[Rank 6] [MEM:POST-CPU-LOAD] RAM: 53,784MB / 2,216,624MB (2.4%)
[Rank 6] [MEM:POST-GC] RAM: 53,782MB / 2,216,624MB (2.4%)
[Rank 0] MODEL: all ranks have loaded model
[Rank 0] LORA: configuring...
[Rank 1] MODEL: all ranks have loaded model
[Rank 1] LORA: configuring...
[Rank 2] MODEL: all ranks have loaded model
[Rank 2] LORA: configuring...
[Rank 3] MODEL: all ranks have loaded model
[Rank 3] LORA: configuring...
[Rank 4] MODEL: all ranks have loaded model[Rank 6] MODEL: all ranks have loaded model
[Rank 5] MODEL: all ranks have loaded model[Rank 4] LORA: configuring...[Rank 7] MODEL: all ranks have loaded model

[Rank 6] LORA: configuring...

[Rank 5] LORA: configuring...

[Rank 7] LORA: configuring...
[Rank 4] [MEM:POST-LORA] RAM: 68,085MB / 2,216,624MB (3.1%)
[Rank 4] CONFIG: creating SFTConfig (memory-optimized)...
[Rank 2] [MEM:POST-LORA] RAM: 68,101MB / 2,216,624MB (3.1%)
[Rank 2] CONFIG: creating SFTConfig (memory-optimized)...
[Rank 5] [MEM:POST-LORA] RAM: 68,356MB / 2,216,624MB (3.1%)
[Rank 5] CONFIG: creating SFTConfig (memory-optimized)...
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[Rank 4] CONFIG: batch_size=56, grad_accum=1, seq_len=1024
[Rank 4] CONFIG: OK
[Rank 4] TRAINER: creating...
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[Rank 2] CONFIG: batch_size=56, grad_accum=1, seq_len=1024
[Rank 2] CONFIG: OK
[Rank 2] TRAINER: creating...
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[Rank 5] CONFIG: batch_size=56, grad_accum=1, seq_len=1024
[Rank 5] CONFIG: OK
[Rank 5] TRAINER: creating...
trainable params: 536,870,912 || all params: 33,300,747,264 || trainable%: 1.6122
[Rank 7] [MEM:POST-LORA] RAM: 71,957MB / 2,216,624MB (3.2%)
[Rank 7] CONFIG: creating SFTConfig (memory-optimized)...
[Rank 6] [MEM:POST-LORA] RAM: 71,957MB / 2,216,624MB (3.2%)
[Rank 6] CONFIG: creating SFTConfig (memory-optimized)...
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[Rank 7] CONFIG: batch_size=56, grad_accum=1, seq_len=1024
[Rank 7] CONFIG: OK
[Rank 7] TRAINER: creating...
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[Rank 6] CONFIG: batch_size=56, grad_accum=1, seq_len=1024
[Rank 6] CONFIG: OK
[Rank 6] TRAINER: creating...
[Rank 1] [MEM:POST-LORA] RAM: 72,130MB / 2,216,624MB (3.3%)
[Rank 1] CONFIG: creating SFTConfig (memory-optimized)...
[Rank 3] [MEM:POST-LORA] RAM: 72,147MB / 2,216,624MB (3.3%)
[Rank 3] CONFIG: creating SFTConfig (memory-optimized)...
[Rank 0] [MEM:POST-LORA] RAM: 72,185MB / 2,216,624MB (3.3%)
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[Rank 1] CONFIG: batch_size=56, grad_accum=1, seq_len=1024
[Rank 1] CONFIG: OK
[Rank 1] TRAINER: creating...
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[Rank 3] CONFIG: batch_size=56, grad_accum=1, seq_len=1024
[Rank 3] CONFIG: OK
[Rank 3] TRAINER: creating...
[Rank 0] [MEM:POST-LORA] GPU0: 1784MB / 275040MB
[Rank 0] [MEM:POST-LORA] GPU1: 1784MB / 275040MB
[Rank 0] [MEM:POST-LORA] GPU2: 1784MB / 275040MB
[Rank 0] [MEM:POST-LORA] GPU3: 1784MB / 275040MB
[Rank 0] [MEM:POST-LORA] GPU4: 1784MB / 275040MB
[Rank 0] [MEM:POST-LORA] GPU5: 1784MB / 275040MB
[Rank 0] [MEM:POST-LORA] GPU6: 1784MB / 275040MB
[Rank 0] [MEM:POST-LORA] GPU7: 1784MB / 275040MB
[Rank 0] CONFIG: creating SFTConfig (memory-optimized)...
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[Rank 0] CONFIG: batch_size=56, grad_accum=1, seq_len=1024
[Rank 0] CONFIG: OK
[Rank 0] TRAINER: creating...
[Rank 0] TRAINER: OK
[Rank 0] [MEM:POST-TRAINER] RAM: 72,894MB / 2,216,624MB (3.3%)
[Rank 4] TRAINER: OK
[Rank 4] [MEM:POST-TRAINER] RAM: 72,897MB / 2,216,624MB (3.3%)
[Rank 5] TRAINER: OK
[Rank 5] [MEM:POST-TRAINER] RAM: 72,897MB / 2,216,624MB (3.3%)
[Rank 6] TRAINER: OK
[Rank 6] [MEM:POST-TRAINER] RAM: 72,896MB / 2,216,624MB (3.3%)
[Rank 2] TRAINER: OK
[Rank 2] [MEM:POST-TRAINER] RAM: 72,896MB / 2,216,624MB (3.3%)
[Rank 7] TRAINER: OK
[Rank 7] [MEM:POST-TRAINER] RAM: 72,896MB / 2,216,624MB (3.3%)
[Rank 1] TRAINER: OK
[Rank 1] [MEM:POST-TRAINER] RAM: 72,896MB / 2,216,624MB (3.3%)
[Rank 3] TRAINER: OK
[Rank 3] [MEM:POST-TRAINER] RAM: 72,895MB / 2,216,624MB (3.3%)
[Rank 0] [MEM:POST-TRAINER] GPU0: 1784MB / 275040MB
[Rank 0] [MEM:POST-TRAINER] GPU1: 1784MB / 275040MB
[Rank 0] [MEM:POST-TRAINER] GPU2: 1784MB / 275040MB
[Rank 0] [MEM:POST-TRAINER] GPU3: 1784MB / 275040MB
[Rank 0] [MEM:POST-TRAINER] GPU4: 1784MB / 275040MB
[Rank 0] [MEM:POST-TRAINER] GPU5: 1784MB / 275040MB
[Rank 0] [MEM:POST-TRAINER] GPU6: 1784MB / 275040MB
[Rank 0] [MEM:POST-TRAINER] GPU7: 1784MB / 275040MB
[Rank 4] TRAIN: starting...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.
[Rank 2] TRAIN: starting...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.
[Rank 5] TRAIN: starting...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.
[Rank 6] TRAIN: starting...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.
[Rank 7] TRAIN: starting...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.
[Rank 3] TRAIN: starting...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.
[Rank 1] TRAIN: starting...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.
[Rank 0] TRAIN: resuming from ./output/checkpoint-1000
[Rank 0] TRAIN: starting...
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151645}.
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
  0%|          | 0/2038 [00:00<?, ?it/s] 49%|████▉     | 1001/2038 [00:20<00:21, 48.47it/s] 49%|████▉     | 1001/2038 [00:31<00:21, 48.47it/s] 49%|████▉     | 1002/2038 [00:39<00:48, 21.16it/s] 49%|████▉     | 1003/2038 [00:58<01:28, 11.73it/s] 49%|████▉     | 1004/2038 [01:16<02:22,  7.26it/s] 49%|████▉     | 1005/2038 [01:34<03:39,  4.71it/s] 49%|████▉     | 1006/2038 [01:52<05:27,  3.15it/s] 49%|████▉     | 1007/2038 [02:10<07:59,  2.15it/s] 49%|████▉     | 1008/2038 [02:28<11:31,  1.49it/s] 50%|████▉     | 1009/2038 [02:47<16:38,  1.03it/s] 50%|████▉     | 1010/2038 [03:06<23:23,  1.37s/it]                                                   {'loss': 0.1806, 'grad_norm': 0.026610050350427628, 'learning_rate': 5.207489878542511e-05, 'entropy': 0.18099832832813262, 'num_tokens': 1972424.0, 'mean_token_accuracy': 0.9389932453632355, 'epoch': 0.5}
 50%|████▉     | 1010/2038 [03:06<23:23,  1.37s/it] 50%|████▉     | 1011/2038 [03:24<32:29,  1.90s/it] 50%|████▉     | 1012/2038 [03:42<44:32,  2.60s/it] 50%|████▉     | 1013/2038 [04:00<1:00:03,  3.52s/it] 50%|████▉     | 1014/2038 [04:18<1:19:02,  4.63s/it] 50%|████▉     | 1015/2038 [04:36<1:41:45,  5.97s/it] 50%|████▉     | 1016/2038 [04:55<2:08:57,  7.57s/it] 50%|████▉     | 1017/2038 [05:13<2:35:38,  9.15s/it] 50%|████▉     | 1018/2038 [05:31<3:02:39, 10.74s/it] 50%|█████     | 1019/2038 [05:50<3:28:03, 12.25s/it] 50%|█████     | 1020/2038 [06:08<3:50:10, 13.57s/it]                                                     {'loss': 0.1858, 'grad_norm': 0.025720102712512016, 'learning_rate': 5.1568825910931177e-05, 'entropy': 0.18596912622451783, 'num_tokens': 3968008.0, 'mean_token_accuracy': 0.9374460935592651, 'epoch': 0.5}
 50%|█████     | 1020/2038 [06:08<3:50:10, 13.57s/it] 50%|█████     | 1021/2038 [06:26<4:08:21, 14.65s/it] 50%|█████     | 1022/2038 [06:44<4:24:28, 15.62s/it] 50%|█████     | 1023/2038 [07:03<4:37:44, 16.42s/it] 50%|█████     | 1024/2038 [07:21<4:45:30, 16.89s/it] 50%|█████     | 1025/2038 [07:39<4:51:15, 17.25s/it] 50%|█████     | 1026/2038 [07:57<4:55:19, 17.51s/it] 50%|█████     | 1027/2038 [08:15<4:58:07, 17.69s/it] 50%|█████     | 1028/2038 [08:34<5:01:21, 17.90s/it] 50%|█████     | 1029/2038 [08:53<5:06:39, 18.24s/it] 51%|█████     | 1030/2038 [09:11<5:05:56, 18.21s/it]                                                     {'loss': 0.1789, 'grad_norm': 0.027322009205818176, 'learning_rate': 5.106275303643725e-05, 'entropy': 0.1787508338689804, 'num_tokens': 5933369.0, 'mean_token_accuracy': 0.9394914507865906, 'epoch': 0.51}
 51%|█████     | 1030/2038 [09:11<5:05:56, 18.21s/it] 51%|█████     | 1031/2038 [09:29<5:05:09, 18.18s/it] 51%|█████     | 1032/2038 [09:47<5:04:38, 18.17s/it] 51%|█████     | 1033/2038 [10:05<5:04:14, 18.16s/it] 51%|█████     | 1034/2038 [10:24<5:03:55, 18.16s/it] 51%|█████     | 1035/2038 [10:43<5:09:16, 18.50s/it] 51%|█████     | 1036/2038 [11:01<5:07:09, 18.39s/it] 51%|█████     | 1037/2038 [11:19<5:05:49, 18.33s/it] 51%|█████     | 1038/2038 [11:37<5:04:13, 18.25s/it] 51%|█████     | 1039/2038 [11:55<5:03:12, 18.21s/it] 51%|█████     | 1040/2038 [12:13<5:02:15, 18.17s/it]                                                     {'loss': 0.1833, 'grad_norm': 0.026542911306023598, 'learning_rate': 5.055668016194332e-05, 'entropy': 0.1833755075931549, 'num_tokens': 7921308.0, 'mean_token_accuracy': 0.9382711946964264, 'epoch': 0.51}
 51%|█████     | 1040/2038 [12:13<5:02:15, 18.17s/it] 51%|█████     | 1041/2038 [12:32<5:02:40, 18.21s/it] 51%|█████     | 1042/2038 [12:51<5:06:21, 18.46s/it] 51%|█████     | 1043/2038 [13:09<5:04:20, 18.35s/it] 51%|█████     | 1044/2038 [13:27<5:02:41, 18.27s/it] 51%|█████▏    | 1045/2038 [13:45<5:01:34, 18.22s/it] 51%|█████▏    | 1046/2038 [14:03<5:00:57, 18.20s/it] 51%|█████▏    | 1047/2038 [14:21<5:00:22, 18.19s/it] 51%|█████▏    | 1048/2038 [14:41<5:05:19, 18.50s/it] 51%|█████▏    | 1049/2038 [14:59<5:02:53, 18.38s/it] 52%|█████▏    | 1050/2038 [15:17<5:01:38, 18.32s/it]                                                     {'loss': 0.1804, 'grad_norm': 0.0262804813683033, 'learning_rate': 5.005060728744939e-05, 'entropy': 0.18103027790784837, 'num_tokens': 9892365.0, 'mean_token_accuracy': 0.938872492313385, 'epoch': 0.52}
 52%|█████▏    | 1050/2038 [15:17<5:01:38, 18.32s/it] 52%|█████▏    | 1051/2038 [15:35<5:00:18, 18.26s/it] 52%|█████▏    | 1052/2038 [15:53<4:58:56, 18.19s/it] 52%|█████▏    | 1053/2038 [16:11<4:58:26, 18.18s/it] 52%|█████▏    | 1054/2038 [16:30<5:03:25, 18.50s/it] 52%|█████▏    | 1055/2038 [16:49<5:01:14, 18.39s/it] 52%|█████▏    | 1056/2038 [17:07<5:00:06, 18.34s/it] 52%|█████▏    | 1057/2038 [17:25<4:58:35, 18.26s/it] 52%|█████▏    | 1058/2038 [17:43<4:58:02, 18.25s/it] 52%|█████▏    | 1059/2038 [18:01<4:56:47, 18.19s/it] 52%|█████▏    | 1060/2038 [18:20<4:59:04, 18.35s/it]                                                     {'loss': 0.1812, 'grad_norm': 0.02725955657660961, 'learning_rate': 4.954453441295547e-05, 'entropy': 0.18149595558643342, 'num_tokens': 11880665.0, 'mean_token_accuracy': 0.9387101829051971, 'epoch': 0.52}
 52%|█████▏    | 1060/2038 [18:20<4:59:04, 18.35s/it] 52%|█████▏    | 1061/2038 [18:38<5:00:20, 18.44s/it] 52%|█████▏    | 1062/2038 [18:57<4:58:21, 18.34s/it] 52%|█████▏    | 1063/2038 [19:15<4:57:38, 18.32s/it] 52%|█████▏    | 1064/2038 [19:33<4:56:36, 18.27s/it] 52%|█████▏    | 1065/2038 [19:51<4:55:38, 18.23s/it] 52%|█████▏    | 1066/2038 [20:09<4:54:39, 18.19s/it] 52%|█████▏    | 1067/2038 [20:29<4:59:43, 18.52s/it] 52%|█████▏    | 1068/2038 [20:47<4:57:04, 18.38s/it] 52%|█████▏    | 1069/2038 [21:05<4:55:38, 18.31s/it] 53%|█████▎    | 1070/2038 [21:23<4:54:47, 18.27s/it]                                                     {'loss': 0.183, 'grad_norm': 0.02705450728535652, 'learning_rate': 4.9038461538461536e-05, 'entropy': 0.18303068727254868, 'num_tokens': 13863168.0, 'mean_token_accuracy': 0.9379091560840607, 'epoch': 0.53}
 53%|█████▎    | 1070/2038 [21:23<4:54:47, 18.27s/it] 53%|█████▎    | 1071/2038 [21:41<4:54:06, 18.25s/it] 53%|█████▎    | 1072/2038 [21:59<4:53:26, 18.23s/it] 53%|█████▎    | 1073/2038 [22:18<4:55:56, 18.40s/it] 53%|█████▎    | 1074/2038 [22:37<4:56:31, 18.46s/it] 53%|█████▎    | 1075/2038 [22:55<4:54:26, 18.35s/it] 53%|█████▎    | 1076/2038 [23:13<4:53:12, 18.29s/it] 53%|█████▎    | 1077/2038 [23:31<4:52:34, 18.27s/it] 53%|█████▎    | 1078/2038 [23:49<4:51:20, 18.21s/it] 53%|█████▎    | 1079/2038 [24:07<4:50:57, 18.20s/it] 53%|█████▎    | 1080/2038 [24:27<4:55:21, 18.50s/it]                                                     {'loss': 0.1798, 'grad_norm': 0.027457265183329582, 'learning_rate': 4.8532388663967616e-05, 'entropy': 0.18033381551504135, 'num_tokens': 15836484.0, 'mean_token_accuracy': 0.9389700353145599, 'epoch': 0.53}
 53%|█████▎    | 1080/2038 [24:27<4:55:21, 18.50s/it] 53%|█████▎    | 1081/2038 [24:45<4:53:03, 18.37s/it] 53%|█████▎    | 1082/2038 [25:03<4:51:14, 18.28s/it] 53%|█████▎    | 1083/2038 [25:21<4:49:47, 18.21s/it] 53%|█████▎    | 1084/2038 [25:39<4:49:12, 18.19s/it] 53%|█████▎    | 1085/2038 [25:57<4:48:31, 18.17s/it] 53%|█████▎    | 1086/2038 [26:16<4:53:44, 18.51s/it] 53%|█████▎    | 1087/2038 [26:34<4:51:13, 18.37s/it] 53%|█████▎    | 1088/2038 [26:53<4:49:47, 18.30s/it] 53%|█████▎    | 1089/2038 [27:11<4:48:52, 18.26s/it] 53%|█████▎    | 1090/2038 [27:29<4:47:34, 18.20s/it]                                                     {'loss': 0.1789, 'grad_norm': 0.02621113881468773, 'learning_rate': 4.802631578947368e-05, 'entropy': 0.17827982753515242, 'num_tokens': 17824703.0, 'mean_token_accuracy': 0.9392505347728729, 'epoch': 0.53}
 53%|█████▎    | 1090/2038 [27:29<4:47:34, 18.20s/it] 54%|█████▎    | 1091/2038 [27:47<4:46:32, 18.16s/it] 54%|█████▎    | 1092/2038 [28:05<4:46:50, 18.19s/it] 54%|█████▎    | 1093/2038 [28:24<4:50:08, 18.42s/it] 54%|█████▎    | 1094/2038 [28:42<4:48:39, 18.35s/it] 54%|█████▎    | 1095/2038 [29:00<4:47:18, 18.28s/it] 54%|█████▍    | 1096/2038 [29:19<4:46:34, 18.25s/it] 54%|█████▍    | 1097/2038 [29:37<4:46:04, 18.24s/it] 54%|█████▍    | 1098/2038 [29:55<4:45:27, 18.22s/it] 54%|█████▍    | 1099/2038 [30:14<4:50:10, 18.54s/it] 54%|█████▍    | 1100/2038 [30:32<4:48:19, 18.44s/it]                                                     {'loss': 0.1828, 'grad_norm': 0.027065526694059372, 'learning_rate': 4.7520242914979756e-05, 'entropy': 0.18347689658403396, 'num_tokens': 19809155.0, 'mean_token_accuracy': 0.9382651448249817, 'epoch': 0.54}
 54%|█████▍    | 1100/2038 [30:32<4:48:19, 18.44s/it] 54%|█████▍    | 1101/2038 [30:51<4:46:47, 18.36s/it] 54%|█████▍    | 1102/2038 [31:09<4:45:36, 18.31s/it] 54%|█████▍    | 1103/2038 [31:27<4:44:20, 18.25s/it] 54%|█████▍    | 1104/2038 [31:45<4:43:40, 18.22s/it] 54%|█████▍    | 1105/2038 [32:03<4:44:10, 18.27s/it] 54%|█████▍    | 1106/2038 [32:22<4:46:56, 18.47s/it] 54%|█████▍    | 1107/2038 [32:41<4:45:16, 18.39s/it] 54%|█████▍    | 1108/2038 [32:59<4:43:53, 18.32s/it] 54%|█████▍    | 1109/2038 [33:17<4:42:59, 18.28s/it] 54%|█████▍    | 1110/2038 [33:35<4:42:08, 18.24s/it]                                                     {'loss': 0.1806, 'grad_norm': 0.027471622452139854, 'learning_rate': 4.7014170040485836e-05, 'entropy': 0.1809235468506813, 'num_tokens': 21790337.0, 'mean_token_accuracy': 0.9389594972133637, 'epoch': 0.54}
 54%|█████▍    | 1110/2038 [33:35<4:42:08, 18.24s/it] 55%|█████▍    | 1111/2038 [33:53<4:41:26, 18.22s/it] 55%|█████▍    | 1112/2038 [34:13<4:46:16, 18.55s/it] 55%|█████▍    | 1113/2038 [34:31<4:43:58, 18.42s/it] 55%|█████▍    | 1114/2038 [34:49<4:42:25, 18.34s/it] 55%|█████▍    | 1115/2038 [35:07<4:41:24, 18.29s/it] 55%|█████▍    | 1116/2038 [35:25<4:40:25, 18.25s/it] 55%|█████▍    | 1117/2038 [35:43<4:39:40, 18.22s/it] 55%|█████▍    | 1118/2038 [36:02<4:42:03, 18.40s/it] 55%|█████▍    | 1119/2038 [36:21<4:42:51, 18.47s/it] 55%|█████▍    | 1120/2038 [36:39<4:41:01, 18.37s/it]                                                     {'loss': 0.1811, 'grad_norm': 0.027303269132971764, 'learning_rate': 4.65080971659919e-05, 'entropy': 0.18168720006942748, 'num_tokens': 23767531.0, 'mean_token_accuracy': 0.9387675762176514, 'epoch': 0.55}
 55%|█████▍    | 1120/2038 [36:39<4:41:01, 18.37s/it] 55%|█████▌    | 1121/2038 [36:57<4:39:45, 18.30s/it] 55%|█████▌    | 1122/2038 [37:15<4:38:10, 18.22s/it] 55%|█████▌    | 1123/2038 [37:33<4:37:03, 18.17s/it] 55%|█████▌    | 1124/2038 [37:51<4:37:43, 18.23s/it] 55%|█████▌    | 1125/2038 [38:10<4:38:57, 18.33s/it] 55%|█████▌    | 1126/2038 [38:29<4:39:31, 18.39s/it] 55%|█████▌    | 1127/2038 [38:47<4:38:21, 18.33s/it] 55%|█████▌    | 1128/2038 [39:05<4:37:07, 18.27s/it] 55%|█████▌    | 1129/2038 [39:23<4:35:41, 18.20s/it] 55%|█████▌    | 1130/2038 [39:41<4:34:59, 18.17s/it]                                                     {'loss': 0.184, 'grad_norm': 0.026926591992378235, 'learning_rate': 4.600202429149798e-05, 'entropy': 0.18442529886960984, 'num_tokens': 25775198.0, 'mean_token_accuracy': 0.9378111302852631, 'epoch': 0.55}
 55%|█████▌    | 1130/2038 [39:41<4:34:59, 18.17s/it] 55%|█████▌    | 1131/2038 [40:00<4:37:35, 18.36s/it] 56%|█████▌    | 1132/2038 [40:18<4:38:13, 18.43s/it] 56%|█████▌    | 1133/2038 [40:37<4:36:31, 18.33s/it] 56%|█████▌    | 1134/2038 [40:55<4:35:21, 18.28s/it] 56%|█████▌    | 1135/2038 [41:13<4:34:03, 18.21s/it] 56%|█████▌    | 1136/2038 [41:31<4:33:43, 18.21s/it] 56%|█████▌    | 1137/2038 [41:49<4:34:28, 18.28s/it] 56%|█████▌    | 1138/2038 [42:08<4:35:38, 18.38s/it] 56%|█████▌    | 1139/2038 [42:26<4:35:50, 18.41s/it] 56%|█████▌    | 1140/2038 [42:45<4:34:10, 18.32s/it]                                                     {'loss': 0.1823, 'grad_norm': 0.027883421629667282, 'learning_rate': 4.549595141700405e-05, 'entropy': 0.18245336264371873, 'num_tokens': 27771277.0, 'mean_token_accuracy': 0.9382540881633759, 'epoch': 0.56}
 56%|█████▌    | 1140/2038 [42:45<4:34:10, 18.32s/it] 56%|█████▌    | 1141/2038 [43:03<4:32:59, 18.26s/it] 56%|█████▌    | 1142/2038 [43:21<4:32:26, 18.24s/it] 56%|█████▌    | 1143/2038 [43:39<4:31:50, 18.22s/it] 56%|█████▌    | 1144/2038 [43:58<4:34:24, 18.42s/it] 56%|█████▌    | 1145/2038 [44:16<4:34:28, 18.44s/it] 56%|█████▌    | 1146/2038 [44:35<4:32:24, 18.32s/it] 56%|█████▋    | 1147/2038 [44:53<4:31:26, 18.28s/it] 56%|█████▋    | 1148/2038 [45:11<4:30:39, 18.25s/it] 56%|█████▋    | 1149/2038 [45:29<4:29:28, 18.19s/it] 56%|█████▋    | 1150/2038 [45:48<4:31:46, 18.36s/it]                                                     {'loss': 0.1779, 'grad_norm': 0.02635498344898224, 'learning_rate': 4.498987854251012e-05, 'entropy': 0.17867552489042282, 'num_tokens': 29750799.0, 'mean_token_accuracy': 0.9398586511611938, 'epoch': 0.56}
 56%|█████▋    | 1150/2038 [45:48<4:31:46, 18.36s/it] 56%|█████▋    | 1151/2038 [46:06<4:32:52, 18.46s/it] 57%|█████▋    | 1152/2038 [46:25<4:31:19, 18.37s/it] 57%|█████▋    | 1153/2038 [46:43<4:30:14, 18.32s/it] 57%|█████▋    | 1154/2038 [47:01<4:28:50, 18.25s/it] 57%|█████▋    | 1155/2038 [47:19<4:28:14, 18.23s/it] 57%|█████▋    | 1156/2038 [47:37<4:28:52, 18.29s/it] 57%|█████▋    | 1157/2038 [47:56<4:29:13, 18.34s/it] 57%|█████▋    | 1158/2038 [48:14<4:29:51, 18.40s/it] 57%|█████▋    | 1159/2038 [48:33<4:28:41, 18.34s/it] 57%|█████▋    | 1160/2038 [48:51<4:27:23, 18.27s/it]                                                     {'loss': 0.1782, 'grad_norm': 0.025972196832299232, 'learning_rate': 4.4483805668016196e-05, 'entropy': 0.1780437633395195, 'num_tokens': 31727924.0, 'mean_token_accuracy': 0.939666622877121, 'epoch': 0.57}
 57%|█████▋    | 1160/2038 [48:51<4:27:23, 18.27s/it] 57%|█████▋    | 1161/2038 [49:09<4:26:05, 18.21s/it] 57%|█████▋    | 1162/2038 [49:27<4:25:51, 18.21s/it] 57%|█████▋    | 1163/2038 [49:46<4:27:57, 18.37s/it] 57%|█████▋    | 1164/2038 [50:04<4:28:15, 18.42s/it] 57%|█████▋    | 1165/2038 [50:22<4:26:22, 18.31s/it] 57%|█████▋    | 1166/2038 [50:41<4:25:37, 18.28s/it] 57%|█████▋    | 1167/2038 [50:59<4:24:15, 18.20s/it] 57%|█████▋    | 1168/2038 [51:17<4:23:34, 18.18s/it] 57%|█████▋    | 1169/2038 [51:35<4:24:23, 18.25s/it] 57%|█████▋    | 1170/2038 [51:54<4:25:37, 18.36s/it]                                                     {'loss': 0.1767, 'grad_norm': 0.026069164276123047, 'learning_rate': 4.397773279352227e-05, 'entropy': 0.17681406140327455, 'num_tokens': 33712321.0, 'mean_token_accuracy': 0.9402035653591156, 'epoch': 0.57}
 57%|█████▋    | 1170/2038 [51:54<4:25:37, 18.36s/it] 57%|█████▋    | 1171/2038 [52:12<4:26:30, 18.44s/it] 58%|█████▊    | 1172/2038 [52:31<4:25:01, 18.36s/it] 58%|█████▊    | 1173/2038 [52:49<4:23:31, 18.28s/it] 58%|█████▊    | 1174/2038 [53:07<4:22:20, 18.22s/it] 58%|█████▊    | 1175/2038 [53:25<4:21:42, 18.20s/it] 58%|█████▊    | 1176/2038 [53:44<4:24:07, 18.38s/it] 58%|█████▊    | 1177/2038 [54:02<4:24:45, 18.45s/it] 58%|█████▊    | 1178/2038 [54:21<4:23:39, 18.39s/it] 58%|█████▊    | 1179/2038 [54:39<4:22:21, 18.33s/it] 58%|█████▊    | 1180/2038 [54:57<4:20:49, 18.24s/it]                                                     {'loss': 0.1813, 'grad_norm': 0.026382721960544586, 'learning_rate': 4.347165991902834e-05, 'entropy': 0.18129520118236542, 'num_tokens': 35680327.0, 'mean_token_accuracy': 0.9385941028594971, 'epoch': 0.58}
 58%|█████▊    | 1180/2038 [54:57<4:20:49, 18.24s/it] 58%|█████▊    | 1181/2038 [55:15<4:19:42, 18.18s/it] 58%|█████▊    | 1182/2038 [55:34<4:21:48, 18.35s/it] 58%|█████▊    | 1183/2038 [55:52<4:22:46, 18.44s/it] 58%|█████▊    | 1184/2038 [56:10<4:20:47, 18.32s/it] 58%|█████▊    | 1185/2038 [56:28<4:19:52, 18.28s/it] 58%|█████▊    | 1186/2038 [56:47<4:19:05, 18.25s/it] 58%|█████▊    | 1187/2038 [57:05<4:18:21, 18.22s/it] 58%|█████▊    | 1188/2038 [57:23<4:18:50, 18.27s/it] 58%|█████▊    | 1189/2038 [57:42<4:20:01, 18.38s/it] 58%|█████▊    | 1190/2038 [58:00<4:21:02, 18.47s/it]                                                     {'loss': 0.1834, 'grad_norm': 0.02740408480167389, 'learning_rate': 4.2965587044534416e-05, 'entropy': 0.18363697677850724, 'num_tokens': 37664098.0, 'mean_token_accuracy': 0.9377606630325317, 'epoch': 0.58}
 58%|█████▊    | 1190/2038 [58:00<4:21:02, 18.47s/it] 58%|█████▊    | 1191/2038 [58:19<4:19:40, 18.40s/it] 58%|█████▊    | 1192/2038 [58:37<4:18:34, 18.34s/it] 59%|█████▊    | 1193/2038 [58:55<4:17:39, 18.30s/it] 59%|█████▊    | 1194/2038 [59:13<4:16:16, 18.22s/it] 59%|█████▊    | 1195/2038 [59:32<4:18:38, 18.41s/it] 59%|█████▊    | 1196/2038 [59:51<4:19:07, 18.47s/it] 59%|█████▊    | 1197/2038 [1:00:09<4:17:42, 18.39s/it] 59%|█████▉    | 1198/2038 [1:00:27<4:16:32, 18.32s/it] 59%|█████▉    | 1199/2038 [1:00:45<4:15:22, 18.26s/it] 59%|█████▉    | 1200/2038 [1:01:03<4:14:17, 18.21s/it]                                                       {'loss': 0.1826, 'grad_norm': 0.027188336476683617, 'learning_rate': 4.245951417004049e-05, 'entropy': 0.1827746495604515, 'num_tokens': 39650132.0, 'mean_token_accuracy': 0.9380684494972229, 'epoch': 0.59}
 59%|█████▉    | 1200/2038 [1:01:03<4:14:17, 18.21s/it] 59%|█████▉    | 1201/2038 [1:01:22<4:14:56, 18.28s/it] 59%|█████▉    | 1202/2038 [1:01:40<4:15:39, 18.35s/it] 59%|█████▉    | 1203/2038 [1:01:59<4:16:24, 18.42s/it] 59%|█████▉    | 1204/2038 [1:02:17<4:14:26, 18.30s/it] 59%|█████▉    | 1205/2038 [1:02:35<4:13:32, 18.26s/it] 59%|█████▉    | 1206/2038 [1:02:53<4:12:55, 18.24s/it] 59%|█████▉    | 1207/2038 [1:03:11<4:12:18, 18.22s/it] 59%|█████▉    | 1208/2038 [1:03:30<4:14:40, 18.41s/it] 59%|█████▉    | 1209/2038 [1:03:49<4:15:24, 18.49s/it] 59%|█████▉    | 1210/2038 [1:04:07<4:13:15, 18.35s/it]                                                       {'loss': 0.1786, 'grad_norm': 0.0275210440158844, 'learning_rate': 4.1953441295546556e-05, 'entropy': 0.17922336757183074, 'num_tokens': 41629943.0, 'mean_token_accuracy': 0.9393816471099854, 'epoch': 0.59}
 59%|█████▉    | 1210/2038 [1:04:07<4:13:15, 18.35s/it] 59%|█████▉    | 1211/2038 [1:04:25<4:11:55, 18.28s/it] 59%|█████▉    | 1212/2038 [1:04:43<4:11:12, 18.25s/it] 60%|█████▉    | 1213/2038 [1:05:01<4:10:48, 18.24s/it] 60%|█████▉    | 1214/2038 [1:05:20<4:12:59, 18.42s/it] 60%|█████▉    | 1215/2038 [1:05:39<4:13:43, 18.50s/it] 60%|█████▉    | 1216/2038 [1:05:57<4:12:04, 18.40s/it] 60%|█████▉    | 1217/2038 [1:06:15<4:10:40, 18.32s/it] 60%|█████▉    | 1218/2038 [1:06:33<4:09:36, 18.26s/it] 60%|█████▉    | 1219/2038 [1:06:51<4:08:26, 18.20s/it] 60%|█████▉    | 1220/2038 [1:07:10<4:08:48, 18.25s/it]                                                       {'loss': 0.1803, 'grad_norm': 0.026341330260038376, 'learning_rate': 4.1447368421052636e-05, 'entropy': 0.18072493225336075, 'num_tokens': 43625565.0, 'mean_token_accuracy': 0.9390432476997376, 'epoch': 0.6}
 60%|█████▉    | 1220/2038 [1:07:10<4:08:48, 18.25s/it] 60%|█████▉    | 1221/2038 [1:07:28<4:10:02, 18.36s/it] 60%|█████▉    | 1222/2038 [1:07:47<4:10:42, 18.43s/it] 60%|██████    | 1223/2038 [1:08:05<4:09:17, 18.35s/it] 60%|██████    | 1224/2038 [1:08:23<4:08:24, 18.31s/it] 60%|██████    | 1225/2038 [1:08:41<4:07:24, 18.26s/it] 60%|██████    | 1226/2038 [1:09:00<4:06:33, 18.22s/it] 60%|██████    | 1227/2038 [1:09:18<4:08:42, 18.40s/it] 60%|██████    | 1228/2038 [1:09:37<4:08:58, 18.44s/it] 60%|██████    | 1229/2038 [1:09:55<4:07:12, 18.33s/it] 60%|██████    | 1230/2038 [1:10:13<4:06:14, 18.29s/it]                                                       {'loss': 0.1822, 'grad_norm': 0.026421867311000824, 'learning_rate': 4.09412955465587e-05, 'entropy': 0.18243252635002136, 'num_tokens': 45608454.0, 'mean_token_accuracy': 0.9386141896247864, 'epoch': 0.6}
 60%|██████    | 1230/2038 [1:10:13<4:06:14, 18.29s/it] 60%|██████    | 1231/2038 [1:10:31<4:04:53, 18.21s/it] 60%|██████    | 1232/2038 [1:10:49<4:04:10, 18.18s/it] 61%|██████    | 1233/2038 [1:11:08<4:04:54, 18.25s/it] 61%|██████    | 1234/2038 [1:11:26<4:06:01, 18.36s/it] 61%|██████    | 1235/2038 [1:11:45<4:06:47, 18.44s/it] 61%|██████    | 1236/2038 [1:12:03<4:05:18, 18.35s/it] 61%|██████    | 1237/2038 [1:12:21<4:04:13, 18.29s/it] 61%|██████    | 1238/2038 [1:12:39<4:02:50, 18.21s/it] 61%|██████    | 1239/2038 [1:12:57<4:02:13, 18.19s/it] 61%|██████    | 1240/2038 [1:13:16<4:04:32, 18.39s/it]                                                       {'loss': 0.1782, 'grad_norm': 0.026791224256157875, 'learning_rate': 4.043522267206478e-05, 'entropy': 0.1782120019197464, 'num_tokens': 47584204.0, 'mean_token_accuracy': 0.939657700061798, 'epoch': 0.61}
 61%|██████    | 1240/2038 [1:13:16<4:04:32, 18.39s/it] 61%|██████    | 1241/2038 [1:13:35<4:04:37, 18.42s/it] 61%|██████    | 1242/2038 [1:13:53<4:02:59, 18.32s/it] 61%|██████    | 1243/2038 [1:14:11<4:02:10, 18.28s/it] 61%|██████    | 1244/2038 [1:14:29<4:00:57, 18.21s/it] 61%|██████    | 1245/2038 [1:14:47<4:00:32, 18.20s/it] 61%|██████    | 1246/2038 [1:15:06<4:02:51, 18.40s/it] 61%|██████    | 1247/2038 [1:15:25<4:03:27, 18.47s/it] 61%|██████    | 1248/2038 [1:15:43<4:01:54, 18.37s/it] 61%|██████▏   | 1249/2038 [1:16:01<4:00:42, 18.30s/it] 61%|██████▏   | 1250/2038 [1:16:19<3:59:37, 18.25s/it]                                                       {'loss': 0.1772, 'grad_norm': 0.02821349911391735, 'learning_rate': 3.9929149797570856e-05, 'entropy': 0.17774712592363356, 'num_tokens': 49568058.0, 'mean_token_accuracy': 0.9399816393852234, 'epoch': 0.61}
 61%|██████▏   | 1250/2038 [1:16:19<3:59:37, 18.25s/it] 61%|██████▏   | 1251/2038 [1:16:37<3:58:57, 18.22s/it] 61%|██████▏   | 1252/2038 [1:16:56<3:58:53, 18.24s/it] 61%|██████▏   | 1253/2038 [1:17:14<4:00:16, 18.37s/it] 62%|██████▏   | 1254/2038 [1:17:33<4:00:53, 18.44s/it] 62%|██████▏   | 1255/2038 [1:17:51<3:59:25, 18.35s/it] 62%|██████▏   | 1256/2038 [1:18:09<3:58:23, 18.29s/it] 62%|██████▏   | 1257/2038 [1:18:27<3:56:57, 18.20s/it] 62%|██████▏   | 1258/2038 [1:18:45<3:55:55, 18.15s/it] 62%|██████▏   | 1259/2038 [1:19:04<3:58:22, 18.36s/it] 62%|██████▏   | 1260/2038 [1:19:23<3:59:06, 18.44s/it]                                                       {'loss': 0.1772, 'grad_norm': 0.026229683309793472, 'learning_rate': 3.942307692307692e-05, 'entropy': 0.1770312011241913, 'num_tokens': 51555621.0, 'mean_token_accuracy': 0.9400310397148133, 'epoch': 0.62}
 62%|██████▏   | 1260/2038 [1:19:23<3:59:06, 18.44s/it] 62%|██████▏   | 1261/2038 [1:19:41<3:57:37, 18.35s/it] 62%|██████▏   | 1262/2038 [1:19:59<3:56:02, 18.25s/it] 62%|██████▏   | 1263/2038 [1:20:17<3:55:28, 18.23s/it] 62%|██████▏   | 1264/2038 [1:20:35<3:54:36, 18.19s/it] 62%|██████▏   | 1265/2038 [1:20:53<3:54:47, 18.22s/it] 62%|██████▏   | 1266/2038 [1:21:12<3:55:59, 18.34s/it] 62%|██████▏   | 1267/2038 [1:21:31<3:56:40, 18.42s/it] 62%|██████▏   | 1268/2038 [1:21:49<3:55:16, 18.33s/it] 62%|██████▏   | 1269/2038 [1:22:07<3:54:17, 18.28s/it] 62%|██████▏   | 1270/2038 [1:22:25<3:53:08, 18.21s/it]                                                       {'loss': 0.1852, 'grad_norm': 0.0255016777664423, 'learning_rate': 3.8917004048583e-05, 'entropy': 0.18536854386329651, 'num_tokens': 53572285.0, 'mean_token_accuracy': 0.9373451411724091, 'epoch': 0.62}
 62%|██████▏   | 1270/2038 [1:22:25<3:53:08, 18.21s/it] 62%|██████▏   | 1271/2038 [1:22:43<3:52:45, 18.21s/it] 62%|██████▏   | 1272/2038 [1:23:02<3:54:43, 18.39s/it] 62%|██████▏   | 1273/2038 [1:23:20<3:55:09, 18.44s/it] 63%|██████▎   | 1274/2038 [1:23:39<3:53:54, 18.37s/it] 63%|██████▎   | 1275/2038 [1:23:57<3:52:34, 18.29s/it] 63%|██████▎   | 1276/2038 [1:24:15<3:51:47, 18.25s/it] 63%|██████▎   | 1277/2038 [1:24:33<3:50:30, 18.17s/it] 63%|██████▎   | 1278/2038 [1:24:52<3:52:21, 18.34s/it] 63%|██████▎   | 1279/2038 [1:25:10<3:53:01, 18.42s/it] 63%|██████▎   | 1280/2038 [1:25:28<3:51:52, 18.35s/it]                                                       {'loss': 0.181, 'grad_norm': 0.02651994861662388, 'learning_rate': 3.841093117408907e-05, 'entropy': 0.18145304918289185, 'num_tokens': 55548768.0, 'mean_token_accuracy': 0.9385339617729187, 'epoch': 0.63}
 63%|██████▎   | 1280/2038 [1:25:29<3:51:52, 18.35s/it] 63%|██████▎   | 1281/2038 [1:25:47<3:50:26, 18.26s/it] 63%|██████▎   | 1282/2038 [1:26:05<3:49:26, 18.21s/it] 63%|██████▎   | 1283/2038 [1:26:23<3:48:40, 18.17s/it] 63%|██████▎   | 1284/2038 [1:26:41<3:48:44, 18.20s/it] 63%|██████▎   | 1285/2038 [1:27:00<3:49:55, 18.32s/it] 63%|██████▎   | 1286/2038 [1:27:18<3:51:02, 18.43s/it] 63%|██████▎   | 1287/2038 [1:27:36<3:49:35, 18.34s/it] 63%|██████▎   | 1288/2038 [1:27:55<3:48:24, 18.27s/it] 63%|██████▎   | 1289/2038 [1:28:13<3:47:17, 18.21s/it] 63%|██████▎   | 1290/2038 [1:28:31<3:46:16, 18.15s/it]                                                       {'loss': 0.1816, 'grad_norm': 0.026895714923739433, 'learning_rate': 3.790485829959514e-05, 'entropy': 0.18155914098024367, 'num_tokens': 57551731.0, 'mean_token_accuracy': 0.9385680198669434, 'epoch': 0.63}
 63%|██████▎   | 1290/2038 [1:28:31<3:46:16, 18.15s/it] 63%|██████▎   | 1291/2038 [1:28:49<3:48:38, 18.36s/it] 63%|██████▎   | 1292/2038 [1:29:08<3:49:57, 18.49s/it] 63%|██████▎   | 1293/2038 [1:29:26<3:48:26, 18.40s/it] 63%|██████▎   | 1294/2038 [1:29:45<3:47:16, 18.33s/it] 64%|██████▎   | 1295/2038 [1:30:03<3:45:50, 18.24s/it] 64%|██████▎   | 1296/2038 [1:30:21<3:45:15, 18.21s/it] 64%|██████▎   | 1297/2038 [1:30:39<3:45:40, 18.27s/it] 64%|██████▎   | 1298/2038 [1:30:58<3:47:16, 18.43s/it] 64%|██████▎   | 1299/2038 [1:31:16<3:46:57, 18.43s/it] 64%|██████▍   | 1300/2038 [1:31:35<3:45:40, 18.35s/it]                                                       {'loss': 0.1824, 'grad_norm': 0.028208576142787933, 'learning_rate': 3.7398785425101215e-05, 'entropy': 0.18280670791864395, 'num_tokens': 59543091.0, 'mean_token_accuracy': 0.9382990181446076, 'epoch': 0.64}
 64%|██████▍   | 1300/2038 [1:31:35<3:45:40, 18.35s/it] 64%|██████▍   | 1301/2038 [1:31:53<3:44:27, 18.27s/it] 64%|██████▍   | 1302/2038 [1:32:11<3:43:44, 18.24s/it] 64%|██████▍   | 1303/2038 [1:32:29<3:43:15, 18.23s/it] 64%|██████▍   | 1304/2038 [1:32:48<3:45:02, 18.40s/it] 64%|██████▍   | 1305/2038 [1:33:07<3:46:16, 18.52s/it] 64%|██████▍   | 1306/2038 [1:33:25<3:44:09, 18.37s/it] 64%|██████▍   | 1307/2038 [1:33:43<3:43:08, 18.32s/it] 64%|██████▍   | 1308/2038 [1:34:01<3:42:04, 18.25s/it] 64%|██████▍   | 1309/2038 [1:34:19<3:41:21, 18.22s/it] 64%|██████▍   | 1310/2038 [1:34:38<3:43:03, 18.38s/it]                                                       {'loss': 0.1773, 'grad_norm': 0.027292834594845772, 'learning_rate': 3.689271255060729e-05, 'entropy': 0.17686100155115128, 'num_tokens': 61520947.0, 'mean_token_accuracy': 0.9400241017341614, 'epoch': 0.64}
 64%|██████▍   | 1310/2038 [1:34:38<3:43:03, 18.38s/it] 64%|██████▍   | 1311/2038 [1:34:57<3:44:21, 18.52s/it] 64%|██████▍   | 1312/2038 [1:35:15<3:42:23, 18.38s/it] 64%|██████▍   | 1313/2038 [1:35:33<3:41:00, 18.29s/it] 64%|██████▍   | 1314/2038 [1:35:51<3:39:42, 18.21s/it] 65%|██████▍   | 1315/2038 [1:36:09<3:39:01, 18.18s/it] 65%|██████▍   | 1316/2038 [1:36:27<3:39:43, 18.26s/it] 65%|██████▍   | 1317/2038 [1:36:46<3:40:09, 18.32s/it] 65%|██████▍   | 1318/2038 [1:37:05<3:41:58, 18.50s/it] 65%|██████▍   | 1319/2038 [1:37:23<3:40:16, 18.38s/it] 65%|██████▍   | 1320/2038 [1:37:41<3:39:08, 18.31s/it]                                                       {'loss': 0.1776, 'grad_norm': 0.026985080912709236, 'learning_rate': 3.638663967611336e-05, 'entropy': 0.1783720701932907, 'num_tokens': 63479339.0, 'mean_token_accuracy': 0.9397647023200989, 'epoch': 0.65}
 65%|██████▍   | 1320/2038 [1:37:41<3:39:08, 18.31s/it] 65%|██████▍   | 1321/2038 [1:37:59<3:37:46, 18.22s/it] 65%|██████▍   | 1322/2038 [1:38:17<3:36:46, 18.16s/it] 65%|██████▍   | 1323/2038 [1:38:36<3:38:43, 18.35s/it] 65%|██████▍   | 1324/2038 [1:38:55<3:39:59, 18.49s/it] 65%|██████▌   | 1325/2038 [1:39:13<3:38:37, 18.40s/it] 65%|██████▌   | 1326/2038 [1:39:31<3:37:21, 18.32s/it] 65%|██████▌   | 1327/2038 [1:39:49<3:36:12, 18.25s/it] 65%|██████▌   | 1328/2038 [1:40:07<3:35:32, 18.21s/it] 65%|██████▌   | 1329/2038 [1:40:26<3:35:34, 18.24s/it] 65%|██████▌   | 1330/2038 [1:40:44<3:37:11, 18.41s/it]                                                       {'loss': 0.184, 'grad_norm': 0.027500901371240616, 'learning_rate': 3.5880566801619435e-05, 'entropy': 0.18359781950712203, 'num_tokens': 65467837.0, 'mean_token_accuracy': 0.9377871334552765, 'epoch': 0.65}
 65%|██████▌   | 1330/2038 [1:40:44<3:37:11, 18.41s/it] 65%|██████▌   | 1331/2038 [1:41:03<3:37:03, 18.42s/it] 65%|██████▌   | 1332/2038 [1:41:21<3:35:38, 18.33s/it] 65%|██████▌   | 1333/2038 [1:41:39<3:34:25, 18.25s/it] 65%|██████▌   | 1334/2038 [1:41:57<3:33:28, 18.19s/it] 66%|██████▌   | 1335/2038 [1:42:15<3:32:50, 18.17s/it] 66%|██████▌   | 1336/2038 [1:42:34<3:34:26, 18.33s/it] 66%|██████▌   | 1337/2038 [1:42:53<3:35:31, 18.45s/it] 66%|██████▌   | 1338/2038 [1:43:11<3:33:52, 18.33s/it] 66%|██████▌   | 1339/2038 [1:43:29<3:32:50, 18.27s/it] 66%|██████▌   | 1340/2038 [1:43:47<3:31:58, 18.22s/it]                                                       {'loss': 0.1845, 'grad_norm': 0.027400705963373184, 'learning_rate': 3.537449392712551e-05, 'entropy': 0.18475066870450974, 'num_tokens': 67458815.0, 'mean_token_accuracy': 0.9375748872756958, 'epoch': 0.66}
 66%|██████▌   | 1340/2038 [1:43:47<3:31:58, 18.22s/it] 66%|██████▌   | 1341/2038 [1:44:05<3:31:34, 18.21s/it] 66%|██████▌   | 1342/2038 [1:44:24<3:33:27, 18.40s/it] 66%|██████▌   | 1343/2038 [1:44:43<3:34:47, 18.54s/it] 66%|██████▌   | 1344/2038 [1:45:01<3:32:58, 18.41s/it] 66%|██████▌   | 1345/2038 [1:45:19<3:31:34, 18.32s/it] 66%|██████▌   | 1346/2038 [1:45:37<3:30:37, 18.26s/it] 66%|██████▌   | 1347/2038 [1:45:55<3:30:06, 18.24s/it] 66%|██████▌   | 1348/2038 [1:46:14<3:30:17, 18.29s/it] 66%|██████▌   | 1349/2038 [1:46:32<3:30:46, 18.36s/it] 66%|██████▌   | 1350/2038 [1:46:51<3:31:59, 18.49s/it]                                                       {'loss': 0.1811, 'grad_norm': 0.026873113587498665, 'learning_rate': 3.4868421052631575e-05, 'entropy': 0.18101276755332946, 'num_tokens': 69434811.0, 'mean_token_accuracy': 0.9386120498180389, 'epoch': 0.66}
 66%|██████▌   | 1350/2038 [1:46:51<3:31:59, 18.49s/it] 66%|██████▋   | 1351/2038 [1:47:09<3:30:30, 18.38s/it] 66%|██████▋   | 1352/2038 [1:47:27<3:29:10, 18.30s/it] 66%|██████▋   | 1353/2038 [1:47:45<3:28:15, 18.24s/it] 66%|██████▋   | 1354/2038 [1:48:03<3:27:39, 18.22s/it] 66%|██████▋   | 1355/2038 [1:48:22<3:29:23, 18.39s/it] 67%|██████▋   | 1356/2038 [1:48:41<3:30:10, 18.49s/it] 67%|██████▋   | 1357/2038 [1:48:59<3:28:23, 18.36s/it] 67%|██████▋   | 1358/2038 [1:49:17<3:27:20, 18.30s/it] 67%|██████▋   | 1359/2038 [1:49:35<3:26:15, 18.23s/it] 67%|██████▋   | 1360/2038 [1:49:53<3:25:47, 18.21s/it]                                                       {'loss': 0.1792, 'grad_norm': 0.02779424749314785, 'learning_rate': 3.4362348178137655e-05, 'entropy': 0.17929496616125107, 'num_tokens': 71413833.0, 'mean_token_accuracy': 0.939106422662735, 'epoch': 0.67}
 67%|██████▋   | 1360/2038 [1:49:53<3:25:47, 18.21s/it] 67%|██████▋   | 1361/2038 [1:50:12<3:25:56, 18.25s/it] 67%|██████▋   | 1362/2038 [1:50:30<3:26:42, 18.35s/it] 67%|██████▋   | 1363/2038 [1:50:49<3:28:02, 18.49s/it] 67%|██████▋   | 1364/2038 [1:51:07<3:26:15, 18.36s/it] 67%|██████▋   | 1365/2038 [1:51:25<3:25:06, 18.29s/it] 67%|██████▋   | 1366/2038 [1:51:44<3:24:28, 18.26s/it] 67%|██████▋   | 1367/2038 [1:52:02<3:23:47, 18.22s/it] 67%|██████▋   | 1368/2038 [1:52:21<3:25:33, 18.41s/it] 67%|██████▋   | 1369/2038 [1:52:39<3:25:52, 18.46s/it] 67%|██████▋   | 1370/2038 [1:52:57<3:25:17, 18.44s/it]                                                       {'loss': 0.1803, 'grad_norm': 0.02680724300444126, 'learning_rate': 3.385627530364373e-05, 'entropy': 0.1803630158305168, 'num_tokens': 73396147.0, 'mean_token_accuracy': 0.9389187812805175, 'epoch': 0.67}
 67%|██████▋   | 1370/2038 [1:52:58<3:25:17, 18.44s/it] 67%|██████▋   | 1371/2038 [1:53:16<3:23:58, 18.35s/it] 67%|██████▋   | 1372/2038 [1:53:34<3:22:57, 18.28s/it] 67%|██████▋   | 1373/2038 [1:53:52<3:22:16, 18.25s/it] 67%|██████▋   | 1374/2038 [1:54:11<3:23:34, 18.39s/it] 67%|██████▋   | 1375/2038 [1:54:29<3:24:00, 18.46s/it] 68%|██████▊   | 1376/2038 [1:54:48<3:23:29, 18.44s/it] 68%|██████▊   | 1377/2038 [1:55:06<3:21:56, 18.33s/it] 68%|██████▊   | 1378/2038 [1:55:24<3:20:59, 18.27s/it] 68%|██████▊   | 1379/2038 [1:55:42<3:19:50, 18.20s/it] 68%|██████▊   | 1380/2038 [1:56:00<3:19:51, 18.22s/it]                                                       {'loss': 0.1805, 'grad_norm': 0.027034377679228783, 'learning_rate': 3.33502024291498e-05, 'entropy': 0.1802415207028389, 'num_tokens': 75373168.0, 'mean_token_accuracy': 0.9390791177749633, 'epoch': 0.68}
 68%|██████▊   | 1380/2038 [1:56:00<3:19:51, 18.22s/it] 68%|██████▊   | 1381/2038 [1:56:19<3:20:31, 18.31s/it] 68%|██████▊   | 1382/2038 [1:56:37<3:20:45, 18.36s/it] 68%|██████▊   | 1383/2038 [1:56:55<3:20:13, 18.34s/it] 68%|██████▊   | 1384/2038 [1:57:14<3:19:19, 18.29s/it] 68%|██████▊   | 1385/2038 [1:57:32<3:18:25, 18.23s/it] 68%|██████▊   | 1386/2038 [1:57:50<3:17:18, 18.16s/it] 68%|██████▊   | 1387/2038 [1:58:08<3:18:53, 18.33s/it] 68%|██████▊   | 1388/2038 [1:58:27<3:19:07, 18.38s/it] 68%|██████▊   | 1389/2038 [1:58:45<3:18:54, 18.39s/it] 68%|██████▊   | 1390/2038 [1:59:03<3:17:35, 18.30s/it]                                                       {'loss': 0.1798, 'grad_norm': 0.027591103687882423, 'learning_rate': 3.2844129554655875e-05, 'entropy': 0.18069383651018142, 'num_tokens': 77344858.0, 'mean_token_accuracy': 0.9391534626483917, 'epoch': 0.68}
 68%|██████▊   | 1390/2038 [1:59:03<3:17:35, 18.30s/it] 68%|██████▊   | 1391/2038 [1:59:22<3:16:35, 18.23s/it] 68%|██████▊   | 1392/2038 [1:59:40<3:15:51, 18.19s/it] 68%|██████▊   | 1393/2038 [1:59:58<3:16:08, 18.25s/it] 68%|██████▊   | 1394/2038 [2:00:17<3:16:48, 18.34s/it] 68%|██████▊   | 1395/2038 [2:00:35<3:18:02, 18.48s/it] 68%|██████▊   | 1396/2038 [2:00:54<3:16:43, 18.39s/it] 69%|██████▊   | 1397/2038 [2:01:12<3:15:13, 18.27s/it] 69%|██████▊   | 1398/2038 [2:01:30<3:14:08, 18.20s/it] 69%|██████▊   | 1399/2038 [2:01:48<3:13:16, 18.15s/it] 69%|██████▊   | 1400/2038 [2:02:06<3:15:01, 18.34s/it]                                                       {'loss': 0.1786, 'grad_norm': 0.02674124948680401, 'learning_rate': 3.233805668016194e-05, 'entropy': 0.17943450808525085, 'num_tokens': 79316898.0, 'mean_token_accuracy': 0.9392642855644227, 'epoch': 0.69}
 69%|██████▊   | 1400/2038 [2:02:06<3:15:01, 18.34s/it] 69%|██████▊   | 1401/2038 [2:02:25<3:15:24, 18.41s/it] 69%|██████▉   | 1402/2038 [2:02:43<3:14:54, 18.39s/it] 69%|██████▉   | 1403/2038 [2:03:01<3:13:52, 18.32s/it] 69%|██████▉   | 1404/2038 [2:03:20<3:13:08, 18.28s/it] 69%|██████▉   | 1405/2038 [2:03:38<3:12:23, 18.24s/it] 69%|██████▉   | 1406/2038 [2:03:57<3:13:50, 18.40s/it] 69%|██████▉   | 1407/2038 [2:04:15<3:13:54, 18.44s/it] 69%|██████▉   | 1408/2038 [2:04:33<3:13:27, 18.42s/it] 69%|██████▉   | 1409/2038 [2:04:52<3:12:24, 18.35s/it] 69%|██████▉   | 1410/2038 [2:05:10<3:11:34, 18.30s/it]                                                       {'loss': 0.1796, 'grad_norm': 0.026632120832800865, 'learning_rate': 3.183198380566802e-05, 'entropy': 0.18002745658159255, 'num_tokens': 81315907.0, 'mean_token_accuracy': 0.9390948593616486, 'epoch': 0.69}
 69%|██████▉   | 1410/2038 [2:05:10<3:11:34, 18.30s/it] 69%|██████▉   | 1411/2038 [2:05:28<3:10:36, 18.24s/it] 69%|██████▉   | 1412/2038 [2:05:46<3:10:17, 18.24s/it] 69%|██████▉   | 1413/2038 [2:06:05<3:10:56, 18.33s/it] 69%|██████▉   | 1414/2038 [2:06:23<3:11:26, 18.41s/it] 69%|██████▉   | 1415/2038 [2:06:42<3:10:59, 18.39s/it] 69%|██████▉   | 1416/2038 [2:07:00<3:09:50, 18.31s/it] 70%|██████▉   | 1417/2038 [2:07:18<3:08:46, 18.24s/it] 70%|██████▉   | 1418/2038 [2:07:36<3:08:08, 18.21s/it] 70%|██████▉   | 1419/2038 [2:07:55<3:09:28, 18.37s/it] 70%|██████▉   | 1420/2038 [2:08:13<3:09:53, 18.44s/it]                                                       {'loss': 0.1803, 'grad_norm': 0.027754370123147964, 'learning_rate': 3.132591093117409e-05, 'entropy': 0.17979495972394943, 'num_tokens': 83323472.0, 'mean_token_accuracy': 0.9389040291309356, 'epoch': 0.7}
 70%|██████▉   | 1420/2038 [2:08:13<3:09:53, 18.44s/it] 70%|██████▉   | 1421/2038 [2:08:32<3:09:32, 18.43s/it] 70%|██████▉   | 1422/2038 [2:08:50<3:08:11, 18.33s/it] 70%|██████▉   | 1423/2038 [2:09:08<3:06:51, 18.23s/it] 70%|██████▉   | 1424/2038 [2:09:26<3:06:20, 18.21s/it] 70%|██████▉   | 1425/2038 [2:09:44<3:06:20, 18.24s/it] 70%|██████▉   | 1426/2038 [2:10:03<3:06:34, 18.29s/it] 70%|███████   | 1427/2038 [2:10:21<3:07:23, 18.40s/it] 70%|███████   | 1428/2038 [2:10:40<3:06:23, 18.33s/it] 70%|███████   | 1429/2038 [2:10:58<3:05:38, 18.29s/it] 70%|███████   | 1430/2038 [2:11:16<3:04:52, 18.24s/it]                                                       {'loss': 0.1789, 'grad_norm': 0.02817688323557377, 'learning_rate': 3.081983805668016e-05, 'entropy': 0.17933512330055237, 'num_tokens': 85293490.0, 'mean_token_accuracy': 0.9395615637302399, 'epoch': 0.7}
 70%|███████   | 1430/2038 [2:11:16<3:04:52, 18.24s/it] 70%|███████   | 1431/2038 [2:11:34<3:04:20, 18.22s/it] 70%|███████   | 1432/2038 [2:11:53<3:05:33, 18.37s/it] 70%|███████   | 1433/2038 [2:12:11<3:05:54, 18.44s/it] 70%|███████   | 1434/2038 [2:12:30<3:05:25, 18.42s/it] 70%|███████   | 1435/2038 [2:12:48<3:04:11, 18.33s/it] 70%|███████   | 1436/2038 [2:13:06<3:03:21, 18.28s/it] 71%|███████   | 1437/2038 [2:13:24<3:02:35, 18.23s/it] 71%|███████   | 1438/2038 [2:13:43<3:04:04, 18.41s/it] 71%|███████   | 1439/2038 [2:14:01<3:04:02, 18.43s/it] 71%|███████   | 1440/2038 [2:14:20<3:03:18, 18.39s/it]                                                       {'loss': 0.178, 'grad_norm': 0.027247849851846695, 'learning_rate': 3.0313765182186238e-05, 'entropy': 0.1777183309197426, 'num_tokens': 87271608.0, 'mean_token_accuracy': 0.9397474586963653, 'epoch': 0.71}
 71%|███████   | 1440/2038 [2:14:20<3:03:18, 18.39s/it] 71%|███████   | 1441/2038 [2:14:38<3:02:22, 18.33s/it] 71%|███████   | 1442/2038 [2:14:56<3:01:15, 18.25s/it] 71%|███████   | 1443/2038 [2:15:14<3:00:37, 18.22s/it] 71%|███████   | 1444/2038 [2:15:32<3:00:25, 18.22s/it] 71%|███████   | 1445/2038 [2:15:51<3:00:49, 18.30s/it] 71%|███████   | 1446/2038 [2:16:09<3:01:00, 18.35s/it] 71%|███████   | 1447/2038 [2:16:28<3:01:04, 18.38s/it] 71%|███████   | 1448/2038 [2:16:46<2:59:48, 18.29s/it] 71%|███████   | 1449/2038 [2:17:04<2:59:00, 18.23s/it] 71%|███████   | 1450/2038 [2:17:22<2:58:20, 18.20s/it]                                                       {'loss': 0.1788, 'grad_norm': 0.027034947648644447, 'learning_rate': 2.9807692307692308e-05, 'entropy': 0.17956859469413758, 'num_tokens': 89254073.0, 'mean_token_accuracy': 0.9394681632518769, 'epoch': 0.71}
 71%|███████   | 1450/2038 [2:17:22<2:58:20, 18.20s/it] 71%|███████   | 1451/2038 [2:17:41<2:59:40, 18.37s/it] 71%|███████   | 1452/2038 [2:17:59<3:00:03, 18.44s/it] 71%|███████▏  | 1453/2038 [2:18:18<2:59:44, 18.44s/it] 71%|███████▏  | 1454/2038 [2:18:36<2:58:07, 18.30s/it] 71%|███████▏  | 1455/2038 [2:18:54<2:57:17, 18.25s/it] 71%|███████▏  | 1456/2038 [2:19:12<2:56:31, 18.20s/it] 71%|███████▏  | 1457/2038 [2:19:30<2:56:22, 18.21s/it] 72%|███████▏  | 1458/2038 [2:19:49<2:57:18, 18.34s/it] 72%|███████▏  | 1459/2038 [2:20:08<2:57:58, 18.44s/it] 72%|███████▏  | 1460/2038 [2:20:26<2:56:38, 18.34s/it]                                                       {'loss': 0.1798, 'grad_norm': 0.027258891612291336, 'learning_rate': 2.9301619433198378e-05, 'entropy': 0.17962868958711625, 'num_tokens': 91260927.0, 'mean_token_accuracy': 0.9390028774738312, 'epoch': 0.72}
 72%|███████▏  | 1460/2038 [2:20:26<2:56:38, 18.34s/it] 72%|███████▏  | 1461/2038 [2:20:44<2:55:50, 18.29s/it] 72%|███████▏  | 1462/2038 [2:21:02<2:54:45, 18.20s/it] 72%|███████▏  | 1463/2038 [2:21:20<2:54:08, 18.17s/it] 72%|███████▏  | 1464/2038 [2:21:39<2:55:44, 18.37s/it] 72%|███████▏  | 1465/2038 [2:21:57<2:55:51, 18.42s/it] 72%|███████▏  | 1466/2038 [2:22:16<2:55:36, 18.42s/it] 72%|███████▏  | 1467/2038 [2:22:34<2:54:11, 18.30s/it] 72%|███████▏  | 1468/2038 [2:22:52<2:53:22, 18.25s/it] 72%|███████▏  | 1469/2038 [2:23:10<2:52:46, 18.22s/it] 72%|███████▏  | 1470/2038 [2:23:29<2:53:54, 18.37s/it]                                                       {'loss': 0.1786, 'grad_norm': 0.02641277015209198, 'learning_rate': 2.8795546558704455e-05, 'entropy': 0.17883239388465882, 'num_tokens': 93259298.0, 'mean_token_accuracy': 0.9393030524253845, 'epoch': 0.72}
 72%|███████▏  | 1470/2038 [2:23:29<2:53:54, 18.37s/it] 72%|███████▏  | 1471/2038 [2:23:47<2:53:48, 18.39s/it] 72%|███████▏  | 1472/2038 [2:24:06<2:53:17, 18.37s/it] 72%|███████▏  | 1473/2038 [2:24:24<2:52:19, 18.30s/it] 72%|███████▏  | 1474/2038 [2:24:42<2:51:31, 18.25s/it] 72%|███████▏  | 1475/2038 [2:25:00<2:51:03, 18.23s/it] 72%|███████▏  | 1476/2038 [2:25:18<2:51:03, 18.26s/it] 72%|███████▏  | 1477/2038 [2:25:37<2:51:24, 18.33s/it] 73%|███████▎  | 1478/2038 [2:25:55<2:51:37, 18.39s/it] 73%|███████▎  | 1479/2038 [2:26:14<2:51:02, 18.36s/it] 73%|███████▎  | 1480/2038 [2:26:32<2:50:02, 18.28s/it]                                                       {'loss': 0.1776, 'grad_norm': 0.026502268388867378, 'learning_rate': 2.8289473684210528e-05, 'entropy': 0.1775089144706726, 'num_tokens': 95230025.0, 'mean_token_accuracy': 0.9398746252059936, 'epoch': 0.73}
 73%|███████▎  | 1480/2038 [2:26:32<2:50:02, 18.28s/it] 73%|███████▎  | 1481/2038 [2:26:50<2:49:13, 18.23s/it] 73%|███████▎  | 1482/2038 [2:27:08<2:48:39, 18.20s/it] 73%|███████▎  | 1483/2038 [2:27:27<2:49:50, 18.36s/it] 73%|███████▎  | 1484/2038 [2:27:45<2:50:01, 18.41s/it] 73%|███████▎  | 1485/2038 [2:28:04<2:49:15, 18.37s/it] 73%|███████▎  | 1486/2038 [2:28:22<2:48:25, 18.31s/it] 73%|███████▎  | 1487/2038 [2:28:40<2:47:39, 18.26s/it] 73%|███████▎  | 1488/2038 [2:28:58<2:46:51, 18.20s/it] 73%|███████▎  | 1489/2038 [2:29:16<2:46:52, 18.24s/it] 73%|███████▎  | 1490/2038 [2:29:35<2:47:03, 18.29s/it]                                                       {'loss': 0.1813, 'grad_norm': 0.027073398232460022, 'learning_rate': 2.7783400809716605e-05, 'entropy': 0.18139344602823257, 'num_tokens': 97207908.0, 'mean_token_accuracy': 0.9387245237827301, 'epoch': 0.73}
 73%|███████▎  | 1490/2038 [2:29:35<2:47:03, 18.29s/it] 73%|███████▎  | 1491/2038 [2:29:53<2:47:43, 18.40s/it] 73%|███████▎  | 1492/2038 [2:30:11<2:46:41, 18.32s/it] 73%|███████▎  | 1493/2038 [2:30:30<2:46:01, 18.28s/it] 73%|███████▎  | 1494/2038 [2:30:48<2:45:21, 18.24s/it] 73%|███████▎  | 1495/2038 [2:31:06<2:44:19, 18.16s/it] 73%|███████▎  | 1496/2038 [2:31:24<2:45:24, 18.31s/it] 73%|███████▎  | 1497/2038 [2:31:43<2:45:48, 18.39s/it] 74%|███████▎  | 1498/2038 [2:32:01<2:45:20, 18.37s/it] 74%|███████▎  | 1499/2038 [2:32:19<2:44:16, 18.29s/it] 74%|███████▎  | 1500/2038 [2:32:37<2:43:29, 18.23s/it]                                                       {'loss': 0.1765, 'grad_norm': 0.030838053673505783, 'learning_rate': 2.7277327935222675e-05, 'entropy': 0.1768462210893631, 'num_tokens': 99183821.0, 'mean_token_accuracy': 0.9400570929050446, 'epoch': 0.74}
 74%|███████▎  | 1500/2038 [2:32:38<2:43:29, 18.23s/it] 74%|███████▎  | 1501/2038 [2:32:56<2:43:04, 18.22s/it] 74%|███████▎  | 1502/2038 [2:33:14<2:44:18, 18.39s/it] 74%|███████▎  | 1503/2038 [2:33:33<2:44:28, 18.45s/it] 74%|███████▍  | 1504/2038 [2:33:51<2:43:56, 18.42s/it] 74%|███████▍  | 1505/2038 [2:34:10<2:42:49, 18.33s/it] 74%|███████▍  | 1506/2038 [2:34:28<2:41:57, 18.27s/it] 74%|███████▍  | 1507/2038 [2:34:46<2:41:06, 18.20s/it] 74%|███████▍  | 1508/2038 [2:35:04<2:41:20, 18.26s/it] 74%|███████▍  | 1509/2038 [2:35:23<2:41:28, 18.32s/it] 74%|███████▍  | 1510/2038 [2:35:41<2:41:30, 18.35s/it]                                                       {'loss': 0.1753, 'grad_norm': 0.029943188652396202, 'learning_rate': 2.6771255060728745e-05, 'entropy': 0.1760133132338524, 'num_tokens': 101129551.0, 'mean_token_accuracy': 0.940544193983078, 'epoch': 0.74}
 74%|███████▍  | 1510/2038 [2:35:41<2:41:30, 18.35s/it] 74%|███████▍  | 1511/2038 [2:35:59<2:40:59, 18.33s/it] 74%|███████▍  | 1512/2038 [2:36:17<2:40:12, 18.27s/it] 74%|███████▍  | 1513/2038 [2:36:35<2:39:23, 18.22s/it] 74%|███████▍  | 1514/2038 [2:36:54<2:38:55, 18.20s/it] 74%|███████▍  | 1515/2038 [2:37:12<2:40:09, 18.37s/it] 74%|███████▍  | 1516/2038 [2:37:31<2:40:24, 18.44s/it] 74%|███████▍  | 1517/2038 [2:37:49<2:39:56, 18.42s/it] 74%|███████▍  | 1518/2038 [2:38:07<2:38:35, 18.30s/it] 75%|███████▍  | 1519/2038 [2:38:26<2:37:47, 18.24s/it] 75%|███████▍  | 1520/2038 [2:38:44<2:36:53, 18.17s/it]                                                       {'loss': 0.1781, 'grad_norm': 0.02720549702644348, 'learning_rate': 2.626518218623482e-05, 'entropy': 0.17855899184942245, 'num_tokens': 103127810.0, 'mean_token_accuracy': 0.9396910011768341, 'epoch': 0.75}
 75%|███████▍  | 1520/2038 [2:38:44<2:36:53, 18.17s/it] 75%|███████▍  | 1521/2038 [2:39:02<2:37:02, 18.23s/it] 75%|███████▍  | 1522/2038 [2:39:20<2:37:31, 18.32s/it] 75%|███████▍  | 1523/2038 [2:39:39<2:38:11, 18.43s/it] 75%|███████▍  | 1524/2038 [2:39:57<2:37:02, 18.33s/it] 75%|███████▍  | 1525/2038 [2:40:15<2:36:00, 18.25s/it] 75%|███████▍  | 1526/2038 [2:40:33<2:35:19, 18.20s/it] 75%|███████▍  | 1527/2038 [2:40:51<2:34:31, 18.14s/it] 75%|███████▍  | 1528/2038 [2:41:10<2:35:38, 18.31s/it] 75%|███████▌  | 1529/2038 [2:41:29<2:36:01, 18.39s/it] 75%|███████▌  | 1530/2038 [2:41:47<2:35:33, 18.37s/it]                                                       {'loss': 0.185, 'grad_norm': 0.02749728411436081, 'learning_rate': 2.575910931174089e-05, 'entropy': 0.18494476675987243, 'num_tokens': 105125741.0, 'mean_token_accuracy': 0.9375995457172394, 'epoch': 0.75}
 75%|███████▌  | 1530/2038 [2:41:47<2:35:33, 18.37s/it] 75%|███████▌  | 1531/2038 [2:42:05<2:34:37, 18.30s/it] 75%|███████▌  | 1532/2038 [2:42:23<2:33:33, 18.21s/it] 75%|███████▌  | 1533/2038 [2:42:41<2:33:12, 18.20s/it] 75%|███████▌  | 1534/2038 [2:43:00<2:34:21, 18.38s/it] 75%|███████▌  | 1535/2038 [2:43:19<2:34:26, 18.42s/it] 75%|███████▌  | 1536/2038 [2:43:37<2:34:03, 18.41s/it] 75%|███████▌  | 1537/2038 [2:43:55<2:32:59, 18.32s/it] 75%|███████▌  | 1538/2038 [2:44:13<2:31:59, 18.24s/it] 76%|███████▌  | 1539/2038 [2:44:31<2:31:22, 18.20s/it] 76%|███████▌  | 1540/2038 [2:44:50<2:31:28, 18.25s/it]                                                       {'loss': 0.1767, 'grad_norm': 0.0267629437148571, 'learning_rate': 2.5253036437246965e-05, 'entropy': 0.17747975289821624, 'num_tokens': 107100739.0, 'mean_token_accuracy': 0.9398414552211761, 'epoch': 0.76}
 76%|███████▌  | 1540/2038 [2:44:50<2:31:28, 18.25s/it] 76%|███████▌  | 1541/2038 [2:45:08<2:31:50, 18.33s/it] 76%|███████▌  | 1542/2038 [2:45:27<2:32:23, 18.44s/it] 76%|███████▌  | 1543/2038 [2:45:45<2:31:58, 18.42s/it] 76%|███████▌  | 1544/2038 [2:46:03<2:30:55, 18.33s/it] 76%|███████▌  | 1545/2038 [2:46:21<2:30:02, 18.26s/it] 76%|███████▌  | 1546/2038 [2:46:40<2:29:28, 18.23s/it] 76%|███████▌  | 1547/2038 [2:46:58<2:30:11, 18.35s/it] 76%|███████▌  | 1548/2038 [2:47:17<2:31:11, 18.51s/it] 76%|███████▌  | 1549/2038 [2:47:35<2:30:33, 18.47s/it] 76%|███████▌  | 1550/2038 [2:47:54<2:29:32, 18.39s/it]                                                       {'loss': 0.1804, 'grad_norm': 0.027077671140432358, 'learning_rate': 2.4746963562753038e-05, 'entropy': 0.17996904700994493, 'num_tokens': 109096844.0, 'mean_token_accuracy': 0.9388494431972504, 'epoch': 0.76}
 76%|███████▌  | 1550/2038 [2:47:54<2:29:32, 18.39s/it] 76%|███████▌  | 1551/2038 [2:48:12<2:28:29, 18.29s/it] 76%|███████▌  | 1552/2038 [2:48:30<2:27:38, 18.23s/it] 76%|███████▌  | 1553/2038 [2:48:48<2:27:39, 18.27s/it] 76%|███████▋  | 1554/2038 [2:49:07<2:28:22, 18.39s/it] 76%|███████▋  | 1555/2038 [2:49:26<2:28:53, 18.50s/it] 76%|███████▋  | 1556/2038 [2:49:44<2:27:42, 18.39s/it] 76%|███████▋  | 1557/2038 [2:50:02<2:26:51, 18.32s/it] 76%|███████▋  | 1558/2038 [2:50:20<2:26:00, 18.25s/it] 76%|███████▋  | 1559/2038 [2:50:38<2:25:07, 18.18s/it] 77%|███████▋  | 1560/2038 [2:50:57<2:26:05, 18.34s/it]                                                       {'loss': 0.1797, 'grad_norm': 0.027164295315742493, 'learning_rate': 2.424089068825911e-05, 'entropy': 0.17939672023057937, 'num_tokens': 111070157.0, 'mean_token_accuracy': 0.9392367243766785, 'epoch': 0.77}
 77%|███████▋  | 1560/2038 [2:50:57<2:26:05, 18.34s/it] 77%|███████▋  | 1561/2038 [2:51:15<2:26:40, 18.45s/it] 77%|███████▋  | 1562/2038 [2:51:34<2:26:15, 18.44s/it] 77%|███████▋  | 1563/2038 [2:51:52<2:25:11, 18.34s/it] 77%|███████▋  | 1564/2038 [2:52:10<2:24:29, 18.29s/it] 77%|███████▋  | 1565/2038 [2:52:28<2:23:44, 18.23s/it] 77%|███████▋  | 1566/2038 [2:52:47<2:24:14, 18.34s/it] 77%|███████▋  | 1567/2038 [2:53:06<2:25:01, 18.48s/it] 77%|███████▋  | 1568/2038 [2:53:24<2:24:32, 18.45s/it] 77%|███████▋  | 1569/2038 [2:53:42<2:23:11, 18.32s/it] 77%|███████▋  | 1570/2038 [2:54:00<2:22:27, 18.26s/it]                                                       {'loss': 0.1773, 'grad_norm': 0.02891477383673191, 'learning_rate': 2.3734817813765184e-05, 'entropy': 0.17741698175668716, 'num_tokens': 113033987.0, 'mean_token_accuracy': 0.9396452605724335, 'epoch': 0.77}
 77%|███████▋  | 1570/2038 [2:54:00<2:22:27, 18.26s/it] 77%|███████▋  | 1571/2038 [2:54:18<2:21:46, 18.22s/it] 77%|███████▋  | 1572/2038 [2:54:37<2:22:08, 18.30s/it] 77%|███████▋  | 1573/2038 [2:54:55<2:21:53, 18.31s/it] 77%|███████▋  | 1574/2038 [2:55:14<2:22:47, 18.47s/it] 77%|███████▋  | 1575/2038 [2:55:32<2:22:04, 18.41s/it] 77%|███████▋  | 1576/2038 [2:55:50<2:21:05, 18.32s/it] 77%|███████▋  | 1577/2038 [2:56:08<2:20:07, 18.24s/it] 77%|███████▋  | 1578/2038 [2:56:26<2:19:28, 18.19s/it] 77%|███████▋  | 1579/2038 [2:56:45<2:20:18, 18.34s/it] 78%|███████▊  | 1580/2038 [2:57:04<2:20:48, 18.45s/it]                                                       {'loss': 0.1783, 'grad_norm': 0.02712014876306057, 'learning_rate': 2.3228744939271258e-05, 'entropy': 0.1784482091665268, 'num_tokens': 115026722.0, 'mean_token_accuracy': 0.9396830797195435, 'epoch': 0.78}
 78%|███████▊  | 1580/2038 [2:57:04<2:20:48, 18.45s/it] 78%|███████▊  | 1581/2038 [2:57:22<2:20:27, 18.44s/it] 78%|███████▊  | 1582/2038 [2:57:40<2:19:12, 18.32s/it] 78%|███████▊  | 1583/2038 [2:57:58<2:18:21, 18.25s/it] 78%|███████▊  | 1584/2038 [2:58:16<2:17:51, 18.22s/it] 78%|███████▊  | 1585/2038 [2:58:35<2:18:16, 18.31s/it] 78%|███████▊  | 1586/2038 [2:58:54<2:18:26, 18.38s/it] 78%|███████▊  | 1587/2038 [2:59:12<2:19:01, 18.50s/it] 78%|███████▊  | 1588/2038 [2:59:30<2:17:45, 18.37s/it] 78%|███████▊  | 1589/2038 [2:59:48<2:16:36, 18.26s/it] 78%|███████▊  | 1590/2038 [3:00:06<2:15:55, 18.20s/it]                                                       {'loss': 0.18, 'grad_norm': 0.027421409264206886, 'learning_rate': 2.2722672064777328e-05, 'entropy': 0.17988378405570984, 'num_tokens': 117015560.0, 'mean_token_accuracy': 0.938901549577713, 'epoch': 0.78}
 78%|███████▊  | 1590/2038 [3:00:06<2:15:55, 18.20s/it] 78%|███████▊  | 1591/2038 [3:00:24<2:15:14, 18.15s/it] 78%|███████▊  | 1592/2038 [3:00:43<2:16:21, 18.34s/it] 78%|███████▊  | 1593/2038 [3:01:02<2:17:09, 18.49s/it] 78%|███████▊  | 1594/2038 [3:01:20<2:16:15, 18.41s/it] 78%|███████▊  | 1595/2038 [3:01:38<2:15:08, 18.30s/it] 78%|███████▊  | 1596/2038 [3:01:56<2:14:21, 18.24s/it] 78%|███████▊  | 1597/2038 [3:02:15<2:13:45, 18.20s/it] 78%|███████▊  | 1598/2038 [3:02:33<2:14:29, 18.34s/it] 78%|███████▊  | 1599/2038 [3:02:52<2:15:02, 18.46s/it] 79%|███████▊  | 1600/2038 [3:03:10<2:14:29, 18.42s/it]                                                       {'loss': 0.1798, 'grad_norm': 0.02839861437678337, 'learning_rate': 2.22165991902834e-05, 'entropy': 0.1803884819149971, 'num_tokens': 118978968.0, 'mean_token_accuracy': 0.9391469359397888, 'epoch': 0.79}
 79%|███████▊  | 1600/2038 [3:03:10<2:14:29, 18.42s/it] 79%|███████▊  | 1601/2038 [3:03:28<2:13:27, 18.32s/it] 79%|███████▊  | 1602/2038 [3:03:47<2:12:50, 18.28s/it] 79%|███████▊  | 1603/2038 [3:04:05<2:12:04, 18.22s/it] 79%|███████▊  | 1604/2038 [3:04:23<2:12:25, 18.31s/it] 79%|███████▉  | 1605/2038 [3:04:42<2:12:08, 18.31s/it] 79%|███████▉  | 1606/2038 [3:05:00<2:12:57, 18.47s/it] 79%|███████▉  | 1607/2038 [3:05:19<2:12:14, 18.41s/it] 79%|███████▉  | 1608/2038 [3:05:37<2:11:21, 18.33s/it] 79%|███████▉  | 1609/2038 [3:05:55<2:10:36, 18.27s/it] 79%|███████▉  | 1610/2038 [3:06:13<2:10:02, 18.23s/it]                                                       {'loss': 0.1816, 'grad_norm': 0.028693323954939842, 'learning_rate': 2.1710526315789474e-05, 'entropy': 0.1818791836500168, 'num_tokens': 120945306.0, 'mean_token_accuracy': 0.9382368624210358, 'epoch': 0.79}
 79%|███████▉  | 1610/2038 [3:06:13<2:10:02, 18.23s/it] 79%|███████▉  | 1611/2038 [3:06:32<2:10:56, 18.40s/it] 79%|███████▉  | 1612/2038 [3:06:51<2:11:23, 18.51s/it] 79%|███████▉  | 1613/2038 [3:07:09<2:10:41, 18.45s/it] 79%|███████▉  | 1614/2038 [3:07:27<2:09:40, 18.35s/it] 79%|███████▉  | 1615/2038 [3:07:45<2:08:47, 18.27s/it] 79%|███████▉  | 1616/2038 [3:08:03<2:08:08, 18.22s/it] 79%|███████▉  | 1617/2038 [3:08:22<2:08:22, 18.30s/it] 79%|███████▉  | 1618/2038 [3:08:40<2:08:00, 18.29s/it] 79%|███████▉  | 1619/2038 [3:08:59<2:08:51, 18.45s/it] 79%|███████▉  | 1620/2038 [3:09:17<2:07:48, 18.34s/it]                                                       {'loss': 0.173, 'grad_norm': 0.028214221820235252, 'learning_rate': 2.1204453441295548e-05, 'entropy': 0.17298556566238404, 'num_tokens': 122904889.0, 'mean_token_accuracy': 0.9411693096160889, 'epoch': 0.79}
 79%|███████▉  | 1620/2038 [3:09:17<2:07:48, 18.34s/it] 80%|███████▉  | 1621/2038 [3:09:35<2:06:55, 18.26s/it] 80%|███████▉  | 1622/2038 [3:09:53<2:06:20, 18.22s/it] 80%|███████▉  | 1623/2038 [3:10:11<2:05:48, 18.19s/it] 80%|███████▉  | 1624/2038 [3:10:30<2:06:46, 18.37s/it] 80%|███████▉  | 1625/2038 [3:10:49<2:06:53, 18.44s/it] 80%|███████▉  | 1626/2038 [3:11:07<2:06:19, 18.40s/it] 80%|███████▉  | 1627/2038 [3:11:25<2:05:09, 18.27s/it] 80%|███████▉  | 1628/2038 [3:11:43<2:04:16, 18.19s/it] 80%|███████▉  | 1629/2038 [3:12:01<2:03:55, 18.18s/it] 80%|███████▉  | 1630/2038 [3:12:20<2:04:45, 18.35s/it]                                                       {'loss': 0.1791, 'grad_norm': 0.026636479422450066, 'learning_rate': 2.069838056680162e-05, 'entropy': 0.17850760817527772, 'num_tokens': 124888480.0, 'mean_token_accuracy': 0.9392863929271698, 'epoch': 0.8}
 80%|███████▉  | 1630/2038 [3:12:20<2:04:45, 18.35s/it] 80%|████████  | 1631/2038 [3:12:38<2:04:51, 18.41s/it] 80%|████████  | 1632/2038 [3:12:57<2:04:33, 18.41s/it] 80%|████████  | 1633/2038 [3:13:15<2:03:44, 18.33s/it] 80%|████████  | 1634/2038 [3:13:33<2:02:54, 18.25s/it] 80%|████████  | 1635/2038 [3:13:51<2:02:25, 18.23s/it] 80%|████████  | 1636/2038 [3:14:10<2:02:53, 18.34s/it] 80%|████████  | 1637/2038 [3:14:28<2:02:35, 18.34s/it] 80%|████████  | 1638/2038 [3:14:46<2:02:23, 18.36s/it] 80%|████████  | 1639/2038 [3:15:05<2:02:00, 18.35s/it] 80%|████████  | 1640/2038 [3:15:23<2:01:13, 18.27s/it]                                                       {'loss': 0.1752, 'grad_norm': 0.027418650686740875, 'learning_rate': 2.0192307692307694e-05, 'entropy': 0.1758800745010376, 'num_tokens': 126859091.0, 'mean_token_accuracy': 0.9407062888145447, 'epoch': 0.8}
 80%|████████  | 1640/2038 [3:15:23<2:01:13, 18.27s/it] 81%|████████  | 1641/2038 [3:15:41<2:00:36, 18.23s/it] 81%|████████  | 1642/2038 [3:15:59<2:00:08, 18.20s/it] 81%|████████  | 1643/2038 [3:16:18<2:01:00, 18.38s/it] 81%|████████  | 1644/2038 [3:16:36<2:00:50, 18.40s/it] 81%|████████  | 1645/2038 [3:16:55<2:00:27, 18.39s/it] 81%|████████  | 1646/2038 [3:17:13<1:59:22, 18.27s/it] 81%|████████  | 1647/2038 [3:17:31<1:58:47, 18.23s/it] 81%|████████  | 1648/2038 [3:17:49<1:58:16, 18.20s/it] 81%|████████  | 1649/2038 [3:18:08<1:58:46, 18.32s/it] 81%|████████  | 1650/2038 [3:18:26<1:58:31, 18.33s/it]                                                       {'loss': 0.1787, 'grad_norm': 0.027652017772197723, 'learning_rate': 1.9686234817813767e-05, 'entropy': 0.17834691554307938, 'num_tokens': 128854708.0, 'mean_token_accuracy': 0.9393302440643311, 'epoch': 0.81}
 81%|████████  | 1650/2038 [3:18:26<1:58:31, 18.33s/it] 81%|████████  | 1651/2038 [3:18:45<1:59:02, 18.46s/it] 81%|████████  | 1652/2038 [3:19:03<1:57:56, 18.33s/it] 81%|████████  | 1653/2038 [3:19:21<1:57:13, 18.27s/it] 81%|████████  | 1654/2038 [3:19:39<1:56:42, 18.24s/it] 81%|████████  | 1655/2038 [3:19:57<1:56:09, 18.20s/it] 81%|████████▏ | 1656/2038 [3:20:16<1:56:57, 18.37s/it] 81%|████████▏ | 1657/2038 [3:20:34<1:56:58, 18.42s/it] 81%|████████▏ | 1658/2038 [3:20:53<1:56:45, 18.43s/it] 81%|████████▏ | 1659/2038 [3:21:11<1:55:50, 18.34s/it] 81%|████████▏ | 1660/2038 [3:21:29<1:55:13, 18.29s/it]                                                       {'loss': 0.1766, 'grad_norm': 0.02764882892370224, 'learning_rate': 1.9180161943319837e-05, 'entropy': 0.17662735730409623, 'num_tokens': 130841641.0, 'mean_token_accuracy': 0.9399480760097504, 'epoch': 0.81}
 81%|████████▏ | 1660/2038 [3:21:29<1:55:13, 18.29s/it] 82%|████████▏ | 1661/2038 [3:21:47<1:54:29, 18.22s/it] 82%|████████▏ | 1662/2038 [3:22:06<1:55:15, 18.39s/it] 82%|████████▏ | 1663/2038 [3:22:25<1:55:08, 18.42s/it] 82%|████████▏ | 1664/2038 [3:22:43<1:54:47, 18.41s/it] 82%|████████▏ | 1665/2038 [3:23:01<1:53:57, 18.33s/it] 82%|████████▏ | 1666/2038 [3:23:19<1:53:15, 18.27s/it] 82%|████████▏ | 1667/2038 [3:23:37<1:52:40, 18.22s/it] 82%|████████▏ | 1668/2038 [3:23:56<1:52:58, 18.32s/it] 82%|████████▏ | 1669/2038 [3:24:14<1:52:32, 18.30s/it] 82%|████████▏ | 1670/2038 [3:24:33<1:52:40, 18.37s/it]                                                       {'loss': 0.1764, 'grad_norm': 0.02817816287279129, 'learning_rate': 1.867408906882591e-05, 'entropy': 0.1773375988006592, 'num_tokens': 132839742.0, 'mean_token_accuracy': 0.9402512550354004, 'epoch': 0.82}
 82%|████████▏ | 1670/2038 [3:24:33<1:52:40, 18.37s/it] 82%|████████▏ | 1671/2038 [3:24:51<1:52:19, 18.36s/it] 82%|████████▏ | 1672/2038 [3:25:09<1:51:36, 18.30s/it] 82%|████████▏ | 1673/2038 [3:25:27<1:50:59, 18.25s/it] 82%|████████▏ | 1674/2038 [3:25:45<1:50:17, 18.18s/it] 82%|████████▏ | 1675/2038 [3:26:04<1:51:05, 18.36s/it] 82%|████████▏ | 1676/2038 [3:26:23<1:51:04, 18.41s/it] 82%|████████▏ | 1677/2038 [3:26:41<1:50:42, 18.40s/it] 82%|████████▏ | 1678/2038 [3:26:59<1:49:44, 18.29s/it] 82%|████████▏ | 1679/2038 [3:27:17<1:49:11, 18.25s/it] 82%|████████▏ | 1680/2038 [3:27:35<1:48:35, 18.20s/it]                                                       {'loss': 0.1777, 'grad_norm': 0.02733716368675232, 'learning_rate': 1.8168016194331984e-05, 'entropy': 0.1776595965027809, 'num_tokens': 134834926.0, 'mean_token_accuracy': 0.9397325277328491, 'epoch': 0.82}
 82%|████████▏ | 1680/2038 [3:27:35<1:48:35, 18.20s/it] 82%|████████▏ | 1681/2038 [3:27:54<1:48:54, 18.31s/it] 83%|████████▎ | 1682/2038 [3:28:12<1:48:39, 18.31s/it] 83%|████████▎ | 1683/2038 [3:28:31<1:49:08, 18.45s/it] 83%|████████▎ | 1684/2038 [3:28:49<1:48:13, 18.34s/it] 83%|████████▎ | 1685/2038 [3:29:07<1:47:37, 18.29s/it] 83%|████████▎ | 1686/2038 [3:29:25<1:47:07, 18.26s/it] 83%|████████▎ | 1687/2038 [3:29:43<1:46:22, 18.18s/it] 83%|████████▎ | 1688/2038 [3:30:02<1:46:56, 18.33s/it] 83%|████████▎ | 1689/2038 [3:30:21<1:47:10, 18.43s/it] 83%|████████▎ | 1690/2038 [3:30:39<1:46:35, 18.38s/it]                                                       {'loss': 0.18, 'grad_norm': 0.02791287563741207, 'learning_rate': 1.7661943319838057e-05, 'entropy': 0.18057154417037963, 'num_tokens': 136840185.0, 'mean_token_accuracy': 0.9391065001487732, 'epoch': 0.83}
 83%|████████▎ | 1690/2038 [3:30:39<1:46:35, 18.38s/it] 83%|████████▎ | 1691/2038 [3:30:57<1:45:36, 18.26s/it] 83%|████████▎ | 1692/2038 [3:31:15<1:45:02, 18.22s/it] 83%|████████▎ | 1693/2038 [3:31:33<1:44:21, 18.15s/it] 83%|████████▎ | 1694/2038 [3:31:52<1:45:09, 18.34s/it] 83%|████████▎ | 1695/2038 [3:32:10<1:45:00, 18.37s/it] 83%|████████▎ | 1696/2038 [3:32:29<1:44:46, 18.38s/it] 83%|████████▎ | 1697/2038 [3:32:47<1:44:03, 18.31s/it] 83%|████████▎ | 1698/2038 [3:33:05<1:43:31, 18.27s/it] 83%|████████▎ | 1699/2038 [3:33:23<1:43:02, 18.24s/it] 83%|████████▎ | 1700/2038 [3:33:42<1:43:16, 18.33s/it]                                                       {'loss': 0.178, 'grad_norm': 0.027771703898906708, 'learning_rate': 1.715587044534413e-05, 'entropy': 0.1777850478887558, 'num_tokens': 138832833.0, 'mean_token_accuracy': 0.9395538210868836, 'epoch': 0.83}
 83%|████████▎ | 1700/2038 [3:33:42<1:43:16, 18.33s/it] 83%|████████▎ | 1701/2038 [3:34:00<1:43:04, 18.35s/it] 84%|████████▎ | 1702/2038 [3:34:19<1:42:58, 18.39s/it] 84%|████████▎ | 1703/2038 [3:34:37<1:42:41, 18.39s/it] 84%|████████▎ | 1704/2038 [3:34:55<1:41:44, 18.28s/it] 84%|████████▎ | 1705/2038 [3:35:13<1:41:12, 18.23s/it] 84%|████████▎ | 1706/2038 [3:35:31<1:40:42, 18.20s/it] 84%|████████▍ | 1707/2038 [3:35:50<1:41:25, 18.38s/it] 84%|████████▍ | 1708/2038 [3:36:09<1:41:20, 18.43s/it] 84%|████████▍ | 1709/2038 [3:36:27<1:41:04, 18.43s/it] 84%|████████▍ | 1710/2038 [3:36:45<1:40:10, 18.32s/it]                                                       {'loss': 0.1782, 'grad_norm': 0.02747432328760624, 'learning_rate': 1.6649797570850204e-05, 'entropy': 0.17804035991430284, 'num_tokens': 140804569.0, 'mean_token_accuracy': 0.9398697853088379, 'epoch': 0.84}
 84%|████████▍ | 1710/2038 [3:36:45<1:40:10, 18.32s/it] 84%|████████▍ | 1711/2038 [3:37:03<1:39:28, 18.25s/it] 84%|████████▍ | 1712/2038 [3:37:21<1:38:59, 18.22s/it] 84%|████████▍ | 1713/2038 [3:37:40<1:39:07, 18.30s/it] 84%|████████▍ | 1714/2038 [3:37:58<1:38:48, 18.30s/it] 84%|████████▍ | 1715/2038 [3:38:17<1:39:17, 18.44s/it] 84%|████████▍ | 1716/2038 [3:38:35<1:38:19, 18.32s/it] 84%|████████▍ | 1717/2038 [3:38:53<1:37:38, 18.25s/it] 84%|████████▍ | 1718/2038 [3:39:11<1:37:14, 18.23s/it] 84%|████████▍ | 1719/2038 [3:39:29<1:36:34, 18.16s/it] 84%|████████▍ | 1720/2038 [3:39:48<1:37:19, 18.36s/it]                                                       {'loss': 0.1771, 'grad_norm': 0.027697725221514702, 'learning_rate': 1.6143724696356277e-05, 'entropy': 0.1773778021335602, 'num_tokens': 142793647.0, 'mean_token_accuracy': 0.9398720324039459, 'epoch': 0.84}
 84%|████████▍ | 1720/2038 [3:39:48<1:37:19, 18.36s/it] 84%|████████▍ | 1721/2038 [3:40:06<1:37:08, 18.39s/it] 84%|████████▍ | 1722/2038 [3:40:25<1:36:51, 18.39s/it] 85%|████████▍ | 1723/2038 [3:40:43<1:36:02, 18.29s/it] 85%|████████▍ | 1724/2038 [3:41:01<1:35:17, 18.21s/it] 85%|████████▍ | 1725/2038 [3:41:19<1:34:38, 18.14s/it] 85%|████████▍ | 1726/2038 [3:41:38<1:35:25, 18.35s/it] 85%|████████▍ | 1727/2038 [3:41:56<1:35:15, 18.38s/it] 85%|████████▍ | 1728/2038 [3:42:14<1:34:45, 18.34s/it] 85%|████████▍ | 1729/2038 [3:42:33<1:34:03, 18.27s/it] 85%|████████▍ | 1730/2038 [3:42:51<1:33:26, 18.20s/it]                                                       {'loss': 0.1783, 'grad_norm': 0.02793663553893566, 'learning_rate': 1.5637651821862347e-05, 'entropy': 0.17905851006507872, 'num_tokens': 144763285.0, 'mean_token_accuracy': 0.9395039498805999, 'epoch': 0.85}
 85%|████████▍ | 1730/2038 [3:42:51<1:33:26, 18.20s/it] 85%|████████▍ | 1731/2038 [3:43:09<1:33:01, 18.18s/it] 85%|████████▍ | 1732/2038 [3:43:27<1:33:17, 18.29s/it] 85%|████████▌ | 1733/2038 [3:43:46<1:33:09, 18.32s/it] 85%|████████▌ | 1734/2038 [3:44:04<1:33:09, 18.39s/it] 85%|████████▌ | 1735/2038 [3:44:23<1:32:47, 18.37s/it] 85%|████████▌ | 1736/2038 [3:44:41<1:31:54, 18.26s/it] 85%|████████▌ | 1737/2038 [3:44:59<1:31:26, 18.23s/it] 85%|████████▌ | 1738/2038 [3:45:17<1:30:50, 18.17s/it] 85%|████████▌ | 1739/2038 [3:45:36<1:31:28, 18.36s/it] 85%|████████▌ | 1740/2038 [3:45:54<1:31:14, 18.37s/it]                                                       {'loss': 0.1797, 'grad_norm': 0.026733508333563805, 'learning_rate': 1.5131578947368422e-05, 'entropy': 0.18014470040798186, 'num_tokens': 146745531.0, 'mean_token_accuracy': 0.9389703094959259, 'epoch': 0.85}
 85%|████████▌ | 1740/2038 [3:45:54<1:31:14, 18.37s/it] 85%|████████▌ | 1741/2038 [3:46:12<1:30:46, 18.34s/it] 85%|████████▌ | 1742/2038 [3:46:30<1:30:09, 18.27s/it] 86%|████████▌ | 1743/2038 [3:46:48<1:29:31, 18.21s/it] 86%|████████▌ | 1744/2038 [3:47:07<1:29:12, 18.21s/it] 86%|████████▌ | 1745/2038 [3:47:25<1:29:26, 18.32s/it] 86%|████████▌ | 1746/2038 [3:47:44<1:29:11, 18.33s/it] 86%|████████▌ | 1747/2038 [3:48:02<1:29:32, 18.46s/it] 86%|████████▌ | 1748/2038 [3:48:20<1:28:42, 18.35s/it] 86%|████████▌ | 1749/2038 [3:48:38<1:27:57, 18.26s/it] 86%|████████▌ | 1750/2038 [3:48:56<1:27:20, 18.20s/it]                                                       {'loss': 0.1831, 'grad_norm': 0.02719438634812832, 'learning_rate': 1.4625506072874495e-05, 'entropy': 0.18308640420436859, 'num_tokens': 148741834.0, 'mean_token_accuracy': 0.9381239473819732, 'epoch': 0.86}
 86%|████████▌ | 1750/2038 [3:48:56<1:27:20, 18.20s/it] 86%|████████▌ | 1751/2038 [3:49:15<1:26:51, 18.16s/it] 86%|████████▌ | 1752/2038 [3:49:33<1:27:33, 18.37s/it] 86%|████████▌ | 1753/2038 [3:49:52<1:27:31, 18.43s/it] 86%|████████▌ | 1754/2038 [3:50:11<1:27:25, 18.47s/it] 86%|████████▌ | 1755/2038 [3:50:29<1:26:38, 18.37s/it] 86%|████████▌ | 1756/2038 [3:50:47<1:25:58, 18.29s/it] 86%|████████▌ | 1757/2038 [3:51:05<1:25:27, 18.25s/it] 86%|████████▋ | 1758/2038 [3:51:24<1:25:47, 18.38s/it] 86%|████████▋ | 1759/2038 [3:51:42<1:25:49, 18.46s/it] 86%|████████▋ | 1760/2038 [3:52:01<1:25:47, 18.51s/it]                                                       {'loss': 0.1731, 'grad_norm': 0.027519486844539642, 'learning_rate': 1.4119433198380565e-05, 'entropy': 0.17243355065584182, 'num_tokens': 150706896.0, 'mean_token_accuracy': 0.9411968052387237, 'epoch': 0.86}
 86%|████████▋ | 1760/2038 [3:52:01<1:25:47, 18.51s/it] 86%|████████▋ | 1761/2038 [3:52:19<1:24:47, 18.37s/it] 86%|████████▋ | 1762/2038 [3:52:37<1:24:08, 18.29s/it] 87%|████████▋ | 1763/2038 [3:52:55<1:23:35, 18.24s/it] 87%|████████▋ | 1764/2038 [3:53:14<1:23:34, 18.30s/it] 87%|████████▋ | 1765/2038 [3:53:32<1:23:23, 18.33s/it] 87%|████████▋ | 1766/2038 [3:53:51<1:23:32, 18.43s/it] 87%|████████▋ | 1767/2038 [3:54:09<1:23:10, 18.41s/it] 87%|████████▋ | 1768/2038 [3:54:27<1:22:25, 18.32s/it] 87%|████████▋ | 1769/2038 [3:54:45<1:21:50, 18.26s/it] 87%|████████▋ | 1770/2038 [3:55:03<1:21:27, 18.24s/it]                                                       {'loss': 0.1796, 'grad_norm': 0.028206586837768555, 'learning_rate': 1.361336032388664e-05, 'entropy': 0.18003146797418595, 'num_tokens': 152704970.0, 'mean_token_accuracy': 0.9393317759037018, 'epoch': 0.87}
 87%|████████▋ | 1770/2038 [3:55:03<1:21:27, 18.24s/it] 87%|████████▋ | 1771/2038 [3:55:22<1:21:51, 18.40s/it] 87%|████████▋ | 1772/2038 [3:55:41<1:21:36, 18.41s/it] 87%|████████▋ | 1773/2038 [3:55:59<1:21:36, 18.48s/it] 87%|████████▋ | 1774/2038 [3:56:17<1:20:48, 18.37s/it] 87%|████████▋ | 1775/2038 [3:56:35<1:20:07, 18.28s/it] 87%|████████▋ | 1776/2038 [3:56:54<1:19:41, 18.25s/it] 87%|████████▋ | 1777/2038 [3:57:12<1:19:42, 18.32s/it] 87%|████████▋ | 1778/2038 [3:57:31<1:19:28, 18.34s/it] 87%|████████▋ | 1779/2038 [3:57:50<1:20:13, 18.59s/it] 87%|████████▋ | 1780/2038 [3:58:08<1:19:22, 18.46s/it]                                                       {'loss': 0.1775, 'grad_norm': 0.027415048331022263, 'learning_rate': 1.3107287449392714e-05, 'entropy': 0.1778000622987747, 'num_tokens': 154678174.0, 'mean_token_accuracy': 0.9399690151214599, 'epoch': 0.87}
 87%|████████▋ | 1780/2038 [3:58:08<1:19:22, 18.46s/it] 87%|████████▋ | 1781/2038 [3:58:26<1:18:36, 18.35s/it] 87%|████████▋ | 1782/2038 [3:58:44<1:18:00, 18.28s/it] 87%|████████▋ | 1783/2038 [3:59:02<1:17:33, 18.25s/it] 88%|████████▊ | 1784/2038 [3:59:21<1:17:53, 18.40s/it] 88%|████████▊ | 1785/2038 [3:59:40<1:17:49, 18.45s/it] 88%|████████▊ | 1786/2038 [3:59:58<1:17:38, 18.49s/it] 88%|████████▊ | 1787/2038 [4:00:16<1:16:50, 18.37s/it] 88%|████████▊ | 1788/2038 [4:00:34<1:16:05, 18.26s/it] 88%|████████▊ | 1789/2038 [4:00:52<1:15:35, 18.21s/it] 88%|████████▊ | 1790/2038 [4:01:11<1:15:59, 18.38s/it]                                                       {'loss': 0.1764, 'grad_norm': 0.027995498850941658, 'learning_rate': 1.2601214574898787e-05, 'entropy': 0.17649464607238768, 'num_tokens': 156675618.0, 'mean_token_accuracy': 0.9403259038925171, 'epoch': 0.88}
 88%|████████▊ | 1790/2038 [4:01:11<1:15:59, 18.38s/it] 88%|████████▊ | 1791/2038 [4:01:30<1:15:55, 18.44s/it] 88%|████████▊ | 1792/2038 [4:01:48<1:15:52, 18.51s/it] 88%|████████▊ | 1793/2038 [4:02:07<1:15:08, 18.40s/it] 88%|████████▊ | 1794/2038 [4:02:25<1:14:29, 18.32s/it] 88%|████████▊ | 1795/2038 [4:02:43<1:13:55, 18.25s/it] 88%|████████▊ | 1796/2038 [4:03:01<1:13:59, 18.35s/it] 88%|████████▊ | 1797/2038 [4:03:20<1:13:44, 18.36s/it] 88%|████████▊ | 1798/2038 [4:03:38<1:13:56, 18.49s/it] 88%|████████▊ | 1799/2038 [4:03:57<1:13:26, 18.44s/it] 88%|████████▊ | 1800/2038 [4:04:15<1:12:45, 18.34s/it]                                                       {'loss': 0.1793, 'grad_norm': 0.027345534414052963, 'learning_rate': 1.2095141700404859e-05, 'entropy': 0.17900189012289047, 'num_tokens': 158671441.0, 'mean_token_accuracy': 0.9392457842826843, 'epoch': 0.88}
 88%|████████▊ | 1800/2038 [4:04:15<1:12:45, 18.34s/it] 88%|████████▊ | 1801/2038 [4:04:33<1:12:11, 18.28s/it] 88%|████████▊ | 1802/2038 [4:04:51<1:11:37, 18.21s/it] 88%|████████▊ | 1803/2038 [4:05:10<1:12:01, 18.39s/it] 89%|████████▊ | 1804/2038 [4:05:28<1:11:46, 18.40s/it] 89%|████████▊ | 1805/2038 [4:05:47<1:11:34, 18.43s/it] 89%|████████▊ | 1806/2038 [4:06:05<1:10:47, 18.31s/it] 89%|████████▊ | 1807/2038 [4:06:23<1:10:14, 18.24s/it] 89%|████████▊ | 1808/2038 [4:06:41<1:09:51, 18.22s/it] 89%|████████▉ | 1809/2038 [4:07:00<1:09:52, 18.31s/it] 89%|████████▉ | 1810/2038 [4:07:18<1:09:40, 18.34s/it]                                                       {'loss': 0.1783, 'grad_norm': 0.028580086305737495, 'learning_rate': 1.1589068825910932e-05, 'entropy': 0.17845573276281357, 'num_tokens': 160668481.0, 'mean_token_accuracy': 0.9397870481014252, 'epoch': 0.89}
 89%|████████▉ | 1810/2038 [4:07:18<1:09:40, 18.34s/it] 89%|████████▉ | 1811/2038 [4:07:37<1:10:14, 18.57s/it] 89%|████████▉ | 1812/2038 [4:07:55<1:09:22, 18.42s/it] 89%|████████▉ | 1813/2038 [4:08:13<1:08:48, 18.35s/it] 89%|████████▉ | 1814/2038 [4:08:32<1:08:19, 18.30s/it] 89%|████████▉ | 1815/2038 [4:08:50<1:07:41, 18.21s/it] 89%|████████▉ | 1816/2038 [4:09:08<1:08:03, 18.40s/it] 89%|████████▉ | 1817/2038 [4:09:27<1:07:54, 18.44s/it] 89%|████████▉ | 1818/2038 [4:09:46<1:07:52, 18.51s/it] 89%|████████▉ | 1819/2038 [4:10:04<1:07:02, 18.37s/it] 89%|████████▉ | 1820/2038 [4:10:22<1:06:30, 18.31s/it]                                                       {'loss': 0.1715, 'grad_norm': 0.028047138825058937, 'learning_rate': 1.1082995951417005e-05, 'entropy': 0.17224658131599427, 'num_tokens': 162633907.0, 'mean_token_accuracy': 0.9415478467941284, 'epoch': 0.89}
 89%|████████▉ | 1820/2038 [4:10:22<1:06:30, 18.31s/it] 89%|████████▉ | 1821/2038 [4:10:40<1:06:01, 18.25s/it] 89%|████████▉ | 1822/2038 [4:10:59<1:06:15, 18.40s/it] 89%|████████▉ | 1823/2038 [4:11:17<1:06:14, 18.48s/it] 89%|████████▉ | 1824/2038 [4:11:36<1:06:06, 18.53s/it] 90%|████████▉ | 1825/2038 [4:11:54<1:05:14, 18.38s/it] 90%|████████▉ | 1826/2038 [4:12:12<1:04:41, 18.31s/it] 90%|████████▉ | 1827/2038 [4:12:30<1:04:12, 18.26s/it] 90%|████████▉ | 1828/2038 [4:12:49<1:04:13, 18.35s/it] 90%|████████▉ | 1829/2038 [4:13:07<1:03:53, 18.34s/it] 90%|████████▉ | 1830/2038 [4:13:26<1:03:45, 18.39s/it]                                                       {'loss': 0.1772, 'grad_norm': 0.02706456556916237, 'learning_rate': 1.0576923076923077e-05, 'entropy': 0.17710080742835999, 'num_tokens': 164631442.0, 'mean_token_accuracy': 0.9398772835731506, 'epoch': 0.9}
 90%|████████▉ | 1830/2038 [4:13:26<1:03:45, 18.39s/it] 90%|████████▉ | 1831/2038 [4:13:44<1:03:42, 18.47s/it] 90%|████████▉ | 1832/2038 [4:14:02<1:02:54, 18.32s/it] 90%|████████▉ | 1833/2038 [4:14:20<1:02:17, 18.23s/it] 90%|████████▉ | 1834/2038 [4:14:39<1:01:52, 18.20s/it] 90%|█████████ | 1835/2038 [4:14:57<1:02:11, 18.38s/it] 90%|█████████ | 1836/2038 [4:15:16<1:02:04, 18.44s/it] 90%|█████████ | 1837/2038 [4:15:34<1:01:42, 18.42s/it] 90%|█████████ | 1838/2038 [4:15:53<1:01:13, 18.37s/it] 90%|█████████ | 1839/2038 [4:16:11<1:00:40, 18.30s/it] 90%|█████████ | 1840/2038 [4:16:29<1:00:04, 18.21s/it]                                                       {'loss': 0.1803, 'grad_norm': 0.027758389711380005, 'learning_rate': 1.0070850202429152e-05, 'entropy': 0.1806382104754448, 'num_tokens': 166611986.0, 'mean_token_accuracy': 0.9389015913009644, 'epoch': 0.9}
 90%|█████████ | 1840/2038 [4:16:29<1:00:04, 18.21s/it] 90%|█████████ | 1841/2038 [4:16:47<59:58, 18.27s/it]   90%|█████████ | 1842/2038 [4:17:05<59:45, 18.29s/it] 90%|█████████ | 1843/2038 [4:17:24<59:54, 18.43s/it] 90%|█████████ | 1844/2038 [4:17:43<59:33, 18.42s/it] 91%|█████████ | 1845/2038 [4:18:01<58:58, 18.33s/it] 91%|█████████ | 1846/2038 [4:18:19<58:20, 18.23s/it] 91%|█████████ | 1847/2038 [4:18:37<57:57, 18.20s/it] 91%|█████████ | 1848/2038 [4:18:56<58:09, 18.37s/it] 91%|█████████ | 1849/2038 [4:19:14<58:01, 18.42s/it] 91%|█████████ | 1850/2038 [4:19:32<57:32, 18.36s/it]                                                     {'loss': 0.1773, 'grad_norm': 0.02793501503765583, 'learning_rate': 9.564777327935223e-06, 'entropy': 0.17749016731977463, 'num_tokens': 168583372.0, 'mean_token_accuracy': 0.9399109721183777, 'epoch': 0.91}
 91%|█████████ | 1850/2038 [4:19:32<57:32, 18.36s/it] 91%|█████████ | 1851/2038 [4:19:51<57:12, 18.35s/it] 91%|█████████ | 1852/2038 [4:20:09<56:35, 18.26s/it] 91%|█████████ | 1853/2038 [4:20:27<56:02, 18.17s/it] 91%|█████████ | 1854/2038 [4:20:45<56:14, 18.34s/it] 91%|█████████ | 1855/2038 [4:21:04<56:08, 18.41s/it] 91%|█████████ | 1856/2038 [4:21:22<55:48, 18.40s/it] 91%|█████████ | 1857/2038 [4:21:41<55:32, 18.41s/it] 91%|█████████ | 1858/2038 [4:21:59<54:58, 18.32s/it] 91%|█████████ | 1859/2038 [4:22:17<54:30, 18.27s/it] 91%|█████████▏| 1860/2038 [4:22:36<54:25, 18.34s/it]                                                     {'loss': 0.1786, 'grad_norm': 0.027308711782097816, 'learning_rate': 9.058704453441295e-06, 'entropy': 0.17851293385028838, 'num_tokens': 170573648.0, 'mean_token_accuracy': 0.9394655406475068, 'epoch': 0.91}
 91%|█████████▏| 1860/2038 [4:22:36<54:25, 18.34s/it] 91%|█████████▏| 1861/2038 [4:22:54<54:01, 18.31s/it] 91%|█████████▏| 1862/2038 [4:23:12<53:55, 18.39s/it] 91%|█████████▏| 1863/2038 [4:23:31<53:54, 18.48s/it] 91%|█████████▏| 1864/2038 [4:23:49<53:17, 18.38s/it] 92%|█████████▏| 1865/2038 [4:24:07<52:39, 18.26s/it] 92%|█████████▏| 1866/2038 [4:24:25<52:06, 18.18s/it] 92%|█████████▏| 1867/2038 [4:24:44<52:13, 18.33s/it] 92%|█████████▏| 1868/2038 [4:25:02<52:05, 18.39s/it] 92%|█████████▏| 1869/2038 [4:25:21<51:50, 18.40s/it] 92%|█████████▏| 1870/2038 [4:25:39<51:23, 18.35s/it]                                                     {'loss': 0.1757, 'grad_norm': 0.027017125859856606, 'learning_rate': 8.552631578947368e-06, 'entropy': 0.176784747838974, 'num_tokens': 172549326.0, 'mean_token_accuracy': 0.9402834713459015, 'epoch': 0.92}
 92%|█████████▏| 1870/2038 [4:25:39<51:23, 18.35s/it] 92%|█████████▏| 1871/2038 [4:25:57<50:46, 18.24s/it] 92%|█████████▏| 1872/2038 [4:26:15<50:24, 18.22s/it] 92%|█████████▏| 1873/2038 [4:26:34<50:24, 18.33s/it] 92%|█████████▏| 1874/2038 [4:26:52<50:07, 18.34s/it] 92%|█████████▏| 1875/2038 [4:27:11<50:12, 18.48s/it] 92%|█████████▏| 1876/2038 [4:27:29<49:48, 18.44s/it] 92%|█████████▏| 1877/2038 [4:27:47<49:14, 18.35s/it] 92%|█████████▏| 1878/2038 [4:28:06<48:41, 18.26s/it] 92%|█████████▏| 1879/2038 [4:28:24<48:14, 18.21s/it] 92%|█████████▏| 1880/2038 [4:28:42<48:26, 18.39s/it]                                                     {'loss': 0.1752, 'grad_norm': 0.027945881709456444, 'learning_rate': 8.046558704453442e-06, 'entropy': 0.17480345517396928, 'num_tokens': 174521548.0, 'mean_token_accuracy': 0.9406562268733978, 'epoch': 0.92}
 92%|█████████▏| 1880/2038 [4:28:42<48:26, 18.39s/it] 92%|█████████▏| 1881/2038 [4:29:01<48:16, 18.45s/it] 92%|█████████▏| 1882/2038 [4:29:20<48:05, 18.50s/it] 92%|█████████▏| 1883/2038 [4:29:38<47:25, 18.36s/it] 92%|█████████▏| 1884/2038 [4:29:56<46:55, 18.28s/it] 92%|█████████▏| 1885/2038 [4:30:14<46:23, 18.19s/it] 93%|█████████▎| 1886/2038 [4:30:33<46:34, 18.38s/it] 93%|█████████▎| 1887/2038 [4:30:51<46:20, 18.41s/it] 93%|█████████▎| 1888/2038 [4:31:09<46:01, 18.41s/it] 93%|█████████▎| 1889/2038 [4:31:28<45:43, 18.42s/it] 93%|█████████▎| 1890/2038 [4:31:46<45:11, 18.32s/it]                                                     {'loss': 0.1732, 'grad_norm': 0.02830723486840725, 'learning_rate': 7.540485829959515e-06, 'entropy': 0.17382556647062303, 'num_tokens': 176502007.0, 'mean_token_accuracy': 0.9413201749324799, 'epoch': 0.93}
 93%|█████████▎| 1890/2038 [4:31:46<45:11, 18.32s/it] 93%|█████████▎| 1891/2038 [4:32:04<44:46, 18.27s/it] 93%|█████████▎| 1892/2038 [4:32:23<44:39, 18.35s/it] 93%|█████████▎| 1893/2038 [4:32:41<44:16, 18.32s/it] 93%|█████████▎| 1894/2038 [4:33:00<44:11, 18.41s/it] 93%|█████████▎| 1895/2038 [4:33:18<44:06, 18.51s/it] 93%|█████████▎| 1896/2038 [4:33:36<43:26, 18.36s/it] 93%|█████████▎| 1897/2038 [4:33:54<42:53, 18.25s/it] 93%|█████████▎| 1898/2038 [4:34:12<42:30, 18.21s/it] 93%|█████████▎| 1899/2038 [4:34:31<42:31, 18.36s/it] 93%|█████████▎| 1900/2038 [4:34:50<42:31, 18.49s/it]                                                     {'loss': 0.1742, 'grad_norm': 0.02858477644622326, 'learning_rate': 7.034412955465587e-06, 'entropy': 0.17457015663385392, 'num_tokens': 178487828.0, 'mean_token_accuracy': 0.940937077999115, 'epoch': 0.93}
 93%|█████████▎| 1900/2038 [4:34:50<42:31, 18.49s/it] 93%|█████████▎| 1901/2038 [4:35:08<42:02, 18.41s/it] 93%|█████████▎| 1902/2038 [4:35:27<41:44, 18.41s/it] 93%|█████████▎| 1903/2038 [4:35:45<41:16, 18.35s/it] 93%|█████████▎| 1904/2038 [4:36:03<40:52, 18.30s/it] 93%|█████████▎| 1905/2038 [4:36:21<40:38, 18.33s/it] 94%|█████████▎| 1906/2038 [4:36:40<40:28, 18.40s/it] 94%|█████████▎| 1907/2038 [4:36:59<40:24, 18.51s/it] 94%|█████████▎| 1908/2038 [4:37:17<39:54, 18.42s/it] 94%|█████████▎| 1909/2038 [4:37:35<39:21, 18.31s/it] 94%|█████████▎| 1910/2038 [4:37:53<38:58, 18.27s/it]                                                     {'loss': 0.1782, 'grad_norm': 0.02771700918674469, 'learning_rate': 6.528340080971661e-06, 'entropy': 0.17775340527296066, 'num_tokens': 180472113.0, 'mean_token_accuracy': 0.9395903706550598, 'epoch': 0.94}
 94%|█████████▎| 1910/2038 [4:37:53<38:58, 18.27s/it] 94%|█████████▍| 1911/2038 [4:38:11<38:36, 18.24s/it] 94%|█████████▍| 1912/2038 [4:38:30<38:41, 18.42s/it] 94%|█████████▍| 1913/2038 [4:38:49<38:36, 18.53s/it] 94%|█████████▍| 1914/2038 [4:39:07<38:18, 18.54s/it] 94%|█████████▍| 1915/2038 [4:39:26<37:46, 18.43s/it] 94%|█████████▍| 1916/2038 [4:39:44<37:11, 18.29s/it] 94%|█████████▍| 1917/2038 [4:40:02<36:48, 18.25s/it] 94%|█████████▍| 1918/2038 [4:40:20<36:44, 18.37s/it] 94%|█████████▍| 1919/2038 [4:40:39<36:41, 18.50s/it] 94%|█████████▍| 1920/2038 [4:40:57<36:14, 18.43s/it]                                                     {'loss': 0.1788, 'grad_norm': 0.02786017395555973, 'learning_rate': 6.022267206477733e-06, 'entropy': 0.17930277585983276, 'num_tokens': 182465638.0, 'mean_token_accuracy': 0.939511650800705, 'epoch': 0.94}
 94%|█████████▍| 1920/2038 [4:40:58<36:14, 18.43s/it] 94%|█████████▍| 1921/2038 [4:41:16<35:53, 18.40s/it] 94%|█████████▍| 1922/2038 [4:41:34<35:24, 18.31s/it] 94%|█████████▍| 1923/2038 [4:41:52<35:00, 18.26s/it] 94%|█████████▍| 1924/2038 [4:42:11<34:49, 18.33s/it] 94%|█████████▍| 1925/2038 [4:42:29<34:33, 18.35s/it] 95%|█████████▍| 1926/2038 [4:42:48<34:26, 18.45s/it] 95%|█████████▍| 1927/2038 [4:43:06<34:14, 18.51s/it] 95%|█████████▍| 1928/2038 [4:43:24<33:43, 18.40s/it] 95%|█████████▍| 1929/2038 [4:43:43<33:14, 18.30s/it] 95%|█████████▍| 1930/2038 [4:44:01<32:47, 18.21s/it]                                                     {'loss': 0.1732, 'grad_norm': 0.027877790853381157, 'learning_rate': 5.5161943319838056e-06, 'entropy': 0.17336871176958085, 'num_tokens': 184440070.0, 'mean_token_accuracy': 0.9411059021949768, 'epoch': 0.95}
 95%|█████████▍| 1930/2038 [4:44:01<32:47, 18.21s/it] 95%|█████████▍| 1931/2038 [4:44:19<32:45, 18.37s/it] 95%|█████████▍| 1932/2038 [4:44:38<32:39, 18.49s/it] 95%|█████████▍| 1933/2038 [4:44:56<32:19, 18.47s/it] 95%|█████████▍| 1934/2038 [4:45:15<31:53, 18.40s/it] 95%|█████████▍| 1935/2038 [4:45:33<31:23, 18.28s/it] 95%|█████████▍| 1936/2038 [4:45:51<30:57, 18.21s/it] 95%|█████████▌| 1937/2038 [4:46:09<30:48, 18.30s/it] 95%|█████████▌| 1938/2038 [4:46:28<30:36, 18.37s/it] 95%|█████████▌| 1939/2038 [4:46:46<30:26, 18.45s/it] 95%|█████████▌| 1940/2038 [4:47:05<30:06, 18.43s/it]                                                     {'loss': 0.1771, 'grad_norm': 0.02823949046432972, 'learning_rate': 5.010121457489879e-06, 'entropy': 0.17725655287504197, 'num_tokens': 186418077.0, 'mean_token_accuracy': 0.9401070535182953, 'epoch': 0.95}
 95%|█████████▌| 1940/2038 [4:47:05<30:06, 18.43s/it] 95%|█████████▌| 1941/2038 [4:47:23<29:35, 18.30s/it] 95%|█████████▌| 1942/2038 [4:47:41<29:10, 18.24s/it] 95%|█████████▌| 1943/2038 [4:47:59<28:51, 18.23s/it] 95%|█████████▌| 1944/2038 [4:48:18<28:50, 18.41s/it] 95%|█████████▌| 1945/2038 [4:48:37<28:43, 18.53s/it] 95%|█████████▌| 1946/2038 [4:48:55<28:17, 18.45s/it] 96%|█████████▌| 1947/2038 [4:49:13<27:57, 18.43s/it] 96%|█████████▌| 1948/2038 [4:49:31<27:29, 18.32s/it] 96%|█████████▌| 1949/2038 [4:49:50<27:04, 18.25s/it] 96%|█████████▌| 1950/2038 [4:50:08<26:57, 18.38s/it]                                                     {'loss': 0.1766, 'grad_norm': 0.028425535187125206, 'learning_rate': 4.504048582995952e-06, 'entropy': 0.1765402838587761, 'num_tokens': 188396274.0, 'mean_token_accuracy': 0.9400363206863404, 'epoch': 0.96}
 96%|█████████▌| 1950/2038 [4:50:08<26:57, 18.38s/it] 96%|█████████▌| 1951/2038 [4:50:27<26:51, 18.52s/it] 96%|█████████▌| 1952/2038 [4:50:45<26:27, 18.46s/it] 96%|█████████▌| 1953/2038 [4:51:04<26:00, 18.35s/it] 96%|█████████▌| 1954/2038 [4:51:22<25:40, 18.34s/it] 96%|█████████▌| 1955/2038 [4:51:40<25:18, 18.30s/it] 96%|█████████▌| 1956/2038 [4:51:58<25:05, 18.35s/it] 96%|█████████▌| 1957/2038 [4:52:17<24:44, 18.32s/it] 96%|█████████▌| 1958/2038 [4:52:35<24:34, 18.43s/it] 96%|█████████▌| 1959/2038 [4:52:54<24:11, 18.37s/it] 96%|█████████▌| 1960/2038 [4:53:12<23:53, 18.38s/it]                                                     {'loss': 0.1796, 'grad_norm': 0.026980683207511902, 'learning_rate': 3.997975708502025e-06, 'entropy': 0.17932209521532058, 'num_tokens': 190358121.0, 'mean_token_accuracy': 0.9391814887523651, 'epoch': 0.96}
 96%|█████████▌| 1960/2038 [4:53:12<23:53, 18.38s/it] 96%|█████████▌| 1961/2038 [4:53:30<23:30, 18.32s/it] 96%|█████████▋| 1962/2038 [4:53:48<23:07, 18.25s/it] 96%|█████████▋| 1963/2038 [4:54:07<23:02, 18.43s/it] 96%|█████████▋| 1964/2038 [4:54:26<22:51, 18.54s/it] 96%|█████████▋| 1965/2038 [4:54:44<22:30, 18.50s/it] 96%|█████████▋| 1966/2038 [4:55:02<22:03, 18.38s/it] 97%|█████████▋| 1967/2038 [4:55:21<21:44, 18.38s/it] 97%|█████████▋| 1968/2038 [4:55:39<21:20, 18.30s/it] 97%|█████████▋| 1969/2038 [4:55:58<21:07, 18.38s/it] 97%|█████████▋| 1970/2038 [4:56:16<20:47, 18.35s/it]                                                     {'loss': 0.1786, 'grad_norm': 0.027694905176758766, 'learning_rate': 3.4919028340080975e-06, 'entropy': 0.17891369313001632, 'num_tokens': 192348233.0, 'mean_token_accuracy': 0.9394542753696442, 'epoch': 0.97}
 97%|█████████▋| 1970/2038 [4:56:16<20:47, 18.35s/it] 97%|█████████▋| 1971/2038 [4:56:35<20:37, 18.47s/it] 97%|█████████▋| 1972/2038 [4:56:53<20:12, 18.37s/it] 97%|█████████▋| 1973/2038 [4:57:11<19:53, 18.37s/it] 97%|█████████▋| 1974/2038 [4:57:29<19:30, 18.29s/it] 97%|█████████▋| 1975/2038 [4:57:47<19:06, 18.20s/it] 97%|█████████▋| 1976/2038 [4:58:06<18:59, 18.39s/it] 97%|█████████▋| 1977/2038 [4:58:24<18:42, 18.40s/it] 97%|█████████▋| 1978/2038 [4:58:43<18:24, 18.41s/it] 97%|█████████▋| 1979/2038 [4:59:01<18:03, 18.36s/it] 97%|█████████▋| 1980/2038 [4:59:19<17:39, 18.27s/it]                                                     {'loss': 0.1766, 'grad_norm': 0.028561202809214592, 'learning_rate': 2.9858299595141704e-06, 'entropy': 0.1765900209546089, 'num_tokens': 194342822.0, 'mean_token_accuracy': 0.9400942981243133, 'epoch': 0.97}
 97%|█████████▋| 1980/2038 [4:59:19<17:39, 18.27s/it] 97%|█████████▋| 1981/2038 [4:59:37<17:19, 18.24s/it] 97%|█████████▋| 1982/2038 [4:59:56<17:10, 18.41s/it] 97%|█████████▋| 1983/2038 [5:00:15<16:54, 18.44s/it] 97%|█████████▋| 1984/2038 [5:00:33<16:32, 18.39s/it] 97%|█████████▋| 1985/2038 [5:00:51<16:11, 18.33s/it] 97%|█████████▋| 1986/2038 [5:01:09<15:54, 18.35s/it] 97%|█████████▋| 1987/2038 [5:01:28<15:31, 18.26s/it] 98%|█████████▊| 1988/2038 [5:01:46<15:16, 18.33s/it] 98%|█████████▊| 1989/2038 [5:02:04<14:58, 18.33s/it] 98%|█████████▊| 1990/2038 [5:02:23<14:43, 18.41s/it]                                                     {'loss': 0.1765, 'grad_norm': 0.02725023217499256, 'learning_rate': 2.4797570850202433e-06, 'entropy': 0.17649362683296205, 'num_tokens': 196321039.0, 'mean_token_accuracy': 0.9402230739593506, 'epoch': 0.98}
 98%|█████████▊| 1990/2038 [5:02:23<14:43, 18.41s/it] 98%|█████████▊| 1991/2038 [5:02:41<14:25, 18.42s/it] 98%|█████████▊| 1992/2038 [5:03:00<14:07, 18.42s/it] 98%|█████████▊| 1993/2038 [5:03:18<13:43, 18.31s/it] 98%|█████████▊| 1994/2038 [5:03:36<13:23, 18.27s/it] 98%|█████████▊| 1995/2038 [5:03:55<13:10, 18.39s/it] 98%|█████████▊| 1996/2038 [5:04:13<12:53, 18.42s/it] 98%|█████████▊| 1997/2038 [5:04:32<12:35, 18.42s/it] 98%|█████████▊| 1998/2038 [5:04:50<12:13, 18.33s/it] 98%|█████████▊| 1999/2038 [5:05:08<11:54, 18.33s/it] 98%|█████████▊| 2000/2038 [5:05:26<11:33, 18.25s/it]                                                     {'loss': 0.1799, 'grad_norm': 0.02712443098425865, 'learning_rate': 1.9736842105263157e-06, 'entropy': 0.1804812356829643, 'num_tokens': 198299495.0, 'mean_token_accuracy': 0.9390783607959747, 'epoch': 0.98}
 98%|█████████▊| 2000/2038 [5:05:26<11:33, 18.25s/it]/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
 98%|█████████▊| 2001/2038 [5:07:06<26:16, 42.60s/it] 98%|█████████▊| 2002/2038 [5:07:24<21:15, 35.43s/it] 98%|█████████▊| 2003/2038 [5:07:42<17:38, 30.25s/it] 98%|█████████▊| 2004/2038 [5:08:01<15:06, 26.66s/it] 98%|█████████▊| 2005/2038 [5:08:19<13:15, 24.10s/it] 98%|█████████▊| 2006/2038 [5:08:37<11:56, 22.40s/it] 98%|█████████▊| 2007/2038 [5:08:56<10:57, 21.20s/it] 99%|█████████▊| 2008/2038 [5:09:14<10:12, 20.40s/it] 99%|█████████▊| 2009/2038 [5:09:33<09:33, 19.78s/it] 99%|█████████▊| 2010/2038 [5:09:51<08:58, 19.24s/it]                                                     {'loss': 0.1748, 'grad_norm': 0.02686523087322712, 'learning_rate': 1.4676113360323886e-06, 'entropy': 0.1747909501194954, 'num_tokens': 200297299.0, 'mean_token_accuracy': 0.9406647741794586, 'epoch': 0.99}
 99%|█████████▊| 2010/2038 [5:09:51<08:58, 19.24s/it] 99%|█████████▊| 2011/2038 [5:10:09<08:32, 18.98s/it] 99%|█████████▊| 2012/2038 [5:10:27<08:07, 18.74s/it] 99%|█████████▉| 2013/2038 [5:10:46<07:46, 18.68s/it] 99%|█████████▉| 2014/2038 [5:11:04<07:26, 18.59s/it] 99%|█████████▉| 2015/2038 [5:11:23<07:08, 18.62s/it] 99%|█████████▉| 2016/2038 [5:11:41<06:46, 18.47s/it] 99%|█████████▉| 2017/2038 [5:11:59<06:27, 18.44s/it] 99%|█████████▉| 2018/2038 [5:12:17<06:07, 18.36s/it] 99%|█████████▉| 2019/2038 [5:12:36<05:49, 18.38s/it] 99%|█████████▉| 2020/2038 [5:12:54<05:30, 18.39s/it]                                                     {'loss': 0.177, 'grad_norm': 0.027611592784523964, 'learning_rate': 9.615384615384617e-07, 'entropy': 0.1769497275352478, 'num_tokens': 202271750.0, 'mean_token_accuracy': 0.9399545550346374, 'epoch': 0.99}
 99%|█████████▉| 2020/2038 [5:12:54<05:30, 18.39s/it] 99%|█████████▉| 2021/2038 [5:13:13<05:13, 18.43s/it] 99%|█████████▉| 2022/2038 [5:13:31<04:54, 18.40s/it] 99%|█████████▉| 2023/2038 [5:13:49<04:35, 18.34s/it] 99%|█████████▉| 2024/2038 [5:14:07<04:15, 18.26s/it] 99%|█████████▉| 2025/2038 [5:14:26<03:58, 18.35s/it] 99%|█████████▉| 2026/2038 [5:14:44<03:39, 18.27s/it] 99%|█████████▉| 2027/2038 [5:15:02<03:21, 18.32s/it]100%|█████████▉| 2028/2038 [5:15:21<03:04, 18.46s/it]100%|█████████▉| 2029/2038 [5:15:39<02:45, 18.38s/it]100%|█████████▉| 2030/2038 [5:15:58<02:27, 18.39s/it]                                                     {'loss': 0.1797, 'grad_norm': 0.027689380571246147, 'learning_rate': 4.554655870445344e-07, 'entropy': 0.17936617881059647, 'num_tokens': 204261345.0, 'mean_token_accuracy': 0.9391494035720825, 'epoch': 1.0}
100%|█████████▉| 2030/2038 [5:15:58<02:27, 18.39s/it]100%|█████████▉| 2031/2038 [5:16:16<02:08, 18.32s/it]100%|█████████▉| 2032/2038 [5:16:34<01:50, 18.38s/it]100%|█████████▉| 2033/2038 [5:16:53<01:31, 18.38s/it]100%|█████████▉| 2034/2038 [5:17:12<01:14, 18.60s/it]100%|█████████▉| 2035/2038 [5:17:30<00:55, 18.46s/it]100%|█████████▉| 2036/2038 [5:17:48<00:36, 18.43s/it]100%|█████████▉| 2037/2038 [5:18:07<00:18, 18.34s/it]100%|██████████| 2038/2038 [5:18:25<00:00, 18.50s/it]/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
                                                     {'train_runtime': 19192.6093, 'train_samples_per_second': 47.568, 'train_steps_per_second': 0.106, 'train_loss': 0.09110135126628632, 'entropy': 0.1726802121847868, 'num_tokens': 205839770.0, 'mean_token_accuracy': 0.9413312077522278, 'epoch': 1.0}
100%|██████████| 2038/2038 [5:19:52<00:00, 18.50s/it]100%|██████████| 2038/2038 [5:19:52<00:00,  9.42s/it]
[Rank 1] TRAIN: completed!
[Rank 3] TRAIN: completed!
[Rank 4] TRAIN: completed![Rank 0] TRAIN: completed!
[Rank 2] TRAIN: completed!
[Rank 5] TRAIN: completed![Rank 6] TRAIN: completed![Rank 7] TRAIN: completed!



[Rank 0] SAVE: saving model...
/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:675: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
  warnings.warn(
[rank0]:[E120 04:49:34.744403653 ProcessGroupNCCL.cpp:683] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3809031, OpType=_ALLGATHER_BASE, NumelIn=194642560, NumelOut=1557140480, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
[rank0]:[E120 04:49:34.751162290 ProcessGroupNCCL.cpp:2241] [PG ID 0 PG GUID 0(default_pg) Rank 0]  failure detected by watchdog at work sequence id: 3809031 PG status: last enqueued work: 3809031, last completed work: 3809030
[rank0]:[E120 04:49:34.751184060 ProcessGroupNCCL.cpp:730] Stack trace of the failed collective not found, potentially because FlightRecorder is disabled. You can enable it by setting TORCH_NCCL_TRACE_BUFFER_SIZE to a non-zero value.
[rank0]:[E120 04:49:34.751281857 ProcessGroupNCCL.cpp:2573] [PG ID 0 PG GUID 0(default_pg) Rank 0] First PG on this rank to signal dumping.
[rank0]:[E120 04:49:35.645586397 ProcessGroupNCCL.cpp:1858] [PG ID 0 PG GUID 0(default_pg) Rank 0] Received a dump signal due to a collective timeout from this local rank and we will try our best to dump the debug info. Last enqueued NCCL work: 3809031, last completed NCCL work: 3809030.This is most likely caused by incorrect usages of collectives, e.g., wrong sizes used across ranks, the order of collectives is not same for all ranks or the scheduled collective, for some reason, didn't run. Additionally, this can be caused by GIL deadlock or other reasons such as network errors or bugs in the communications library (e.g. NCCL), etc. 
[rank0]:[E120 04:49:35.646643075 ProcessGroupNCCL.cpp:1575] [PG ID 0 PG GUID 0(default_pg) Rank 0] ProcessGroupNCCL preparing to dump debug info. Include stack trace: 1
[rank0]:[E120 04:50:35.865939847 ProcessGroupNCCL.cpp:744] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E120 04:50:35.866021035 ProcessGroupNCCL.cpp:758] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E120 04:50:35.866595857 ProcessGroupNCCL.cpp:2057] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3809031, OpType=_ALLGATHER_BASE, NumelIn=194642560, NumelOut=1557140480, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7496e4b7cb80 in /root/train_70b_repo/venv/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7496860585b7 in /root/train_70b_repo/venv/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1691 (0x74968605d1c1 in /root/train_70b_repo/venv/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xdf (0x74968605e40f in /root/train_70b_repo/venv/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xecdb4 (0x7496dd2ecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x9caa4 (0x7496e5a9caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x129c6c (0x7496e5b29c6c in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
[FATAL] DistBackendError: NCCL communicator was aborted on rank 0. 
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3809031, OpType=_ALLGATHER_BASE, NumelIn=194642560, NumelOut=1557140480, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
Exception raised from checkTimeout at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:686 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7496e4b7cb80 in /root/train_70b_repo/venv/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x247 (0x7496860585b7 in /root/train_70b_repo/venv/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::Watchdog::runLoop() + 0x1691 (0x74968605d1c1 in /root/train_70b_repo/venv/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::Watchdog::run() + 0xdf (0x74968605e40f in /root/train_70b_repo/venv/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xecdb4 (0x7496dd2ecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #5: <unknown function> + 0x9caa4 (0x7496e5a9caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x129c6c (0x7496e5b29c6c in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from run at /pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2063 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7496e4b7cb80 in /root/train_70b_repo/venv/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe34731 (0x749686034731 in /root/train_70b_repo/venv/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x9504a1 (0x749685b504a1 in /root/train_70b_repo/venv/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #3: <unknown function> + 0xecdb4 (0x7496dd2ecdb4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #4: <unknown function> + 0x9caa4 (0x7496e5a9caa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #5: <unknown function> + 0x129c6c (0x7496e5b29c6c in /lib/x86_64-linux-gnu/libc.so.6)

E0120 04:55:44.685000 6012 torch/distributed/elastic/multiprocessing/api.py:882] failed (exitcode: -6) local_rank: 0 (pid: 6158) of binary: /root/train_70b_repo/venv/bin/python3
Traceback (most recent call last):
  File "/root/train_70b_repo/venv/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/run.py", line 936, in main
    run(args)
  File "/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/run.py", line 927, in run
    elastic_launch(
  File "/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 156, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/train_70b_repo/venv/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 293, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
=====================================================
train.py FAILED
-----------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
-----------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-01-20_04:55:44
  host      : b300.datacrunch.io
  rank      : 0 (local_rank: 0)
  exitcode  : -6 (pid: 6158)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 6158
=====================================================
